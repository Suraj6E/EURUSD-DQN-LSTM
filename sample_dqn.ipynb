{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple Deep Q-Network Leanring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# Define the Deep Q-Network (DQN) architecture\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x.view(x.size(0), -1)  # Reshape output to have shape [batch_size, num_actions]\n",
    "\n",
    "\n",
    "# Define the Deep Q-Learning agent\n",
    "class DQNAgent:\n",
    "    def __init__(self, input_size, output_size, learning_rate=0.001, gamma=0.99, epsilon=1.0, epsilon_decay=0.999, epsilon_min=0.01):\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.q_network = DQN(input_size, output_size).to(self.device)\n",
    "        self.target_network = DQN(input_size, output_size).to(self.device)\n",
    "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
    "        self.target_network.eval()\n",
    "        self.optimizer = optim.Adam(self.q_network.parameters(), lr=learning_rate)\n",
    "        self.loss_function = nn.MSELoss()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.output_size = output_size\n",
    "\n",
    "    def select_action(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return random.randrange(self.output_size)  # Choose a random action\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.FloatTensor(state).to(self.device)\n",
    "                q_values = self.q_network(state_tensor)\n",
    "                return torch.argmax(q_values).item()  # Choose action with highest Q-value\n",
    "\n",
    "    def train(self, state, action, next_state, reward, done):\n",
    "        state_tensor = torch.FloatTensor(state).to(self.device)\n",
    "        next_state_tensor = torch.FloatTensor(next_state).to(self.device)\n",
    "        action_tensor = torch.LongTensor([action]).to(self.device)\n",
    "        reward_tensor = torch.FloatTensor([reward]).to(self.device)\n",
    "        done_tensor = torch.FloatTensor([done]).to(self.device)\n",
    "\n",
    "        # Calculate Q-values for current and next states\n",
    "        q_values = self.q_network(state_tensor)\n",
    "        next_q_values = self.target_network(next_state_tensor)\n",
    "        print(f\"Q-values shape: {q_values.shape}\")\n",
    "        print(f\"Next Q-values shape: {next_q_values.shape}\")\n",
    "\n",
    "        q_value = q_values.gather(1, action_tensor.unsqueeze(1)).squeeze(1)\n",
    "        print(f\"Q-value shape: {q_value.shape}\")\n",
    "\n",
    "        # Calculate target Q-value using the Bellman equation\n",
    "        target_q = reward_tensor + (1 - done_tensor) * self.gamma * next_q_values.max(1)[0]\n",
    "        print(f\"Target Q-value shape: {target_q.shape}\")\n",
    "\n",
    "        # Calculate loss and update Q-network\n",
    "        loss = self.loss_function(q_value, target_q.detach())\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # Decay epsilon\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n",
    "\n",
    "# Define the environment\n",
    "class GridWorld:\n",
    "    def __init__(self, grid_size=5, num_obstacles=5):\n",
    "        self.grid_size = grid_size\n",
    "        self.num_obstacles = num_obstacles\n",
    "        # self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.agent_position = torch.tensor([0, 0], dtype=torch.float32, device=self.device)\n",
    "        self.goal_position = torch.tensor([self.grid_size - 1, self.grid_size - 1], dtype=torch.float32, device=self.device)\n",
    "        self.obstacle_positions = {(torch.randint(0, self.grid_size, (1,)).item(), torch.randint(0, self.grid_size, (1,)).item()) for _ in range(self.num_obstacles)}\n",
    "        self.done = False\n",
    "        \n",
    "    def step(self, action):\n",
    "        if self.done:\n",
    "            raise ValueError(\"Episode has ended, please call reset() to restart.\")\n",
    "\n",
    "        if action == 0:  # Move right\n",
    "            next_position = (self.agent_position[0], min(self.grid_size - 1, self.agent_position[1] + 1))\n",
    "        elif action == 1:  # Move left\n",
    "            next_position = (self.agent_position[0], max(0, self.agent_position[1] - 1))\n",
    "        elif action == 2:  # Move down\n",
    "            next_position = (min(self.grid_size - 1, self.agent_position[0] + 1), self.agent_position[1])\n",
    "        elif action == 3:  # Move up\n",
    "            next_position = (max(0, self.agent_position[0] - 1), self.agent_position[1])\n",
    "        else:\n",
    "            raise ValueError(\"Invalid action.\")\n",
    "\n",
    "        next_position = tuple(next_position)  # Convert to tuple\n",
    "        next_state = self.render()  # Get the entire grid state\n",
    "        if next_position in self.obstacle_positions:\n",
    "            reward = torch.tensor(-1, dtype=torch.float32, device=self.device)\n",
    "        elif torch.all(torch.tensor(next_position) == self.goal_position):\n",
    "            reward = torch.tensor(1, dtype=torch.float32, device=self.device)\n",
    "            self.done = True\n",
    "        else:\n",
    "            reward = torch.tensor(0, dtype=torch.float32, device=self.device)\n",
    "\n",
    "        self.agent_position = torch.tensor(next_position, device=self.device)\n",
    "\n",
    "        return next_state, reward, self.done\n",
    "\n",
    "\n",
    "    def render(self):\n",
    "        grid = torch.zeros((self.grid_size, self.grid_size))\n",
    "        agent_row, agent_col = self.agent_position.long().tolist()  # Convert to integers\n",
    "        grid[agent_row, agent_col] = 0.5  # Agent position\n",
    "        goal_row, goal_col = self.goal_position.long().tolist()  # Convert to integers\n",
    "        grid[goal_row, goal_col] = 0.8  # Goal position\n",
    "        for obstacle_pos in self.obstacle_positions:\n",
    "            obstacle_row, obstacle_col = obstacle_pos\n",
    "            grid[obstacle_row, obstacle_col] = 0.2  # Obstacle position\n",
    "        return grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running above environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Next state tensor shape: torch.Size([1, 25])\n",
      "Next state flattened: [0.5, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.20000000298023224, 0.20000000298023224, 0.20000000298023224, 0.0, 0.20000000298023224, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.800000011920929]\n",
      "Environment grid size: 5\n",
      "Q-values shape: torch.Size([4, 1])\n",
      "Next Q-values shape: torch.Size([4, 1])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "index 3 is out of bounds for dimension 1 with size 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 24\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNext state flattened: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_state_flat\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Print the flattened next state\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnvironment grid size: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00menv\u001b[38;5;241m.\u001b[39mgrid_size\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \u001b[38;5;66;03m# Print the grid size of the environment\u001b[39;00m\n\u001b[1;32m---> 24\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnext_state_flat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Pass the flattened next state to the agent\u001b[39;00m\n\u001b[0;32m     25\u001b[0m state \u001b[38;5;241m=\u001b[39m next_state_flat\n\u001b[0;32m     26\u001b[0m step_count \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[1], line 61\u001b[0m, in \u001b[0;36mDQNAgent.train\u001b[1;34m(self, state, action, next_state, reward, done)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ-values shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_values\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNext Q-values shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnext_q_values\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m q_value \u001b[38;5;241m=\u001b[39m \u001b[43mq_values\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgather\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maction_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQ-value shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mq_value\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Calculate target Q-value using the Bellman equation\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: index 3 is out of bounds for dimension 1 with size 1"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Training loop\n",
    "env = GridWorld(grid_size=5, num_obstacles=5)\n",
    "input_size = env.grid_size * env.grid_size  # Flatten the grid\n",
    "output_size = 4  # Number of possible actions (right, left, down, up)\n",
    "agent = DQNAgent(input_size, output_size)\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    env.reset()\n",
    "    done = False\n",
    "    step_count = 0\n",
    "    state = env.render().flatten().tolist()\n",
    "    \n",
    "    while not done:\n",
    "        action = agent.select_action(state)\n",
    "        next_state, reward, done = env.step(action)\n",
    "        next_state_flat = np.ravel(next_state).tolist()  # Flatten the next state\n",
    "        next_state_tensor = torch.FloatTensor(next_state_flat).unsqueeze(0)  # Convert to tensor and add batch dimension\n",
    "        print(f\"Next state tensor shape: {next_state_tensor.shape}\")  # Print the shape of next_state_tensor\n",
    "        print(f\"Next state flattened: {next_state_flat}\")  # Print the flattened next state\n",
    "        print(f\"Environment grid size: {env.grid_size}\")  # Print the grid size of the environment\n",
    "        agent.train(state, action, next_state_flat, reward, done)  # Pass the flattened next state to the agent\n",
    "        state = next_state_flat\n",
    "        step_count += 1\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}: Total Steps = {step_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is available. Found 1 GPU(s):\n",
      "  - GPU 0: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA is available. Found {torch.cuda.device_count()} GPU(s):\")\n",
    "    for i in range(torch.cuda.device_count()):\n",
    "        print(f\"  - GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
