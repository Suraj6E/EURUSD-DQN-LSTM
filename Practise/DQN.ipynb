{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import random\n",
    "from itertools import count\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Create a custom log formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configure logging with custom format and file output\n",
    "logging.basicConfig(filename='./logs/app.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        open     high      low    close\n",
      "timestamp                                              \n",
      "2023-01-01 17:05:00  1.06973  1.06978  1.06970  1.06970\n",
      "2023-01-01 17:06:00  1.06966  1.06966  1.06966  1.06966\n",
      "2023-01-01 17:08:00  1.06970  1.06974  1.06970  1.06970\n",
      "2023-01-01 17:10:00  1.06975  1.06980  1.06972  1.06972\n",
      "2023-01-01 17:11:00  1.06972  1.06972  1.06972  1.06972\n",
      "2023-01-01 17:12:00  1.06975  1.06980  1.06975  1.06975\n",
      "2023-01-01 17:13:00  1.07066  1.07066  1.06917  1.06917\n",
      "2023-01-01 17:14:00  1.06937  1.06937  1.06899  1.06899\n",
      "2023-01-01 17:15:00  1.06788  1.06788  1.06788  1.06788\n",
      "2023-01-01 17:16:00  1.06788  1.06788  1.06788  1.06788\n",
      "            open       high        low      close\n",
      "count  10.000000  10.000000  10.000000  10.000000\n",
      "mean    1.069410   1.069429   1.069217   1.069217\n",
      "std     0.000871   0.000879   0.000751   0.000751\n",
      "min     1.067880   1.067880   1.067880   1.067880\n",
      "25%     1.069443   1.069443   1.069035   1.069035\n",
      "50%     1.069710   1.069730   1.069680   1.069680\n",
      "75%     1.069745   1.069795   1.069715   1.069715\n",
      "max     1.070660   1.070660   1.069750   1.069750\n"
     ]
    }
   ],
   "source": [
    "filename = \"EURUSD_M1_2023.csv\"\n",
    "\n",
    "df = pd.read_csv(\"./data_saved/\"+filename)\n",
    " # Convert 'timestamp' column to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "\n",
    "#timestamp as index\n",
    "df.set_index('timestamp', inplace=True)\n",
    "data = df.resample('15min').agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last'}).reset_index()\n",
    "\n",
    "df = df.iloc[:10]\n",
    "\n",
    "data = df.copy()\n",
    "#Drop NA rows\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "print(data.head(10))\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "class Actions(Enum):\n",
    "    Sell = 0\n",
    "    Buy = 1\n",
    "    Hold = 2\n",
    "\n",
    "\n",
    "class Positions(Enum):\n",
    "    Short = 0\n",
    "    Long = 1\n",
    "    Hold = 2\n",
    "\n",
    "    def opposite(self):\n",
    "        return Positions.Short if self == Positions.Long else Positions.Long\n",
    "\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': 3}\n",
    "\n",
    "    def __init__(self, df, window_size, render_mode=None):\n",
    "        # logging.debug(\"Trading Env init.\")\n",
    "        assert df.ndim == 2\n",
    "        assert render_mode is None or render_mode in self.metadata['render_modes']\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self._done = False\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.prices, self.signal_features = self._process_data()\n",
    "        self.shape = (window_size, self.signal_features.shape[1])\n",
    "\n",
    "        # spaces\n",
    "        self.action_space = gym.spaces.Discrete(len(Actions))\n",
    "        INF = 1e10\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-INF, high=INF, shape=self.shape, dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # episode\n",
    "        self._start_tick = self.window_size\n",
    "        self._end_tick = len(self.prices) - 1\n",
    "        self._truncated = None\n",
    "        self._current_tick = None\n",
    "        self._last_trade_tick = None\n",
    "        self._position = None\n",
    "        self._position_history = None\n",
    "        self._total_reward = None\n",
    "        self._total_profit = None\n",
    "        self._first_rendering = None\n",
    "        self.history = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # logging.debug(\"Trading Env reset.\")\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.action_space.seed(int((self.np_random.uniform(0, seed if seed is not None else 1))))\n",
    "\n",
    "        self._truncated = False\n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_trade_tick = self._current_tick - 1\n",
    "\n",
    "        self._position = Positions.Short\n",
    "        # logging.debug(\"Last tick  \", self._last_trade_tick)\n",
    "\n",
    "        # self._position = self.get_position(seed)\n",
    "        self._position_history = (self.window_size * [None]) + [self._position]\n",
    "        self._total_reward = 0.\n",
    "        self._total_profit = 1.  # unit\n",
    "        self._first_rendering = True\n",
    "        self.history = {}\n",
    "\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._done = True\n",
    "\n",
    "        observation = self._get_observation()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # logging.debug(\"Trading Env step.\")\n",
    "        self._truncated = False\n",
    "        self._current_tick += 1\n",
    "\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._truncated = True\n",
    "\n",
    "        step_reward = self._calculate_reward(action)\n",
    "        print(\"step_reward: \", step_reward)\n",
    "        print(\"self._total_reward: \", self._total_reward)\n",
    "        self._total_reward += step_reward\n",
    "\n",
    "        # print(\"Updating profit\")\n",
    "\n",
    "        self._update_profit(action)\n",
    "\n",
    "        trade = False\n",
    "        if (\n",
    "            (action == Actions.Buy.value and self._position == Positions.Short) or\n",
    "            (action == Actions.Sell.value and self._position == Positions.Long)\n",
    "        ):\n",
    "            trade = True\n",
    "\n",
    "        if trade:\n",
    "            self._position = self._position.opposite()\n",
    "            self._last_trade_tick = self._current_tick\n",
    "\n",
    "        self._position_history.append(self._position)\n",
    "        observation = self._get_observation()\n",
    "        info = self._get_info()\n",
    "        self._update_history(info)\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self._render_frame()\n",
    "        \n",
    "\n",
    "\n",
    "        return observation, step_reward, self._done, self._truncated, info\n",
    "\n",
    "    def _get_info(self):\n",
    "        # logging.debug(\"Trading Env get info.\")\n",
    "        return dict(\n",
    "            total_reward=self._total_reward,\n",
    "            total_profit=self._total_profit,\n",
    "            position=self._position\n",
    "        )\n",
    "    #return a random position\n",
    "    # def get_position(seed=None):\n",
    "    #     logging.debug(\"Trading Env get position.\")\n",
    "    #     if seed is not None:\n",
    "    #         random.seed(seed)\n",
    "\n",
    "    #     return random.choice(list(Positions))\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # logging.debug(\"Trading Env get observation.\")\n",
    "        return self.signal_features[(self._current_tick - self.window_size + 1) : self._current_tick + 1]\n",
    "\n",
    "    def _update_history(self, info):\n",
    "        # logging.debug(\"Trading Env uodate history.\")\n",
    "        if not self.history:\n",
    "            self.history = {key: [] for key in info.keys()}\n",
    "\n",
    "        for key, value in info.items():\n",
    "            self.history[key].append(value)\n",
    "\n",
    "    def _render_frame(self):\n",
    "        self.render()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # logging.debug(\"Trading Env Render.\")\n",
    "\n",
    "        def _plot_position(position, tick):\n",
    "            color = None\n",
    "            if position == Positions.Short:\n",
    "                color = 'red'\n",
    "            elif position == Positions.Long:\n",
    "                color = 'green'\n",
    "            if color:\n",
    "                plt.scatter(tick, self.prices[tick], color=color)\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        if self._first_rendering:\n",
    "            self._first_rendering = False\n",
    "            plt.cla()\n",
    "            plt.plot(self.prices)\n",
    "            start_position = self._position_history[self._start_tick]\n",
    "            _plot_position(start_position, self._start_tick)\n",
    "\n",
    "        _plot_position(self._position, self._current_tick)\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
    "            \"Total Profit: %.6f\" % self._total_profit\n",
    "        )\n",
    "\n",
    "        end_time = time()\n",
    "        process_time = end_time - start_time\n",
    "\n",
    "        pause_time = (1 / self.metadata['render_fps']) - process_time\n",
    "        assert pause_time > 0., \"High FPS! Try to reduce the 'render_fps' value.\"\n",
    "\n",
    "        plt.pause(pause_time)\n",
    "\n",
    "    def render_all(self, title=None):\n",
    "        # logging.debug(\"Trading Env render all.\")\n",
    "        window_ticks = np.arange(len(self._position_history))\n",
    "        plt.plot(self.prices)\n",
    "\n",
    "        short_ticks = []\n",
    "        long_ticks = []\n",
    "        for i, tick in enumerate(window_ticks):\n",
    "            if self._position_history[i] == Positions.Short:\n",
    "                short_ticks.append(tick)\n",
    "            elif self._position_history[i] == Positions.Long:\n",
    "                long_ticks.append(tick)\n",
    "\n",
    "        plt.plot(short_ticks, self.prices[short_ticks], 'ro')\n",
    "        plt.plot(long_ticks, self.prices[long_ticks], 'go')\n",
    "\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
    "            \"Total Profit: %.6f\" % self._total_profit\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        plt.close()\n",
    "\n",
    "    def save_rendering(self, filepath):\n",
    "        plt.savefig(filepath)\n",
    "\n",
    "    def pause_rendering(self):\n",
    "        plt.show()\n",
    "\n",
    "    def _process_data(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _update_profit(self, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def max_possible_profit(self):  # trade fees are ignored\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from .TradingEnv import TradingEnv, Actions, Positions\n",
    "\n",
    "\n",
    "class ForexEnv(TradingEnv):\n",
    "\n",
    "    def __init__(self, df, window_size, frame_bound, unit_side='left', render_mode=None):\n",
    "        # logging.debug(\"Forex Env init.\")\n",
    "        assert len(frame_bound) == 2\n",
    "        assert unit_side.lower() in ['left', 'right']\n",
    "\n",
    "        self.frame_bound = frame_bound\n",
    "        self.unit_side = unit_side.lower()\n",
    "        super().__init__(df, window_size, render_mode)\n",
    "\n",
    "        self.trade_fee = 0.0003  # unit\n",
    "\n",
    "    def _process_data(self):\n",
    "        # logging.debug(\"Forex Env process data.\")\n",
    "        prices = self.df.loc[:, 'close'].to_numpy()\n",
    "\n",
    "        prices[self.frame_bound[0] - self.window_size]  # validate index (TODO: Improve validation)\n",
    "        prices = prices[self.frame_bound[0]-self.window_size:self.frame_bound[1]]\n",
    "\n",
    "        diff = np.insert(np.diff(prices), 0, 0)\n",
    "        signal_features = np.column_stack((prices, diff))\n",
    "\n",
    "        return prices.astype(np.float32), signal_features.astype(np.float32)\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        # logging.debug(\"Forex Env calculate reward.\")\n",
    "        \n",
    "        step_reward = 0  # pip\n",
    "\n",
    "        # print(\"action :\", action)\n",
    "        # print(\"position :\", self._position)\n",
    "        trade = False\n",
    "        if (\n",
    "            (action == Actions.Buy.value and self._position == Positions.Short) or\n",
    "            (action == Actions.Sell.value and self._position == Positions.Long)\n",
    "        ):\n",
    "            trade = True\n",
    "\n",
    "        if trade:\n",
    "            # print(\"Forex Env calculate reward - trade is true :\", self._position)\n",
    "            # if self._current_tick < len(self.prices):  # Check if current_tick is within bounds\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            last_trade_price = self.prices[self._last_trade_tick]\n",
    "            price_diff = current_price - last_trade_price\n",
    "\n",
    "            if self._position == Positions.Short:\n",
    "                step_reward += -price_diff * 10000\n",
    "            elif self._position == Positions.Long:\n",
    "                step_reward += price_diff * 10000\n",
    "            \n",
    "            # print(\"price_diff: \", price_diff, current_price, last_trade_price)\n",
    "\n",
    "        return step_reward\n",
    "\n",
    "\n",
    "\n",
    "    def _update_profit(self, action):\n",
    "        # logging.debug(\"Forex Env Update profit.\")\n",
    "        trade = False\n",
    "        # print(\"Updating profit with action: \", action)\n",
    "\n",
    "        if (\n",
    "            (action == Actions.Buy.value and self._position == Positions.Short) or\n",
    "            (action == Actions.Sell.value and self._position == Positions.Long)\n",
    "        ):\n",
    "            trade = True\n",
    "\n",
    "        if trade or self._truncated:\n",
    "            # if 0 <= self._current_tick < len(self.prices) and 0 <= self._last_trade_tick < len(self.prices):\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            last_trade_price = self.prices[self._last_trade_tick]\n",
    "\n",
    "            if self.unit_side == 'left':\n",
    "                if self._position == Positions.Short:\n",
    "                    quantity = self._total_profit * (last_trade_price - self.trade_fee)\n",
    "                    self._total_profit = quantity / current_price\n",
    "\n",
    "            elif self.unit_side == 'right':\n",
    "                if self._position == Positions.Long:\n",
    "                    quantity = self._total_profit / last_trade_price\n",
    "                    self._total_profit = quantity * (current_price - self.trade_fee)\n",
    "\n",
    "    def max_possible_profit(self):\n",
    "        # logging.debug(\"Forex Env max posssible progit.\")\n",
    "        current_tick = self._start_tick\n",
    "        last_trade_tick = current_tick - 1\n",
    "        profit = 1.\n",
    "\n",
    "        while current_tick <= self._end_tick:\n",
    "            position = None\n",
    "            if self.prices[current_tick] < self.prices[current_tick - 1]:\n",
    "                while (current_tick <= self._end_tick and\n",
    "                       self.prices[current_tick] < self.prices[current_tick - 1]):\n",
    "                    current_tick += 1\n",
    "                position = Positions.Short\n",
    "            else:\n",
    "                while (current_tick <= self._end_tick and\n",
    "                       self.prices[current_tick] >= self.prices[current_tick - 1]):\n",
    "                    current_tick += 1\n",
    "                position = Positions.Long\n",
    "\n",
    "            current_price = self.prices[current_tick - 1]\n",
    "            last_trade_price = self.prices[last_trade_tick]\n",
    "\n",
    "            if self.unit_side == 'left':\n",
    "                if position == Positions.Short:\n",
    "                    quantity = profit * (last_trade_price - self.trade_fee)\n",
    "                    profit = quantity / current_price\n",
    "\n",
    "            elif self.unit_side == 'right':\n",
    "                if position == Positions.Long:\n",
    "                    quantity = profit / last_trade_price\n",
    "                    profit = quantity * (current_price - self.trade_fee)\n",
    "\n",
    "            last_trade_tick = current_tick - 1\n",
    "\n",
    "        return profit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# import random\n",
    "\n",
    "\n",
    "\n",
    "class Actions(Enum):\n",
    "    Sell = 0\n",
    "    Buy = 1\n",
    "    Hold = 2\n",
    "\n",
    "\n",
    "class Positions(Enum):\n",
    "    Short = 0\n",
    "    Long = 1\n",
    "    Hold = 2\n",
    "\n",
    "    def opposite(self):\n",
    "        return Positions.Short if self == Positions.Long else Positions.Long\n",
    "\n",
    "\n",
    "class CustomTradingEnv(gym.Env):\n",
    "\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': 3}\n",
    "\n",
    "    def __init__(self, df, window_size, render_mode=None):\n",
    "        # super().__init__(df, window_size, render_mode)\n",
    "\n",
    "        self.trade_fee = 0.0003  # unit\n",
    "        assert df.ndim == 2\n",
    "        assert render_mode is None or render_mode in self.metadata['render_modes']\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self._done = False\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.prices, self.signal_features = self._process_data()\n",
    "        self.shape = (window_size, self.signal_features.shape[1])\n",
    "\n",
    "        # spaces\n",
    "        self.action_space = gym.spaces.Discrete(len(Actions))\n",
    "        INF = 1e10\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-INF, high=INF, shape=self.shape, dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # episode\n",
    "        self._start_tick = self.window_size\n",
    "        self._end_tick = len(self.prices) - 1\n",
    "        self._truncated = None\n",
    "        self._current_tick = None\n",
    "        self._last_trade_tick = None\n",
    "        self._position = None\n",
    "        self._position_history = None\n",
    "        self._total_reward = None\n",
    "        self._total_profit = None\n",
    "        self._first_rendering = None\n",
    "        self.history = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # logging.debug(\"Trading Env reset.\")\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.action_space.seed(int((self.np_random.uniform(0, seed if seed is not None else 1))))\n",
    "\n",
    "        self._truncated = False\n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_trade_tick = self._current_tick - 1\n",
    "\n",
    "        self._position = Positions.Short\n",
    "\n",
    "        # self._position = self.get_position(seed)\n",
    "        self._position_history = (self.window_size * [None]) + [self._position]\n",
    "        self._total_reward = 0.\n",
    "        self._total_profit = 0.  # unit\n",
    "        self._first_rendering = True\n",
    "        self.history = {}\n",
    "\n",
    "        observation = self._get_observation(self._done)\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # logging.debug(\"Trading Env step.\")\n",
    "        self._truncated = False\n",
    "        self._current_tick += 1\n",
    "\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._truncated = True\n",
    "\n",
    "        print(\"Current Action: \", action)\n",
    "        #update position according with action\n",
    "        if action == Actions.Buy.value:\n",
    "            self._position = Positions.Long\n",
    "        elif action == Actions.Sell.value:\n",
    "            self._position = Positions.Short\n",
    "        else:\n",
    "            self._position = Positions.Hold\n",
    "            \n",
    "        \n",
    "        trade = False\n",
    "        if action == Actions.Buy.value  or action == Actions.Sell.value:\n",
    "            trade = True\n",
    "\n",
    "        step_reward  = 0.0\n",
    "        \n",
    "        ### Last trade tick\n",
    "        self._last_trade_tick = self._current_tick - 1\n",
    "        if trade:\n",
    "            self._position_history.append(self._position)\n",
    "\n",
    "        ## End in last tick\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._done = True\n",
    "        \n",
    "        \n",
    "        ## Calculate step rewards\n",
    "        step_reward = self._calculate_reward(action)\n",
    "        self._total_reward += step_reward\n",
    "        self._total_profit += (step_reward - self.trade_fee)\n",
    "\n",
    "        print(\"step_reward: \", step_reward)\n",
    "        print(\"total_reward: \", self._total_reward)\n",
    "        print(\"self._total_profit: \", self._total_profit)\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self._render_frame()\n",
    "\n",
    "        observation = self._get_observation(self._done)\n",
    "        info = self._get_info()\n",
    "        self._update_history(info)\n",
    "\n",
    "        return observation, step_reward, self._done, self._truncated, info\n",
    "\n",
    "    def _get_info(self):\n",
    "        # logging.debug(\"Trading Env get info.\")\n",
    "        return dict(\n",
    "            total_reward=self._total_reward,\n",
    "            total_profit=self._total_profit,\n",
    "            position=self._position\n",
    "        )\n",
    "\n",
    "    def _get_observation(self, done):\n",
    "        # logging.debug(\"Trading Env get observation.\")\n",
    "        # return self.signal_features[(self._current_tick - self.window_size + 1) : self._current_tick + 1]\n",
    "        if done:\n",
    "            return [self.prices[self._current_tick], self._total_reward]\n",
    "        return [self.prices[self._current_tick + 1], self._total_reward]\n",
    "\n",
    "    def _update_history(self, info):\n",
    "        # logging.debug(\"Trading Env uodate history.\")\n",
    "        if not self.history:\n",
    "            self.history = {key: [] for key in info.keys()}\n",
    "\n",
    "        for key, value in info.items():\n",
    "            self.history[key].append(value)\n",
    "\n",
    "    def _render_frame(self):\n",
    "        self.render()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # logging.debug(\"Trading Env Render.\")\n",
    "\n",
    "        def _plot_position(position, tick):\n",
    "            color = None\n",
    "            if position == Positions.Short:\n",
    "                color = 'red'\n",
    "            elif position == Positions.Long:\n",
    "                color = 'green'\n",
    "            if color:\n",
    "                plt.scatter(tick, self.prices[tick], color=color)\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        if self._first_rendering:\n",
    "            self._first_rendering = False\n",
    "            plt.cla()\n",
    "            plt.plot(self.prices)\n",
    "            start_position = self._position_history[self._start_tick]\n",
    "            _plot_position(start_position, self._start_tick)\n",
    "\n",
    "        _plot_position(self._position, self._current_tick)\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
    "            \"Total Profit: %.6f\" % self._total_profit\n",
    "        )\n",
    "\n",
    "        end_time = time()\n",
    "        process_time = end_time - start_time\n",
    "\n",
    "        pause_time = (1 / self.metadata['render_fps']) - process_time\n",
    "        assert pause_time > 0., \"High FPS! Try to reduce the 'render_fps' value.\"\n",
    "\n",
    "        plt.pause(pause_time)\n",
    "\n",
    "    def render_all(self, title=None):\n",
    "        # logging.debug(\"Trading Env render all.\")\n",
    "        window_ticks = np.arange(len(self._position_history))\n",
    "        plt.plot(self.prices)\n",
    "\n",
    "        short_ticks = []\n",
    "        long_ticks = []\n",
    "        for i, tick in enumerate(window_ticks):\n",
    "            if self._position_history[i] == Positions.Short:\n",
    "                short_ticks.append(tick)\n",
    "            elif self._position_history[i] == Positions.Long:\n",
    "                long_ticks.append(tick)\n",
    "\n",
    "        plt.plot(short_ticks, self.prices[short_ticks], 'ro')\n",
    "        plt.plot(long_ticks, self.prices[long_ticks], 'go')\n",
    "\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
    "            \"Total Profit: %.6f\" % self._total_profit\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        plt.close()\n",
    "\n",
    "    def save_rendering(self, filepath):\n",
    "        plt.savefig(filepath)\n",
    "\n",
    "    def pause_rendering(self):\n",
    "        plt.show()\n",
    "\n",
    "    def _process_data(self):\n",
    "        prices = self.df.loc[:, 'close'].to_numpy()\n",
    "        \n",
    "        diff = np.insert(np.diff(prices), 0, 0)\n",
    "        signal_features = np.column_stack((prices, diff))\n",
    "\n",
    "        return prices.astype(np.float32), signal_features.astype(np.float32)\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        step_reward = 0  # pip\n",
    "        trade = False\n",
    "        if self._position == Positions.Short or self._position == Positions.Long:\n",
    "            trade = True\n",
    "            print(\"Trade Found.\")\n",
    "        \n",
    "\n",
    "        if not trade and self._position == Positions.Hold:\n",
    "            print(\"trade not found...\")\n",
    "            step_reward =  -0.5\n",
    "        \n",
    "\n",
    "        if trade:\n",
    "            print(\"Cacluating reward..\")\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            last_trade_price = self.prices[self._last_trade_tick]\n",
    "            price_diff = current_price - last_trade_price\n",
    "            \n",
    "            formatted_price_diff = \"{:.5f}\".format(price_diff)\n",
    "            print(\"price_diff\", formatted_price_diff)\n",
    "\n",
    "            if self._position == Positions.Short:\n",
    "                step_reward += -price_diff * 10000\n",
    "            elif self._position == Positions.Long:\n",
    "                step_reward += price_diff * 10000\n",
    "            \n",
    "            print(\"step_reward : \", step_reward)\n",
    "        return step_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Trading envionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from envs import ForexEnv, Actions\n",
    "\n",
    "\n",
    "window_size = 2\n",
    "start_index = window_size\n",
    "end_index = len(data)\n",
    "# len(data)\n",
    "\n",
    "env  = CustomTradingEnv(\n",
    "    df= data,\n",
    "    window_size=window_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import DQN\n",
    "# import torch.optim as optim\n",
    "\n",
    "# learning_rate = 0.01\n",
    "# optimizer = optim.Adam\n",
    "\n",
    "# model = DQN(\n",
    "#     \"MlpPolicy\",\n",
    "#     env = env,\n",
    "#     buffer_size=10000, \n",
    "#     batch_size = 32,\n",
    "#     train_freq = 32,\n",
    "#     gradient_steps = 32,\n",
    "#     target_update_interval = 32,\n",
    "#     learning_rate=learning_rate,\n",
    "#     verbose=1,\n",
    "#     device=device,\n",
    "# )\n",
    "# model.learn(total_timesteps=10, log_interval=10)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "q_values_cpu:  tensor([-0.0710, -0.0526,  0.2877])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  -0.5003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': -0.5003, 'position': <Positions.Hold: 2>}\n",
      "Episode 0, Reward: -0.5\n",
      "q_values_cpu:  tensor([-0.0710, -0.0526,  0.2877])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  -0.5003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': -0.5003, 'position': <Positions.Hold: 2>}\n",
      "q_values_cpu:  tensor([-0.0710, -0.0526,  0.2877])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  -0.5003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': -0.5003, 'position': <Positions.Hold: 2>}\n",
      "q_values_cpu:  tensor([-0.0710, -0.0526,  0.2877])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  -0.5003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': -0.5003, 'position': <Positions.Hold: 2>}\n",
      "q_values_cpu:  tensor([-0.0710, -0.0526,  0.2877])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  -0.5003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': -0.5003, 'position': <Positions.Hold: 2>}\n",
      "q_values_cpu:  tensor([-0.0710, -0.0526,  0.2877])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  -0.5003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': -0.5003, 'position': <Positions.Hold: 2>}\n",
      "q_values_cpu:  tensor([-0.0710, -0.0526,  0.2877])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  -0.5003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': -0.5003, 'position': <Positions.Hold: 2>}\n",
      "q_values_cpu:  tensor([-0.0710, -0.0526,  0.2877])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  -0.5003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': -0.5003, 'position': <Positions.Hold: 2>}\n",
      "q_values_cpu:  tensor([-0.0710, -0.0526,  0.2877])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  -0.5003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': -0.5003, 'position': <Positions.Hold: 2>}\n",
      "q_values_cpu:  tensor([-0.0710, -0.0526,  0.2877])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  -0.5003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': -0.5003, 'position': <Positions.Hold: 2>}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "def epsilon_greedy_policy(model, epsilon, num_actions, device):\n",
    "    def policy_fn(state_info):\n",
    "        state_array = state_info[0] if isinstance(state_info, tuple) else state_info  # Extract the array from the tuple if needed\n",
    "        # state_array = state_array[0]\n",
    "        # print(\"state_array: \", state_array)\n",
    "        state_tensor = torch.tensor(state_array, dtype=torch.float32, device=device)\n",
    "        # print(\"state_tensor: \", state_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = model(state_tensor)\n",
    "            \n",
    "        \n",
    "        if q_values.numel() == 0:  # Check if q_values tensor is empty\n",
    "            return np.random.choice(num_actions)\n",
    "        else:\n",
    "            q_values_cpu = q_values.cpu()\n",
    "            print(\"q_values_cpu: \", q_values_cpu)\n",
    "            print(\"torch.argmax(q_values).item()\", torch.argmax(q_values_cpu).item())\n",
    "            return torch.argmax(q_values_cpu).item()\n",
    "    return policy_fn\n",
    "\n",
    "\n",
    "def train_dqn(env, model, target_model, optimizer, gamma, batch_size, device, epsilon):\n",
    "    state_info = env.reset()  # Ensure env.reset() returns a tuple (state, info) or just the state\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(model, epsilon, env.action_space.n, device)(state_info)\n",
    "        print(\"action from training: \", action)\n",
    "        step_result = env.step(action)\n",
    "        # print(\"step_result: \",step_result)\n",
    "        next_state, reward, done, truncate, info = step_result[:5]  # Extract the first four values\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # # print(\"Shapes before converting to PyTorch tensors:\")\n",
    "        # if isinstance(state_info, tuple) and len(state_info) == 2:\n",
    "        #     print(\"state shape:\", state_info[0].shape)  # Access the state from the state_info tuple\n",
    "        # else:\n",
    "        #     print(\"state shape:\", state_info.shape)  # If state_info is not a tuple, assume it's the state itself\n",
    "        \n",
    "        # print(\"next_state:\", next_state)\n",
    "        # print(\"action:\", action)\n",
    "        print(\"reward:\", reward)\n",
    "        print(\"info:\", info)\n",
    "        \n",
    "        # if len(next_state) > 0:  # Check if next_state is not empty\n",
    "        #     print(\"next_state shape:\", next_state[0].shape)\n",
    "        # else:\n",
    "        #     print(\"next_state is empty\")\n",
    "        \n",
    "        # print(\"done:\", done)\n",
    "\n",
    "        done = done or truncate\n",
    "\n",
    "        state_info = next_state  # Update state_info\n",
    "        \n",
    "    return episode_reward\n",
    "\n",
    "\n",
    "# Parameters\n",
    "input_dim = 2  # Adjusted input dimensions\n",
    "output_dim = env.action_space.n\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "epsilon = 0.1\n",
    "lr = 1e-3\n",
    "target_update = 10\n",
    "num_episodes = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize DQN and target DQN\n",
    "model = DQN(input_dim, output_dim).to(device)\n",
    "target_model = DQN(input_dim, output_dim).to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "target_model.eval()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    # print(episode)\n",
    "    episode_reward = train_dqn(env, model, target_model, optimizer, gamma, batch_size, device, epsilon)\n",
    "    \n",
    "    if episode % target_update == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "        target_model.eval()\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "# After training, you can use the model for inference as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9997\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9994000000000001\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9997, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9991000000000001\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9994000000000001, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9988000000000001\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9991000000000001, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9985000000000002\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9988000000000001, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9982000000000002\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9985000000000002, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9979000000000002\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9982000000000002, 'position': <Positions.Hold: 2>} 0 2\n",
      "Episode 0, Reward: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape of LSTM layer should be (batch_size, seq_len, input_size)\n",
    "        # Assuming batch size is 1 for reinforcement learning\n",
    "        x, _ = self.lstm(x.unsqueeze(0))\n",
    "        x = x.view(-1, self.lstm.hidden_size)  # Reshape to (batch_size, hidden_dim)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def epsilon_greedy_policy(model, epsilon, num_actions, device):\n",
    "    def policy_fn(state_info):\n",
    "        state_array = state_info[0] if isinstance(state_info, tuple) else state_info  # Extract the array from the tuple if needed\n",
    "        state_array = state_array[0]\n",
    "        state_tensor = torch.tensor(state_array, dtype=torch.float32, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = model(state_tensor)\n",
    "            \n",
    "        if q_values.numel() == 0:  # Check if q_values tensor is empty\n",
    "            return np.random.choice(num_actions)\n",
    "        else:\n",
    "            q_values_cpu = q_values.cpu()\n",
    "            return torch.argmax(q_values_cpu).item()\n",
    "    return policy_fn\n",
    "\n",
    "def train_dqn(env, model, target_model, optimizer, gamma, batch_size, device, epsilon):\n",
    "    state_info = env.reset()  # Ensure env.reset() returns a tuple (state, info) or just the state\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(model, epsilon, env.action_space.n, device)(state_info)\n",
    "        step_result = env.step(action)\n",
    "        next_state, reward, done, truncate, info = step_result[:5]  # Extract the first four values\n",
    "        episode_reward += reward\n",
    "        \n",
    "        done = done or truncate\n",
    "\n",
    "        print(\"Info: \", info, reward, action)\n",
    "        state_info = next_state  # Update state_info\n",
    "        \n",
    "    return episode_reward\n",
    "\n",
    "# Parameters\n",
    "input_dim = 2  # Adjusted input dimensions\n",
    "output_dim = env.action_space.n\n",
    "hidden_dim = 64  # Dimensionality of the hidden state in LSTM\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "epsilon = 0.1\n",
    "lr = 1e-3\n",
    "target_update = 10\n",
    "num_episodes = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize DQN and target DQN\n",
    "model = DQN(input_dim, output_dim, hidden_dim).to(device)\n",
    "target_model = DQN(input_dim, output_dim, hidden_dim).to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "target_model.eval()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    print(episode)\n",
    "    episode_reward = train_dqn(env, model, target_model, optimizer, gamma, batch_size, device, epsilon)\n",
    "    \n",
    "    if episode % target_update == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "        target_model.eval()\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "# After training, you can use the model for inference as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ArrayRef: invalid index Index = 18446744073709551615; Length = 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[253], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m num_eval_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m eval_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Plot the evaluation rewards\u001b[39;00m\n\u001b[0;32m     35\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(eval_rewards)\n",
      "Cell \u001b[1;32mIn[253], line 17\u001b[0m, in \u001b[0;36mevaluate_dqn\u001b[1;34m(env, model, num_episodes, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     16\u001b[0m     model\u001b[38;5;241m.\u001b[39meval();\n\u001b[1;32m---> 17\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m action \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     19\u001b[0m next_state, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[252], line 15\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: ArrayRef: invalid index Index = 18446744073709551615; Length = 0"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_dqn(env, model, num_episodes, device):\n",
    "    episode_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            # print(state)\n",
    "            state_array = state[0] if isinstance(state, tuple) else state  # Extract the array from the tuple if needed\n",
    "            state_array = state_array[0]\n",
    "            state_tensor = torch.tensor(state_array, dtype=torch.float32, device=device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model.eval();\n",
    "                q_values = model(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "            next_state, reward, done, truncate, info = env.step(action)\n",
    "\n",
    "            # print(reward)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            done = done or truncate\n",
    "        episode_rewards.append(episode_reward)\n",
    "    return episode_rewards\n",
    "\n",
    "# Evaluation parameters\n",
    "num_eval_episodes = 100\n",
    "\n",
    "# Evaluate the model\n",
    "eval_rewards = evaluate_dqn(env, model, num_eval_episodes, device)\n",
    "\n",
    "# Plot the evaluation rewards\n",
    "plt.plot(eval_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DQN Evaluation Rewards')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM - DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './results/dqn_model_10000ep.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"dqn_stable_baseline3_eurusd_2023_1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "env.unwrapped.render_all()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (45) does not match length of index (89)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[213], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mquantstats\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mqs\u001b[39;00m\n\u001b[0;32m      3\u001b[0m qs\u001b[38;5;241m.\u001b[39mextend_pandas()\n\u001b[1;32m----> 5\u001b[0m net_worth \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrapped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_profit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mend_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m returns \u001b[38;5;241m=\u001b[39m net_worth\u001b[38;5;241m.\u001b[39mpct_change()\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(returns)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:503\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    501\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n\u001b[1;32m--> 503\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# create/copy the manager\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (SingleBlockManager, SingleArrayManager)):\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (45) does not match length of index (89)"
     ]
    }
   ],
   "source": [
    "import quantstats as qs\n",
    "\n",
    "qs.extend_pandas()\n",
    "\n",
    "net_worth = pd.Series(env.unwrapped.history['total_profit'], index=df.index[start_index+1:end_index])\n",
    "returns = net_worth.pct_change().iloc[1:]\n",
    "\n",
    "print(returns)\n",
    "\n",
    "qs.reports.full(returns)\n",
    "qs.reports.html(returns, output='SB3_a2c_quantstats.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
