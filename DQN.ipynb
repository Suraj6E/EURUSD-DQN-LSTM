{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import random\n",
    "from itertools import count\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Create a custom log formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configure logging with custom format and file output\n",
    "logging.basicConfig(filename='./logs/app.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        open     high      low    close\n",
      "timestamp                                              \n",
      "2023-01-01 17:05:00  1.06973  1.06978  1.06970  1.06970\n",
      "2023-01-01 17:06:00  1.06966  1.06966  1.06966  1.06966\n",
      "2023-01-01 17:08:00  1.06970  1.06974  1.06970  1.06970\n",
      "2023-01-01 17:10:00  1.06975  1.06980  1.06972  1.06972\n",
      "2023-01-01 17:11:00  1.06972  1.06972  1.06972  1.06972\n",
      "2023-01-01 17:12:00  1.06975  1.06980  1.06975  1.06975\n",
      "2023-01-01 17:13:00  1.07066  1.07066  1.06917  1.06917\n",
      "2023-01-01 17:14:00  1.06937  1.06937  1.06899  1.06899\n",
      "2023-01-01 17:15:00  1.06788  1.06788  1.06788  1.06788\n",
      "2023-01-01 17:16:00  1.06788  1.06788  1.06788  1.06788\n",
      "            open       high        low      close\n",
      "count  10.000000  10.000000  10.000000  10.000000\n",
      "mean    1.069410   1.069429   1.069217   1.069217\n",
      "std     0.000871   0.000879   0.000751   0.000751\n",
      "min     1.067880   1.067880   1.067880   1.067880\n",
      "25%     1.069443   1.069443   1.069035   1.069035\n",
      "50%     1.069710   1.069730   1.069680   1.069680\n",
      "75%     1.069745   1.069795   1.069715   1.069715\n",
      "max     1.070660   1.070660   1.069750   1.069750\n"
     ]
    }
   ],
   "source": [
    "filename = \"EURUSD_M1_2023.csv\"\n",
    "\n",
    "df = pd.read_csv(\"./data_saved/\"+filename)\n",
    " # Convert 'timestamp' column to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "\n",
    "#timestamp as index\n",
    "df.set_index('timestamp', inplace=True)\n",
    "data = df.resample('15min').agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last'}).reset_index()\n",
    "\n",
    "df = df.iloc[:10]\n",
    "\n",
    "data = df.copy()\n",
    "#Drop NA rows\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "print(data.head(10))\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "class Actions(Enum):\n",
    "    Sell = 0\n",
    "    Buy = 1\n",
    "    Hold = 2\n",
    "\n",
    "\n",
    "class Positions(Enum):\n",
    "    Short = 0\n",
    "    Long = 1\n",
    "    Hold = 2\n",
    "\n",
    "    def opposite(self):\n",
    "        return Positions.Short if self == Positions.Long else Positions.Long\n",
    "\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': 3}\n",
    "\n",
    "    def __init__(self, df, window_size, render_mode=None):\n",
    "        # logging.debug(\"Trading Env init.\")\n",
    "        assert df.ndim == 2\n",
    "        assert render_mode is None or render_mode in self.metadata['render_modes']\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self._done = False\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.prices, self.signal_features = self._process_data()\n",
    "        self.shape = (window_size, self.signal_features.shape[1])\n",
    "\n",
    "        # spaces\n",
    "        self.action_space = gym.spaces.Discrete(len(Actions))\n",
    "        INF = 1e10\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-INF, high=INF, shape=self.shape, dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # episode\n",
    "        self._start_tick = self.window_size\n",
    "        self._end_tick = len(self.prices) - 1\n",
    "        self._truncated = None\n",
    "        self._current_tick = None\n",
    "        self._last_trade_tick = None\n",
    "        self._position = None\n",
    "        self._position_history = None\n",
    "        self._total_reward = None\n",
    "        self._total_profit = None\n",
    "        self._first_rendering = None\n",
    "        self.history = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # logging.debug(\"Trading Env reset.\")\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.action_space.seed(int((self.np_random.uniform(0, seed if seed is not None else 1))))\n",
    "\n",
    "        self._truncated = False\n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_trade_tick = self._current_tick - 1\n",
    "\n",
    "        self._position = Positions.Short\n",
    "        # logging.debug(\"Last tick  \", self._last_trade_tick)\n",
    "\n",
    "        # self._position = self.get_position(seed)\n",
    "        self._position_history = (self.window_size * [None]) + [self._position]\n",
    "        self._total_reward = 0.\n",
    "        self._total_profit = 1.  # unit\n",
    "        self._first_rendering = True\n",
    "        self.history = {}\n",
    "\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._done = True\n",
    "\n",
    "        observation = self._get_observation()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # logging.debug(\"Trading Env step.\")\n",
    "        self._truncated = False\n",
    "        self._current_tick += 1\n",
    "\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._truncated = True\n",
    "\n",
    "        step_reward = self._calculate_reward(action)\n",
    "        print(\"step_reward: \", step_reward)\n",
    "        print(\"self._total_reward: \", self._total_reward)\n",
    "        self._total_reward += step_reward\n",
    "\n",
    "        # print(\"Updating profit\")\n",
    "\n",
    "        self._update_profit(action)\n",
    "\n",
    "        trade = False\n",
    "        if (\n",
    "            (action == Actions.Buy.value and self._position == Positions.Short) or\n",
    "            (action == Actions.Sell.value and self._position == Positions.Long)\n",
    "        ):\n",
    "            trade = True\n",
    "\n",
    "        if trade:\n",
    "            self._position = self._position.opposite()\n",
    "            self._last_trade_tick = self._current_tick\n",
    "\n",
    "        self._position_history.append(self._position)\n",
    "        observation = self._get_observation()\n",
    "        info = self._get_info()\n",
    "        self._update_history(info)\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self._render_frame()\n",
    "        \n",
    "\n",
    "\n",
    "        return observation, step_reward, self._done, self._truncated, info\n",
    "\n",
    "    def _get_info(self):\n",
    "        # logging.debug(\"Trading Env get info.\")\n",
    "        return dict(\n",
    "            total_reward=self._total_reward,\n",
    "            total_profit=self._total_profit,\n",
    "            position=self._position\n",
    "        )\n",
    "    #return a random position\n",
    "    # def get_position(seed=None):\n",
    "    #     logging.debug(\"Trading Env get position.\")\n",
    "    #     if seed is not None:\n",
    "    #         random.seed(seed)\n",
    "\n",
    "    #     return random.choice(list(Positions))\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # logging.debug(\"Trading Env get observation.\")\n",
    "        return self.signal_features[(self._current_tick - self.window_size + 1) : self._current_tick + 1]\n",
    "\n",
    "    def _update_history(self, info):\n",
    "        # logging.debug(\"Trading Env uodate history.\")\n",
    "        if not self.history:\n",
    "            self.history = {key: [] for key in info.keys()}\n",
    "\n",
    "        for key, value in info.items():\n",
    "            self.history[key].append(value)\n",
    "\n",
    "    def _render_frame(self):\n",
    "        self.render()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # logging.debug(\"Trading Env Render.\")\n",
    "\n",
    "        def _plot_position(position, tick):\n",
    "            color = None\n",
    "            if position == Positions.Short:\n",
    "                color = 'red'\n",
    "            elif position == Positions.Long:\n",
    "                color = 'green'\n",
    "            if color:\n",
    "                plt.scatter(tick, self.prices[tick], color=color)\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        if self._first_rendering:\n",
    "            self._first_rendering = False\n",
    "            plt.cla()\n",
    "            plt.plot(self.prices)\n",
    "            start_position = self._position_history[self._start_tick]\n",
    "            _plot_position(start_position, self._start_tick)\n",
    "\n",
    "        _plot_position(self._position, self._current_tick)\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
    "            \"Total Profit: %.6f\" % self._total_profit\n",
    "        )\n",
    "\n",
    "        end_time = time()\n",
    "        process_time = end_time - start_time\n",
    "\n",
    "        pause_time = (1 / self.metadata['render_fps']) - process_time\n",
    "        assert pause_time > 0., \"High FPS! Try to reduce the 'render_fps' value.\"\n",
    "\n",
    "        plt.pause(pause_time)\n",
    "\n",
    "    def render_all(self, title=None):\n",
    "        # logging.debug(\"Trading Env render all.\")\n",
    "        window_ticks = np.arange(len(self._position_history))\n",
    "        plt.plot(self.prices)\n",
    "\n",
    "        short_ticks = []\n",
    "        long_ticks = []\n",
    "        for i, tick in enumerate(window_ticks):\n",
    "            if self._position_history[i] == Positions.Short:\n",
    "                short_ticks.append(tick)\n",
    "            elif self._position_history[i] == Positions.Long:\n",
    "                long_ticks.append(tick)\n",
    "\n",
    "        plt.plot(short_ticks, self.prices[short_ticks], 'ro')\n",
    "        plt.plot(long_ticks, self.prices[long_ticks], 'go')\n",
    "\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
    "            \"Total Profit: %.6f\" % self._total_profit\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        plt.close()\n",
    "\n",
    "    def save_rendering(self, filepath):\n",
    "        plt.savefig(filepath)\n",
    "\n",
    "    def pause_rendering(self):\n",
    "        plt.show()\n",
    "\n",
    "    def _process_data(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _update_profit(self, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def max_possible_profit(self):  # trade fees are ignored\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from .TradingEnv import TradingEnv, Actions, Positions\n",
    "\n",
    "\n",
    "class ForexEnv(TradingEnv):\n",
    "\n",
    "    def __init__(self, df, window_size, frame_bound, unit_side='left', render_mode=None):\n",
    "        # logging.debug(\"Forex Env init.\")\n",
    "        assert len(frame_bound) == 2\n",
    "        assert unit_side.lower() in ['left', 'right']\n",
    "\n",
    "        self.frame_bound = frame_bound\n",
    "        self.unit_side = unit_side.lower()\n",
    "        super().__init__(df, window_size, render_mode)\n",
    "\n",
    "        self.trade_fee = 0.0003  # unit\n",
    "\n",
    "    def _process_data(self):\n",
    "        # logging.debug(\"Forex Env process data.\")\n",
    "        prices = self.df.loc[:, 'close'].to_numpy()\n",
    "\n",
    "        prices[self.frame_bound[0] - self.window_size]  # validate index (TODO: Improve validation)\n",
    "        prices = prices[self.frame_bound[0]-self.window_size:self.frame_bound[1]]\n",
    "\n",
    "        diff = np.insert(np.diff(prices), 0, 0)\n",
    "        signal_features = np.column_stack((prices, diff))\n",
    "\n",
    "        return prices.astype(np.float32), signal_features.astype(np.float32)\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        # logging.debug(\"Forex Env calculate reward.\")\n",
    "        \n",
    "        step_reward = 0  # pip\n",
    "\n",
    "        # print(\"action :\", action)\n",
    "        # print(\"position :\", self._position)\n",
    "        trade = False\n",
    "        if (\n",
    "            (action == Actions.Buy.value and self._position == Positions.Short) or\n",
    "            (action == Actions.Sell.value and self._position == Positions.Long)\n",
    "        ):\n",
    "            trade = True\n",
    "\n",
    "        if trade:\n",
    "            # print(\"Forex Env calculate reward - trade is true :\", self._position)\n",
    "            # if self._current_tick < len(self.prices):  # Check if current_tick is within bounds\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            last_trade_price = self.prices[self._last_trade_tick]\n",
    "            price_diff = current_price - last_trade_price\n",
    "\n",
    "            if self._position == Positions.Short:\n",
    "                step_reward += -price_diff * 10000\n",
    "            elif self._position == Positions.Long:\n",
    "                step_reward += price_diff * 10000\n",
    "            \n",
    "            # print(\"price_diff: \", price_diff, current_price, last_trade_price)\n",
    "\n",
    "        return step_reward\n",
    "\n",
    "\n",
    "\n",
    "    def _update_profit(self, action):\n",
    "        # logging.debug(\"Forex Env Update profit.\")\n",
    "        trade = False\n",
    "        # print(\"Updating profit with action: \", action)\n",
    "\n",
    "        if (\n",
    "            (action == Actions.Buy.value and self._position == Positions.Short) or\n",
    "            (action == Actions.Sell.value and self._position == Positions.Long)\n",
    "        ):\n",
    "            trade = True\n",
    "\n",
    "        if trade or self._truncated:\n",
    "            # if 0 <= self._current_tick < len(self.prices) and 0 <= self._last_trade_tick < len(self.prices):\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            last_trade_price = self.prices[self._last_trade_tick]\n",
    "\n",
    "            if self.unit_side == 'left':\n",
    "                if self._position == Positions.Short:\n",
    "                    quantity = self._total_profit * (last_trade_price - self.trade_fee)\n",
    "                    self._total_profit = quantity / current_price\n",
    "\n",
    "            elif self.unit_side == 'right':\n",
    "                if self._position == Positions.Long:\n",
    "                    quantity = self._total_profit / last_trade_price\n",
    "                    self._total_profit = quantity * (current_price - self.trade_fee)\n",
    "\n",
    "    def max_possible_profit(self):\n",
    "        # logging.debug(\"Forex Env max posssible progit.\")\n",
    "        current_tick = self._start_tick\n",
    "        last_trade_tick = current_tick - 1\n",
    "        profit = 1.\n",
    "\n",
    "        while current_tick <= self._end_tick:\n",
    "            position = None\n",
    "            if self.prices[current_tick] < self.prices[current_tick - 1]:\n",
    "                while (current_tick <= self._end_tick and\n",
    "                       self.prices[current_tick] < self.prices[current_tick - 1]):\n",
    "                    current_tick += 1\n",
    "                position = Positions.Short\n",
    "            else:\n",
    "                while (current_tick <= self._end_tick and\n",
    "                       self.prices[current_tick] >= self.prices[current_tick - 1]):\n",
    "                    current_tick += 1\n",
    "                position = Positions.Long\n",
    "\n",
    "            current_price = self.prices[current_tick - 1]\n",
    "            last_trade_price = self.prices[last_trade_tick]\n",
    "\n",
    "            if self.unit_side == 'left':\n",
    "                if position == Positions.Short:\n",
    "                    quantity = profit * (last_trade_price - self.trade_fee)\n",
    "                    profit = quantity / current_price\n",
    "\n",
    "            elif self.unit_side == 'right':\n",
    "                if position == Positions.Long:\n",
    "                    quantity = profit / last_trade_price\n",
    "                    profit = quantity * (current_price - self.trade_fee)\n",
    "\n",
    "            last_trade_tick = current_tick - 1\n",
    "\n",
    "        return profit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# import random\n",
    "\n",
    "\n",
    "\n",
    "class Actions(Enum):\n",
    "    Sell = 0\n",
    "    Buy = 1\n",
    "    Hold = 2\n",
    "\n",
    "\n",
    "class Positions(Enum):\n",
    "    Short = 0\n",
    "    Long = 1\n",
    "    Hold = 2\n",
    "\n",
    "    def opposite(self):\n",
    "        return Positions.Short if self == Positions.Long else Positions.Long\n",
    "\n",
    "\n",
    "class CustomTradingEnv(gym.Env):\n",
    "\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': 3}\n",
    "\n",
    "    def __init__(self, df, window_size, render_mode=None):\n",
    "        # super().__init__(df, window_size, render_mode)\n",
    "\n",
    "        self.trade_fee = 0.0003  # unit\n",
    "        assert df.ndim == 2\n",
    "        assert render_mode is None or render_mode in self.metadata['render_modes']\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self._done = False\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.prices, self.signal_features = self._process_data()\n",
    "        self.shape = (window_size, self.signal_features.shape[1])\n",
    "\n",
    "        # spaces\n",
    "        self.action_space = gym.spaces.Discrete(len(Actions))\n",
    "        INF = 1e10\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-INF, high=INF, shape=self.shape, dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # episode\n",
    "        self._start_tick = self.window_size\n",
    "        self._end_tick = len(self.prices) - 1\n",
    "        self._truncated = None\n",
    "        self._current_tick = None\n",
    "        self._last_trade_tick = None\n",
    "        self._position = None\n",
    "        self._position_history = None\n",
    "        self._total_reward = None\n",
    "        self._total_profit = None\n",
    "        self._first_rendering = None\n",
    "        self.history = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # logging.debug(\"Trading Env reset.\")\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.action_space.seed(int((self.np_random.uniform(0, seed if seed is not None else 1))))\n",
    "\n",
    "        self._truncated = False\n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_trade_tick = self._current_tick - 1\n",
    "\n",
    "        self._position = Positions.Short\n",
    "\n",
    "        # self._position = self.get_position(seed)\n",
    "        self._position_history = (self.window_size * [None]) + [self._position]\n",
    "        self._total_reward = 0.\n",
    "        self._total_profit = 1.  # unit\n",
    "        self._first_rendering = True\n",
    "        self.history = {}\n",
    "\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._done = True\n",
    "\n",
    "        observation = self._get_observation()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # logging.debug(\"Trading Env step.\")\n",
    "        self._truncated = False\n",
    "        self._current_tick += 1\n",
    "\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._truncated = True\n",
    "\n",
    "        print(\"Current Action: \", action)\n",
    "        #update position according with action\n",
    "        if action == Actions.Buy.value:\n",
    "            self._position = Positions.Long\n",
    "        elif action == Actions.Sell.value:\n",
    "            self._position = Positions.Short\n",
    "        else:\n",
    "            self._position = Positions.Hold\n",
    "            \n",
    "        \n",
    "        trade = False\n",
    "        if action == Actions.Buy.value  or action == Actions.Sell.value:\n",
    "            print(\"trade found...\")\n",
    "            trade = True\n",
    "\n",
    "        print(\"Trade - \", trade)\n",
    "        step_reward  = 0.0\n",
    "        \n",
    "        if trade:\n",
    "            self._position = self._position.opposite()\n",
    "            self._last_trade_tick = self._current_tick\n",
    "\n",
    "        self._position_history.append(self._position)\n",
    "        observation = self._get_observation()\n",
    "        info = self._get_info()\n",
    "        self._update_history(info)\n",
    "        \n",
    "        ## Calculate step rewards\n",
    "        step_reward = self._calculate_reward(action)\n",
    "        self._total_reward += step_reward\n",
    "        self._total_profit += (step_reward - self.trade_fee)\n",
    "\n",
    "        print(\"step_reward: \", step_reward)\n",
    "        print(\"total_reward: \", self._total_reward)\n",
    "        print(\"self._total_profit: \", self._total_profit)\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, step_reward, self._done, self._truncated, info\n",
    "\n",
    "    def _get_info(self):\n",
    "        # logging.debug(\"Trading Env get info.\")\n",
    "        return dict(\n",
    "            total_reward=self._total_reward,\n",
    "            total_profit=self._total_profit,\n",
    "            position=self._position\n",
    "        )\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # logging.debug(\"Trading Env get observation.\")\n",
    "        return self.signal_features[(self._current_tick - self.window_size + 1) : self._current_tick + 1]\n",
    "\n",
    "    def _update_history(self, info):\n",
    "        # logging.debug(\"Trading Env uodate history.\")\n",
    "        if not self.history:\n",
    "            self.history = {key: [] for key in info.keys()}\n",
    "\n",
    "        for key, value in info.items():\n",
    "            self.history[key].append(value)\n",
    "\n",
    "    def _render_frame(self):\n",
    "        self.render()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # logging.debug(\"Trading Env Render.\")\n",
    "\n",
    "        def _plot_position(position, tick):\n",
    "            color = None\n",
    "            if position == Positions.Short:\n",
    "                color = 'red'\n",
    "            elif position == Positions.Long:\n",
    "                color = 'green'\n",
    "            if color:\n",
    "                plt.scatter(tick, self.prices[tick], color=color)\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        if self._first_rendering:\n",
    "            self._first_rendering = False\n",
    "            plt.cla()\n",
    "            plt.plot(self.prices)\n",
    "            start_position = self._position_history[self._start_tick]\n",
    "            _plot_position(start_position, self._start_tick)\n",
    "\n",
    "        _plot_position(self._position, self._current_tick)\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
    "            \"Total Profit: %.6f\" % self._total_profit\n",
    "        )\n",
    "\n",
    "        end_time = time()\n",
    "        process_time = end_time - start_time\n",
    "\n",
    "        pause_time = (1 / self.metadata['render_fps']) - process_time\n",
    "        assert pause_time > 0., \"High FPS! Try to reduce the 'render_fps' value.\"\n",
    "\n",
    "        plt.pause(pause_time)\n",
    "\n",
    "    def render_all(self, title=None):\n",
    "        # logging.debug(\"Trading Env render all.\")\n",
    "        window_ticks = np.arange(len(self._position_history))\n",
    "        plt.plot(self.prices)\n",
    "\n",
    "        short_ticks = []\n",
    "        long_ticks = []\n",
    "        for i, tick in enumerate(window_ticks):\n",
    "            if self._position_history[i] == Positions.Short:\n",
    "                short_ticks.append(tick)\n",
    "            elif self._position_history[i] == Positions.Long:\n",
    "                long_ticks.append(tick)\n",
    "\n",
    "        plt.plot(short_ticks, self.prices[short_ticks], 'ro')\n",
    "        plt.plot(long_ticks, self.prices[long_ticks], 'go')\n",
    "\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
    "            \"Total Profit: %.6f\" % self._total_profit\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        plt.close()\n",
    "\n",
    "    def save_rendering(self, filepath):\n",
    "        plt.savefig(filepath)\n",
    "\n",
    "    def pause_rendering(self):\n",
    "        plt.show()\n",
    "\n",
    "    def _process_data(self):\n",
    "        prices = self.df.loc[:, 'close'].to_numpy()\n",
    "        \n",
    "        diff = np.insert(np.diff(prices), 0, 0)\n",
    "        signal_features = np.column_stack((prices, diff))\n",
    "\n",
    "        return prices.astype(np.float32), signal_features.astype(np.float32)\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        step_reward = 0  # pip\n",
    "        trade = False\n",
    "        if self._position == Positions.Short or self._position == Positions.Long:\n",
    "            trade = True\n",
    "        \n",
    "        if not trade and self._position == Positions.Hold:\n",
    "            print(\"trade not found...\")\n",
    "            step_reward =  -0.5\n",
    "        \n",
    "\n",
    "        if trade:\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            last_trade_price = self.prices[self._last_trade_tick]\n",
    "            price_diff = current_price - last_trade_price\n",
    "\n",
    "            if self._position == Positions.Short:\n",
    "                step_reward += -price_diff * 10000\n",
    "            elif self._position == Positions.Long:\n",
    "                step_reward += price_diff * 10000\n",
    "\n",
    "        return step_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Trading envionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from envs import ForexEnv, Actions\n",
    "\n",
    "\n",
    "window_size = 2\n",
    "start_index = window_size\n",
    "end_index = len(data)\n",
    "# len(data)\n",
    "\n",
    "env  = CustomTradingEnv(\n",
    "    df= data,\n",
    "    window_size=window_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import DQN\n",
    "# import torch.optim as optim\n",
    "\n",
    "# learning_rate = 0.01\n",
    "# optimizer = optim.Adam\n",
    "\n",
    "# model = DQN(\n",
    "#     \"MlpPolicy\",\n",
    "#     env = env,\n",
    "#     buffer_size=10000, \n",
    "#     batch_size = 32,\n",
    "#     train_freq = 32,\n",
    "#     gradient_steps = 32,\n",
    "#     target_update_interval = 32,\n",
    "#     learning_rate=learning_rate,\n",
    "#     verbose=1,\n",
    "#     device=device,\n",
    "# )\n",
    "# model.learn(total_timesteps=10, log_interval=10)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_info:  (array([[ 1.06966e+00, -4.00000e-05],\n",
      "       [ 1.06970e+00,  4.00000e-05]], dtype=float32), {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06970e+00 4.00000e-05]\n",
      " [1.06972e+00 2.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.0\n",
      "self._total_profit:  -0.0005999999999999339\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 2.00000e-05]\n",
      " [1.06972e+00 0.00000e+00]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.5\n",
      "self._total_profit:  -0.5008999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.0, 'total_profit': -0.0005999999999999339, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 0.00000e+00]\n",
      " [1.06975e+00 3.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.0\n",
      "self._total_profit:  -1.0011999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.5, 'total_profit': -0.5008999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06975e+00  3.00000e-05]\n",
      " [ 1.06917e+00 -5.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.5\n",
      "self._total_profit:  -1.5014999999999998\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.0, 'total_profit': -1.0011999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06917e+00 -5.80000e-04]\n",
      " [ 1.06899e+00 -1.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.0\n",
      "self._total_profit:  -2.0018\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.5, 'total_profit': -1.5014999999999998, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06899e+00 -1.80000e-04]\n",
      " [ 1.06788e+00 -1.11000e-03]]\n",
      "q_values_cpu:  tensor([-0.0143, -0.0635,  0.0663])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.5\n",
      "self._total_profit:  -2.5020999999999995\n",
      "reward: -0.5\n",
      "info: {'total_reward': -3.0, 'total_profit': -2.0018, 'position': <Positions.Hold: 2>}\n",
      "Episode 0, Reward: -3.5\n",
      "state_info:  (array([[ 1.06966e+00, -4.00000e-05],\n",
      "       [ 1.06970e+00,  4.00000e-05]], dtype=float32), {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06970e+00 4.00000e-05]\n",
      " [1.06972e+00 2.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.0\n",
      "self._total_profit:  -0.0005999999999999339\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 2.00000e-05]\n",
      " [1.06972e+00 0.00000e+00]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.5\n",
      "self._total_profit:  -0.5008999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.0, 'total_profit': -0.0005999999999999339, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 0.00000e+00]\n",
      " [1.06975e+00 3.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.0\n",
      "self._total_profit:  -1.0011999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.5, 'total_profit': -0.5008999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06975e+00  3.00000e-05]\n",
      " [ 1.06917e+00 -5.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.5\n",
      "self._total_profit:  -1.5014999999999998\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.0, 'total_profit': -1.0011999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06917e+00 -5.80000e-04]\n",
      " [ 1.06899e+00 -1.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.0\n",
      "self._total_profit:  -2.0018\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.5, 'total_profit': -1.5014999999999998, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06899e+00 -1.80000e-04]\n",
      " [ 1.06788e+00 -1.11000e-03]]\n",
      "q_values_cpu:  tensor([-0.0143, -0.0635,  0.0663])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.5\n",
      "self._total_profit:  -2.5020999999999995\n",
      "reward: -0.5\n",
      "info: {'total_reward': -3.0, 'total_profit': -2.0018, 'position': <Positions.Hold: 2>}\n",
      "state_info:  (array([[ 1.06966e+00, -4.00000e-05],\n",
      "       [ 1.06970e+00,  4.00000e-05]], dtype=float32), {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06970e+00 4.00000e-05]\n",
      " [1.06972e+00 2.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.0\n",
      "self._total_profit:  -0.0005999999999999339\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 2.00000e-05]\n",
      " [1.06972e+00 0.00000e+00]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.5\n",
      "self._total_profit:  -0.5008999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.0, 'total_profit': -0.0005999999999999339, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 0.00000e+00]\n",
      " [1.06975e+00 3.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.0\n",
      "self._total_profit:  -1.0011999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.5, 'total_profit': -0.5008999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06975e+00  3.00000e-05]\n",
      " [ 1.06917e+00 -5.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.5\n",
      "self._total_profit:  -1.5014999999999998\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.0, 'total_profit': -1.0011999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06917e+00 -5.80000e-04]\n",
      " [ 1.06899e+00 -1.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.0\n",
      "self._total_profit:  -2.0018\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.5, 'total_profit': -1.5014999999999998, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06899e+00 -1.80000e-04]\n",
      " [ 1.06788e+00 -1.11000e-03]]\n",
      "q_values_cpu:  tensor([-0.0143, -0.0635,  0.0663])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.5\n",
      "self._total_profit:  -2.5020999999999995\n",
      "reward: -0.5\n",
      "info: {'total_reward': -3.0, 'total_profit': -2.0018, 'position': <Positions.Hold: 2>}\n",
      "state_info:  (array([[ 1.06966e+00, -4.00000e-05],\n",
      "       [ 1.06970e+00,  4.00000e-05]], dtype=float32), {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06970e+00 4.00000e-05]\n",
      " [1.06972e+00 2.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.0\n",
      "self._total_profit:  -0.0005999999999999339\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 2.00000e-05]\n",
      " [1.06972e+00 0.00000e+00]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.5\n",
      "self._total_profit:  -0.5008999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.0, 'total_profit': -0.0005999999999999339, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 0.00000e+00]\n",
      " [1.06975e+00 3.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.0\n",
      "self._total_profit:  -1.0011999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.5, 'total_profit': -0.5008999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06975e+00  3.00000e-05]\n",
      " [ 1.06917e+00 -5.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.5\n",
      "self._total_profit:  -1.5014999999999998\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.0, 'total_profit': -1.0011999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06917e+00 -5.80000e-04]\n",
      " [ 1.06899e+00 -1.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.0\n",
      "self._total_profit:  -2.0018\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.5, 'total_profit': -1.5014999999999998, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06899e+00 -1.80000e-04]\n",
      " [ 1.06788e+00 -1.11000e-03]]\n",
      "q_values_cpu:  tensor([-0.0143, -0.0635,  0.0663])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.5\n",
      "self._total_profit:  -2.5020999999999995\n",
      "reward: -0.5\n",
      "info: {'total_reward': -3.0, 'total_profit': -2.0018, 'position': <Positions.Hold: 2>}\n",
      "state_info:  (array([[ 1.06966e+00, -4.00000e-05],\n",
      "       [ 1.06970e+00,  4.00000e-05]], dtype=float32), {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06970e+00 4.00000e-05]\n",
      " [1.06972e+00 2.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.0\n",
      "self._total_profit:  -0.0005999999999999339\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 2.00000e-05]\n",
      " [1.06972e+00 0.00000e+00]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.5\n",
      "self._total_profit:  -0.5008999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.0, 'total_profit': -0.0005999999999999339, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 0.00000e+00]\n",
      " [1.06975e+00 3.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.0\n",
      "self._total_profit:  -1.0011999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.5, 'total_profit': -0.5008999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06975e+00  3.00000e-05]\n",
      " [ 1.06917e+00 -5.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.5\n",
      "self._total_profit:  -1.5014999999999998\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.0, 'total_profit': -1.0011999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06917e+00 -5.80000e-04]\n",
      " [ 1.06899e+00 -1.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.0\n",
      "self._total_profit:  -2.0018\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.5, 'total_profit': -1.5014999999999998, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06899e+00 -1.80000e-04]\n",
      " [ 1.06788e+00 -1.11000e-03]]\n",
      "q_values_cpu:  tensor([-0.0143, -0.0635,  0.0663])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.5\n",
      "self._total_profit:  -2.5020999999999995\n",
      "reward: -0.5\n",
      "info: {'total_reward': -3.0, 'total_profit': -2.0018, 'position': <Positions.Hold: 2>}\n",
      "state_info:  (array([[ 1.06966e+00, -4.00000e-05],\n",
      "       [ 1.06970e+00,  4.00000e-05]], dtype=float32), {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06970e+00 4.00000e-05]\n",
      " [1.06972e+00 2.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.0\n",
      "self._total_profit:  -0.0005999999999999339\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 2.00000e-05]\n",
      " [1.06972e+00 0.00000e+00]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.5\n",
      "self._total_profit:  -0.5008999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.0, 'total_profit': -0.0005999999999999339, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 0.00000e+00]\n",
      " [1.06975e+00 3.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.0\n",
      "self._total_profit:  -1.0011999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.5, 'total_profit': -0.5008999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06975e+00  3.00000e-05]\n",
      " [ 1.06917e+00 -5.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.5\n",
      "self._total_profit:  -1.5014999999999998\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.0, 'total_profit': -1.0011999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06917e+00 -5.80000e-04]\n",
      " [ 1.06899e+00 -1.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.0\n",
      "self._total_profit:  -2.0018\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.5, 'total_profit': -1.5014999999999998, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06899e+00 -1.80000e-04]\n",
      " [ 1.06788e+00 -1.11000e-03]]\n",
      "q_values_cpu:  tensor([-0.0143, -0.0635,  0.0663])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.5\n",
      "self._total_profit:  -2.5020999999999995\n",
      "reward: -0.5\n",
      "info: {'total_reward': -3.0, 'total_profit': -2.0018, 'position': <Positions.Hold: 2>}\n",
      "state_info:  (array([[ 1.06966e+00, -4.00000e-05],\n",
      "       [ 1.06970e+00,  4.00000e-05]], dtype=float32), {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06970e+00 4.00000e-05]\n",
      " [1.06972e+00 2.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.0\n",
      "self._total_profit:  -0.0005999999999999339\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 2.00000e-05]\n",
      " [1.06972e+00 0.00000e+00]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.5\n",
      "self._total_profit:  -0.5008999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.0, 'total_profit': -0.0005999999999999339, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 0.00000e+00]\n",
      " [1.06975e+00 3.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.0\n",
      "self._total_profit:  -1.0011999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.5, 'total_profit': -0.5008999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06975e+00  3.00000e-05]\n",
      " [ 1.06917e+00 -5.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.5\n",
      "self._total_profit:  -1.5014999999999998\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.0, 'total_profit': -1.0011999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06917e+00 -5.80000e-04]\n",
      " [ 1.06899e+00 -1.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.0\n",
      "self._total_profit:  -2.0018\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.5, 'total_profit': -1.5014999999999998, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06899e+00 -1.80000e-04]\n",
      " [ 1.06788e+00 -1.11000e-03]]\n",
      "q_values_cpu:  tensor([-0.0143, -0.0635,  0.0663])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.5\n",
      "self._total_profit:  -2.5020999999999995\n",
      "reward: -0.5\n",
      "info: {'total_reward': -3.0, 'total_profit': -2.0018, 'position': <Positions.Hold: 2>}\n",
      "state_info:  (array([[ 1.06966e+00, -4.00000e-05],\n",
      "       [ 1.06970e+00,  4.00000e-05]], dtype=float32), {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06970e+00 4.00000e-05]\n",
      " [1.06972e+00 2.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.0\n",
      "self._total_profit:  -0.0005999999999999339\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 2.00000e-05]\n",
      " [1.06972e+00 0.00000e+00]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.5\n",
      "self._total_profit:  -0.5008999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.0, 'total_profit': -0.0005999999999999339, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 0.00000e+00]\n",
      " [1.06975e+00 3.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.0\n",
      "self._total_profit:  -1.0011999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.5, 'total_profit': -0.5008999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06975e+00  3.00000e-05]\n",
      " [ 1.06917e+00 -5.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.5\n",
      "self._total_profit:  -1.5014999999999998\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.0, 'total_profit': -1.0011999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06917e+00 -5.80000e-04]\n",
      " [ 1.06899e+00 -1.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.0\n",
      "self._total_profit:  -2.0018\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.5, 'total_profit': -1.5014999999999998, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06899e+00 -1.80000e-04]\n",
      " [ 1.06788e+00 -1.11000e-03]]\n",
      "q_values_cpu:  tensor([-0.0143, -0.0635,  0.0663])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.5\n",
      "self._total_profit:  -2.5020999999999995\n",
      "reward: -0.5\n",
      "info: {'total_reward': -3.0, 'total_profit': -2.0018, 'position': <Positions.Hold: 2>}\n",
      "state_info:  (array([[ 1.06966e+00, -4.00000e-05],\n",
      "       [ 1.06970e+00,  4.00000e-05]], dtype=float32), {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06970e+00 4.00000e-05]\n",
      " [1.06972e+00 2.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.0\n",
      "self._total_profit:  -0.0005999999999999339\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 2.00000e-05]\n",
      " [1.06972e+00 0.00000e+00]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.5\n",
      "self._total_profit:  -0.5008999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.0, 'total_profit': -0.0005999999999999339, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 0.00000e+00]\n",
      " [1.06975e+00 3.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.0\n",
      "self._total_profit:  -1.0011999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.5, 'total_profit': -0.5008999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06975e+00  3.00000e-05]\n",
      " [ 1.06917e+00 -5.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.5\n",
      "self._total_profit:  -1.5014999999999998\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.0, 'total_profit': -1.0011999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06917e+00 -5.80000e-04]\n",
      " [ 1.06899e+00 -1.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.0\n",
      "self._total_profit:  -2.0018\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.5, 'total_profit': -1.5014999999999998, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06899e+00 -1.80000e-04]\n",
      " [ 1.06788e+00 -1.11000e-03]]\n",
      "q_values_cpu:  tensor([-0.0143, -0.0635,  0.0663])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.5\n",
      "self._total_profit:  -2.5020999999999995\n",
      "reward: -0.5\n",
      "info: {'total_reward': -3.0, 'total_profit': -2.0018, 'position': <Positions.Hold: 2>}\n",
      "state_info:  (array([[ 1.06966e+00, -4.00000e-05],\n",
      "       [ 1.06970e+00,  4.00000e-05]], dtype=float32), {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06970e+00 4.00000e-05]\n",
      " [1.06972e+00 2.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.0\n",
      "self._total_profit:  -0.0005999999999999339\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 2.00000e-05]\n",
      " [1.06972e+00 0.00000e+00]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.5\n",
      "self._total_profit:  -0.5008999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.0, 'total_profit': -0.0005999999999999339, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[1.06972e+00 0.00000e+00]\n",
      " [1.06975e+00 3.00000e-05]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.0\n",
      "self._total_profit:  -1.0011999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.5, 'total_profit': -0.5008999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06975e+00  3.00000e-05]\n",
      " [ 1.06917e+00 -5.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -2.5\n",
      "self._total_profit:  -1.5014999999999998\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.0, 'total_profit': -1.0011999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06917e+00 -5.80000e-04]\n",
      " [ 1.06899e+00 -1.80000e-04]]\n",
      "q_values_cpu:  tensor([-0.0144, -0.0635,  0.0664])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.0\n",
      "self._total_profit:  -2.0018\n",
      "reward: -0.5\n",
      "info: {'total_reward': -2.5, 'total_profit': -1.5014999999999998, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [[ 1.06899e+00 -1.80000e-04]\n",
      " [ 1.06788e+00 -1.11000e-03]]\n",
      "q_values_cpu:  tensor([-0.0143, -0.0635,  0.0663])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -3.5\n",
      "self._total_profit:  -2.5020999999999995\n",
      "reward: -0.5\n",
      "info: {'total_reward': -3.0, 'total_profit': -2.0018, 'position': <Positions.Hold: 2>}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "def epsilon_greedy_policy(model, epsilon, num_actions, device):\n",
    "    def policy_fn(state_info):\n",
    "        print(\"state_info: \", state_info)\n",
    "        state_array = state_info[0] if isinstance(state_info, tuple) else state_info  # Extract the array from the tuple if needed\n",
    "        state_array = state_array[0]\n",
    "        # print(\"state_array: \", state_array)\n",
    "        state_tensor = torch.tensor(state_array, dtype=torch.float32, device=device)\n",
    "        # print(\"state_tensor: \", state_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = model(state_tensor)\n",
    "            \n",
    "        \n",
    "        if q_values.numel() == 0:  # Check if q_values tensor is empty\n",
    "            return np.random.choice(num_actions)\n",
    "        else:\n",
    "            q_values_cpu = q_values.cpu()\n",
    "            print(\"q_values_cpu: \", q_values_cpu)\n",
    "            print(\"torch.argmax(q_values).item()\", torch.argmax(q_values_cpu).item())\n",
    "            return torch.argmax(q_values_cpu).item()\n",
    "    return policy_fn\n",
    "\n",
    "\n",
    "def train_dqn(env, model, target_model, optimizer, gamma, batch_size, device, epsilon):\n",
    "    state_info = env.reset()  # Ensure env.reset() returns a tuple (state, info) or just the state\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(model, epsilon, env.action_space.n, device)(state_info)\n",
    "        print(\"action from training: \", action)\n",
    "        step_result = env.step(action)\n",
    "        # print(\"step_result: \",step_result)\n",
    "        next_state, reward, done, truncate, info = step_result[:5]  # Extract the first four values\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # # print(\"Shapes before converting to PyTorch tensors:\")\n",
    "        # if isinstance(state_info, tuple) and len(state_info) == 2:\n",
    "        #     print(\"state shape:\", state_info[0].shape)  # Access the state from the state_info tuple\n",
    "        # else:\n",
    "        #     print(\"state shape:\", state_info.shape)  # If state_info is not a tuple, assume it's the state itself\n",
    "        \n",
    "        # print(\"next_state:\", next_state)\n",
    "        # print(\"action:\", action)\n",
    "        print(\"reward:\", reward)\n",
    "        print(\"info:\", info)\n",
    "        \n",
    "        # if len(next_state) > 0:  # Check if next_state is not empty\n",
    "        #     print(\"next_state shape:\", next_state[0].shape)\n",
    "        # else:\n",
    "        #     print(\"next_state is empty\")\n",
    "        \n",
    "        # print(\"done:\", done)\n",
    "\n",
    "        done = done or truncate\n",
    "\n",
    "        state_info = next_state  # Update state_info\n",
    "        \n",
    "    return episode_reward\n",
    "\n",
    "\n",
    "# Parameters\n",
    "input_dim = 2  # Adjusted input dimensions\n",
    "output_dim = env.action_space.n\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "epsilon = 0.1\n",
    "lr = 1e-3\n",
    "target_update = 10\n",
    "num_episodes = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize DQN and target DQN\n",
    "model = DQN(input_dim, output_dim).to(device)\n",
    "target_model = DQN(input_dim, output_dim).to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "target_model.eval()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    # print(episode)\n",
    "    episode_reward = train_dqn(env, model, target_model, optimizer, gamma, batch_size, device, epsilon)\n",
    "    \n",
    "    if episode % target_update == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "        target_model.eval()\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "# After training, you can use the model for inference as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9997\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9994000000000001\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9997, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9991000000000001\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9994000000000001, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9988000000000001\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9991000000000001, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9985000000000002\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9988000000000001, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9982000000000002\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9985000000000002, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9979000000000002\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9982000000000002, 'position': <Positions.Hold: 2>} 0 2\n",
      "Episode 0, Reward: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape of LSTM layer should be (batch_size, seq_len, input_size)\n",
    "        # Assuming batch size is 1 for reinforcement learning\n",
    "        x, _ = self.lstm(x.unsqueeze(0))\n",
    "        x = x.view(-1, self.lstm.hidden_size)  # Reshape to (batch_size, hidden_dim)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def epsilon_greedy_policy(model, epsilon, num_actions, device):\n",
    "    def policy_fn(state_info):\n",
    "        state_array = state_info[0] if isinstance(state_info, tuple) else state_info  # Extract the array from the tuple if needed\n",
    "        state_array = state_array[0]\n",
    "        state_tensor = torch.tensor(state_array, dtype=torch.float32, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = model(state_tensor)\n",
    "            \n",
    "        if q_values.numel() == 0:  # Check if q_values tensor is empty\n",
    "            return np.random.choice(num_actions)\n",
    "        else:\n",
    "            q_values_cpu = q_values.cpu()\n",
    "            return torch.argmax(q_values_cpu).item()\n",
    "    return policy_fn\n",
    "\n",
    "def train_dqn(env, model, target_model, optimizer, gamma, batch_size, device, epsilon):\n",
    "    state_info = env.reset()  # Ensure env.reset() returns a tuple (state, info) or just the state\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(model, epsilon, env.action_space.n, device)(state_info)\n",
    "        step_result = env.step(action)\n",
    "        next_state, reward, done, truncate, info = step_result[:5]  # Extract the first four values\n",
    "        episode_reward += reward\n",
    "        \n",
    "        done = done or truncate\n",
    "\n",
    "        print(\"Info: \", info, reward, action)\n",
    "        state_info = next_state  # Update state_info\n",
    "        \n",
    "    return episode_reward\n",
    "\n",
    "# Parameters\n",
    "input_dim = 2  # Adjusted input dimensions\n",
    "output_dim = env.action_space.n\n",
    "hidden_dim = 64  # Dimensionality of the hidden state in LSTM\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "epsilon = 0.1\n",
    "lr = 1e-3\n",
    "target_update = 10\n",
    "num_episodes = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize DQN and target DQN\n",
    "model = DQN(input_dim, output_dim, hidden_dim).to(device)\n",
    "target_model = DQN(input_dim, output_dim, hidden_dim).to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "target_model.eval()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    print(episode)\n",
    "    episode_reward = train_dqn(env, model, target_model, optimizer, gamma, batch_size, device, epsilon)\n",
    "    \n",
    "    if episode % target_update == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "        target_model.eval()\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "# After training, you can use the model for inference as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n",
      "step_reward:  0\n",
      "self._total_reward:  0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_dqn(env, model, num_episodes, device):\n",
    "    episode_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            # print(state)\n",
    "            state_array = state[0] if isinstance(state, tuple) else state  # Extract the array from the tuple if needed\n",
    "            state_array = state_array[0]\n",
    "            state_tensor = torch.tensor(state_array, dtype=torch.float32, device=device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model.eval();\n",
    "                q_values = model(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "            next_state, reward, done, truncate, info = env.step(action)\n",
    "\n",
    "            # print(reward)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            done = done or truncate\n",
    "        episode_rewards.append(episode_reward)\n",
    "    return episode_rewards\n",
    "\n",
    "# Evaluation parameters\n",
    "num_eval_episodes = 100\n",
    "\n",
    "# Evaluate the model\n",
    "eval_rewards = evaluate_dqn(env, model, num_eval_episodes, device)\n",
    "\n",
    "# Plot the evaluation rewards\n",
    "plt.plot(eval_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DQN Evaluation Rewards')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM - DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './results/dqn_model_10000ep.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"dqn_stable_baseline3_eurusd_2023_1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABS4AAAI1CAYAAADYYGQEAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8g+/7EAAAACXBIWXMAAA9hAAAPYQGoP6dpAACPRUlEQVR4nOzdeVyU5d7H8e+wDTsqIksgrrlr5JaaW5FLZlnZcsyFcsktt6eNTtapzqHTYtlimi1iZp3UNC0TM5O01EwLE3fcRcAVEES2uZ8/kMkRVFBxWD7v12uemPu+7nt+Mw49x2+/67pMhmEYAgAAAAAAAIByxMHeBQAAAAAAAADAhQguAQAAAAAAAJQ7BJcAAAAAAAAAyh2CSwAAAAAAAADlDsElAAAAAAAAgHKH4BIAAAAAAABAuUNwCQAAAAAAAKDcIbgEAAAAAAAAUO4QXAIAAAAAAAAodwguAQCoQmJjY2UymRQbG2vvUsoFk8mkf/3rX/YuA1VQefzuzZkzR40bN5azs7OqVasmSerWrZu6detm17oAAEDVRXAJAEAZM5lMJXqUJEyMiorSN998U+Y1R0dH29Tm5OSkG264QREREUpMTCzz16/oUlNTNWLECPn5+cnDw0Pdu3fXH3/8UeLrLRaLpk+frptuuklubm7y9fXVbbfdps2bNxcZ9/rrr6tu3bpydXVVy5Yt9eWXXxZ7z+3bt6tXr17y9PRUjRo1NGjQIB07dqzIuKSkJI0YMUJ169aVm5ub6tevr0mTJunEiRM24zZs2KDRo0erdevWcnZ2lslkuuj7udh3/r///W+JP5Or1a1btxL9HpYkTPziiy80derUMq95//79NrU5Ojqqdu3auvfeexUXF3dNX2vHjh2KiIhQ/fr19dFHH2nmzJnFjjty5Ij+9a9/XZPXT0xM1IMPPqhq1arJ29tb99xzj/bu3Vuia3Nzc/XSSy+pXr16MpvNqlevnv79738rLy+vyNhNmzapV69e8vb2lpeXl3r06FFs/aW5pyT98ccfuvvuu1WjRg25u7urefPmevfdd4uMy8nJUVRUlBo3bixXV1f5+/urT58+Onz4sHXM1q1b9cADD6hevXpyd3dXzZo11aVLF3377bcl+jwAAKisnOxdAAAAld2cOXNsnn/22WdasWJFkeNNmjS57L2ioqLUv39/9evX71qWeFEvv/yy6tatq7Nnz2r9+vWKjo7WL7/8ovj4eLm6ul6XGioai8WiPn36aPPmzXrqqadUs2ZNffDBB+rWrZs2bdqkhg0bXvYejz32mObOnavBgwdr7NixyszM1J9//qmjR4/ajPvnP/+p//73vxo+fLjatm2rxYsXa8CAATKZTHr44Yet4w4fPqwuXbrIx8dHUVFRysjI0JtvvqktW7Zow4YNcnFxkSRlZGSoQ4cOyszM1OjRoxUSEqLNmzfr/fff16pVq7Rp0yY5OBT8d+/vv/9eH3/8sVq2bKl69epp165dl3xPd9xxhwYPHmxzLCwsrESf6bXwz3/+U8OGDbM+//333/Xuu+/queees/nda9my5WXv9cUXXyg+Pl4TJkwoi1KL+Mc//qE777xT+fn52r59u6ZPn65ly5Zp/fr1uummm67Ja8TGxspiseidd95RgwYNrMd/+OEHm3FHjhzRSy+9pDp16lzVa2dkZKh79+5KS0vTc889J2dnZ7399tvq2rWr4uLi5Ovre8nrBw4cqPnz5+uxxx5TmzZttH79ek2ePFkHDx60CV3/+OMP3XrrrQoJCdGLL74oi8WiDz74QF27dtWGDRvUqFGjUt+z8HPp27evwsLCNHnyZHl6emrPnj02YaRUEIb26dNHa9eu1fDhw9WyZUudOnVKv/32m9LS0hQcHCxJOnDggE6fPq0hQ4YoKChIZ86c0ddff627775bH374oUaMGHHFnzUAABWaAQAArqsxY8YYV/r/gj08PIwhQ4Zc8WuvWrXKkGSsWrXqkuNmzZplSDJ+//13m+PPPPOMIcn46quvrriG6ykjI+OS5yUZL7744jV9za+++sqQZMyfP9967OjRo0a1atWMf/zjHyW+fuHChZccd/jwYcPZ2dkYM2aM9ZjFYjE6d+5sBAcHG3l5edbjo0aNMtzc3IwDBw5Yj61YscKQZHz44YfWY3PnzjUkGd99953Na73wwguGJOOPP/6wHktOTjbOnDljGMblv9OSbOosD+bPn1+i34Xi9OnTxwgNDb2q1y/Jd2/fvn2GJOONN96wOb5kyRJDkjFixIiLXnu57/6FXnrpJUOScezYsUuO+/333w1JxqxZs0p1/wu99tprhiRjw4YN1mPbt283HB0djcjIyEteu2HDBkOSMXnyZJvj//d//2eYTCZj8+bN1mN33nmnUb16deP48ePWY0eOHDE8PT2N++6774rumZaWZvj7+xv33nuvkZ+ff9n36ezsbPz222+XHFecvLw8o1WrVkajRo1KfS0AAJUFU8UBACgHMjMz9X//938KCQmR2WxWo0aN9Oabb8owDOsYk8mkzMxMzZ492zp1NCIiQlJBt87o0aPVqFEj69TiBx54QPv377+mdXbu3FmStGfPHpvjO3bsUP/+/VWjRg25urqqTZs2WrJkifV8amqqHB0dbaZRHj9+XA4ODvL19bV5n6NGjVJAQID1+Zo1a/TAAw+odu3aMpvNCgkJ0cSJE5WVlWVTQ0REhLXr6c4775SXl5ceeeQRSVJ2drYmTpwoPz8/eXl56e677y7SGXX+ezl48OAVfkLSggUL5O/vr/vuu896zM/PTw8++KAWL16s7OzsS17/1ltvqV27drr33ntlsViUmZlZ7LjFixcrNzdXo0ePth4zmUwaNWqUDh8+rHXr1lmPf/3117rrrrtUu3Zt67Hw8HDdeOONmjdvnvVYenq6JMnf39/mtQIDAyVJbm5u1mP+/v42z0siKytLZ8+eLdU119sHH3ygZs2ayWw2KygoSGPGjFFqaqr1fLdu3bR06VIdOHDA+ntYp04dSQVTgl944QW1bt1aPj4+8vDwUOfOnbVq1aprWuNtt90mSdq3b5+kv5d2+PnnnzV69GjVqlXL2slXkvdUp04dvfjii5IKvqvnT5k/f43L2NhYtW3bVpL06KOPWt9/dHS0JOnMmTPasWOHjh8/ftn3sGDBArVt29Z6P0lq3Lixbr/9dpvvZHHWrFkjSTZdxYXPDcPQV199ZTM2PDzcpoMzMDBQXbt21XfffaeMjIxS3/OLL75QSkqK/vOf/8jBwUGZmZmyWCxF6izsYL333nvVrl075eXl6cyZM5d8b+dzdHRUSEiIzZ8VAABVDcElAAB2ZhiG7r77br399tvq1auX3nrrLTVq1EhPPfWUJk2aZB03Z84cmc1mde7cWXPmzNGcOXP0+OOPSyqY9rp27Vo9/PDDevfddzVy5EitXLlS3bp1K9VflC+nMAitXr269djWrVt1yy23aPv27Xr22Wc1ZcoUeXh4qF+/flq0aJEkqVq1amrevLlWr15tve6XX36RyWTSyZMntW3bNuvxNWvWWANSSZo/f77OnDmjUaNG6b333lPPnj313nvvFZl2LEl5eXnq2bOnatWqpTfffFP333+/JGnYsGGaOnWqevToof/+979ydnZWnz59in2PTZo0KfbeJfXnn3/q5ptvtk6pLtSuXTudOXPmklOq09PTtWHDBrVt21bPPfecfHx85OnpqXr16hUJc/788095eHgUWWKgXbt21vNSwTqCR48eVZs2bYq8Xrt27azjJKlLly5ycHDQ+PHjtX79eh0+fFjff/+9/vOf/6hfv35q3Lhx6T6M80RHR8vDw0Nubm5q2rSpvvjiiyu+V1n517/+pTFjxigoKEhTpkzR/fffrw8//FA9evRQbm6upIIp5zfddJNq1qxp/T0sXO8yPT1dH3/8sbp166bXXntN//rXv3Ts2DH17Nnzmq5JWfgfDi6cTj169Ght27ZNL7zwgp599tkSv6epU6fq3nvvlSRNnz5dc+bMsQneCzVp0kQvv/yyJGnEiBHW99+lSxdJBeueNmnSRO+///4l67dYLPrrr78u+p3cs2ePTp8+fdHrC8P/C4Nzd3d3SQVrWp4/triA3d3dXTk5OYqPjy/1PX/88Ud5e3srMTFRjRo1kqenp7y9vTVq1CibYH7btm06cuSIWrZsqREjRsjDw0MeHh5q2bLlRcPszMxMHT9+XHv27NHbb7+tZcuW6fbbb7/oZwEAQKVn135PAACqoAun1X7zzTeGJOPf//63zbj+/fsbJpPJSEhIsB672FTxwim751u3bp0hyfjss8+sx0o7VfzHH380jh07Zhw6dMhYsGCB4efnZ5jNZuPQoUPWsbfffrvRokUL4+zZs9ZjFovF6Nixo9GwYUOb9+3v7299PmnSJKNLly5GrVq1jOnTpxuGYRgnTpwwTCaT8c4771zyvb366quGyWSymfo8ZMgQQ5Lx7LPP2oyNi4szJBmjR4+2OT5gwIBip+tKMrp27XrJz+dSPDw8jMcee6zI8aVLlxqSjJiYmIte+8cffxiSDF9fX8Pf39/44IMPjLlz5xrt2rUzTCaTsWzZMuvYPn36GPXq1Styj8zMTJvPoXBq7/nfg0JPPfWUIcnmz+7jjz82qlWrZkiyPoYMGWLk5uZetO7LTRXv2LGjMXXqVGPx4sXG9OnTjebNmxuSjA8++OCi1xQnIyPjonXk5uYamZmZJb7XhVPFjx49ari4uBg9evSwmf77/vvvG5KMTz/91HrsYlPF8/LyjOzsbJtjp06dMvz9/Yt8J4r77l2ocKr4Sy+9ZBw7dsxITk42YmNjjbCwMEOS8fXXXxuG8ffv66233mqzREBp3tOLL75Y7FTxrl272vw+XGqqeOG/Xy73vo4dO2ZIMl5++eUi56ZNm2ZIMnbs2HHR67/++mtDkjFnzhyb4zNmzDAkGc2bN7cea9GihXHjjTfafC7Z2dlG7dq1DUnGggULSn3Pli1bGu7u7oa7u7vxxBNPGF9//bXxxBNPGJKMhx9+2Dpu4cKF1t/nhg0bGrNmzTJmzZplNGzY0HBxcbGZfl7o8ccft/7eOTg4GP379zdOnjx50c8CAIDKjo5LAADs7Pvvv5ejo6PGjRtnc/z//u//ZBiGli1bdtl7nN8llJubqxMnTqhBgwaqVq1aqXazvlB4eLj8/PwUEhKi/v37y8PDQ0uWLLFOQz158qR++uknPfjggzp9+rSOHz+u48eP68SJE+rZs6d2795t3YW8c+fOSklJ0c6dOyUVdFZ26dJFnTt3tk7T/OWXX2QYhk3H5fnvrbAbqWPHjjIMw6ZbsNCoUaNsnn///feSVOTzvdjGKoZhlGiH94vJysqS2WwucrxwM6MLp7ifr3Da6okTJ7R48WKNGjVKAwYM0MqVK+Xr66t///vfpX6dwn+WtKYbbrhB7dq109SpU7Vo0SJNmjRJc+fOtXbwXYlff/1V48eP1913362RI0dq06ZNat68uZ577rlLfh5SQXfehx9+qMaNG8vT01Ourq7q1KmT3njjDW3atEkHDhzQvHnz1KZNm8tuEHQpP/74o3JycjRhwgSbbtnhw4fL29tbS5cuvew9HB0drRsdWSwWnTx5Unl5eWrTps1V/R6++OKL8vPzU0BAgLp166Y9e/botddeK9IVOXz4cDk6Ol7T91Qa3bp1k2EYl92ZvbTfyQvdeeedCg0N1ZNPPqmFCxdavwP//Oc/5eTkZHPt6NGjtWvXLg0dOlTbtm1TfHy8Bg8erKSkJJvXKc09MzIydObMGQ0ePFjvvvuu7rvvPr377rt6/PHH9b///U+7d++2jpOk06dPa+XKlYqIiFBERIR+/PFHGYah119/vch7mzBhglasWKHZs2erd+/eys/PV05OziU/TwAAKjOCSwAA7OzAgQMKCgqSl5eXzfHCKcAHDhy47D2ysrL0wgsvWNfIrFmzpvz8/JSamqq0tLQrrm3atGlasWKFFixYoDvvvFPHjx+3CRsSEhJkGIYmT54sPz8/m0fhmnmFO2EXhpFr1qyx7pLduXNndenSxRpcrlmzRt7e3mrVqpX1NQ4ePKiIiAjVqFFDnp6e8vPzU9euXSWpyHtzcnKyWdtPKvj8HBwcVL9+fZvj5+8mXFo5OTlKTk62eeTn50sqCFqLW8eycArppdaFLDxXt25dtW/f3nrc09NTffv21YYNG5SXl1eq1yn8Z0nG/vrrr7rrrrv0n//8R+PHj1e/fv00ZcoUPf/883rrrbdspvRfDRcXF40dO1apqak2U3CLs2bNGr3wwgt65JFH9O2332rGjBkKDAzUyy+/rDZt2qhOnTqKiIhQeHj4VU1lL/w9u/B74eLionr16pXo91CSZs+erZYtW8rV1VW+vr7y8/PT0qVLr+r3cMSIEVqxYoVWrlypTZs26ejRo3r66aeLjKtbt67N82v1nq610nwni+Pq6qqlS5fK19dX999/v+rUqaPBgwfrhRdesP57otDIkSP13HPP6YsvvlCzZs3UokUL7dmzx/r5FY4tzT0La/vHP/5hU9eAAQMkybq+bOG4Tp06KSQkxDqudu3auvXWW7V27doi761x48YKDw/X4MGDrWtw9u3b12YdYAAAqhInexcAAACu3hNPPKFZs2ZpwoQJ6tChg3x8fGQymfTwww8Xu2lESbVr1866Dl2/fv106623asCAAdq5c6c8PT2t937yySfVs2fPYu/RoEEDSVJQUJDq1q2r1atXq06dOjIMQx06dJCfn5/Gjx+vAwcOaM2aNerYsaO1Oyw/P1933HGHTp48qWeeeUaNGzeWh4eHEhMTFRERUeS9mc3mImtLloW1a9eqe/fuNsf27dunOnXqKDAw0NrNdb7CY0FBQRe9b+G5CzfHkaRatWopNzdXmZmZ8vHxUWBgoFatWiXDMGQymS76OoUb61yspho1aljD6A8//FD+/v5F1h68++679a9//Utr165V06ZNL1p/aRQGOSdPnrzkuBtvvFE7duywWVd12LBhysnJ0ZYtW5Sdna2WLVvaBEv28vnnnysiIkL9+vXTU089pVq1asnR0VGvvvpqkQ2tSqNhw4YKDw+/7LjSbpZkL4XfuSv9PZGkZs2aKT4+Xtu2bdOpU6fUtGlTubm5aeLEidb/sFHoP//5j5588klt3bpVPj4+atGihZ577jlJBd+v0t4zKChIW7duLfJ7WqtWLUnSqVOnbN7DxX6fi+sYv1D//v31+OOPa9euXVf1H1sAAKioCC4BALCz0NBQ/fjjjzp9+rRN1+WOHTus5wudH1Cdb8GCBRoyZIimTJliPXb27NlruhttYQDTvXt3vf/++3r22WdVr149SZKzs3OJgpXOnTtr9erVqlu3rm666SZ5eXmpVatW8vHxUUxMjP744w+99NJL1vFbtmzRrl27NHv2bJsNc1asWFHiukNDQ2WxWLRnzx6bv/gXTlm/Eq1atSpSQ+FO6DfddJPWrFkji8ViE6L+9ttvcnd3twlKLhQUFKSAgADr9PrzHTlyRK6urtbvyE033aSPP/5Y27dvtwkTf/vtN+t5qWDqt5+fnzZu3Fjknhs2bLCOk6SUlBRr5+j5CjdxKez2vBb27t0rqWAX60spDF4v5OLiotatW1+zegp/z3bu3Gn9XksF3bX79u2z+X5f6vewXr16Wrhwoc2Ywu7j660076mkLvbeS8PBwUEtWrQo9jv522+/qV69ekU60C9WS7NmzazPv//+e1kslmLfV/Xq1XXrrbdan//4448KDg4u0qVbknu2bt1aK1assG7OU+jIkSOS/v5Ot2jRQs7Ozhf9fb7cd1/6eyr71XTsAgBQkTFVHAAAO7vzzjuVn59fZCfet99+WyaTSb1797Ye8/DwKDaMdHR0LDKV8L333is2hLoa3bp1s65/ePbsWdWqVUvdunXThx9+WGz31LFjx2yed+7cWfv379dXX31lnTru4OCgjh076q233lJubq7N+paF6/Wd/94Mw9A777xT4poLP793333X5njhTtAX2rFjhw4ePHjJe1avXl3h4eE2j8K1+fr376+UlBQtXLjQOv748eOaP3+++vbtazPVfs+ePUU68R566CEdOnTIJhg9fvy4Fi9erNtuu80aht5zzz1ydnbWBx98YB1nGIZmzJihG264QR07drQev//++/Xdd9/p0KFD1mMrV67Url279MADD1iP3XjjjUpJSSmyxueXX34pSQoLC7vk51KcC78DUsGaf1OnTlXNmjWvafh4NcLDw+Xi4qJ3333X5vv2ySefKC0tzWYXeg8Pj2KDpOK+r7/99pt16vD1Vpr3VFIeHh6SVOy/h86cOaMdO3bo+PHjl71P//799fvvv9uElzt37tRPP/1k852USvY7mZWVpcmTJyswMLDIFO4LffXVV/r999+LrP1Z0ns++OCDkgo+x/N9/PHHcnJyUrdu3SRJXl5euvPOO7V27Vrrf4iSpO3bt2vt2rW64447rMcKl9Q4X25urj777DO5ublds05nAAAqGjouAQCws759+6p79+765z//qf3796tVq1b64YcftHjxYk2YMMFmbcbWrVvrxx9/1FtvvWWdet2+fXvdddddmjNnjnx8fNS0aVOtW7dOP/74o3x9fa95vU899ZQeeOABRUdHa+TIkZo2bZpuvfVWtWjRQsOHD1e9evWUkpKidevW6fDhw9q8ebP12sJQcufOnYqKirIe79Kli5YtWyaz2ay2bdtajzdu3Fj169fXk08+qcTERHl7e+vrr7+2TsUsiZtuukn/+Mc/9MEHHygtLU0dO3bUypUrlZCQUOz4Jk2aqGvXrle8QU///v11yy236NFHH9W2bdtUs2ZNffDBB8rPz7fpJpWk22+/XZK0f/9+67HIyEjNmzdP999/vyZNmiQfHx/NmDFDubm5Np9ZcHCwJkyYoDfeeEO5ublq27atvvnmG61Zs0Zz58612aTlueee0/z589W9e3eNHz9eGRkZeuONN9SiRQs9+uij1nFjx47VrFmz1LdvXz3xxBMKDQ3Vzz//rC+//FJ33HGHzbqbBw4c0Jw5cyTJGj4Vbh4UGhqqQYMGSSpYJ/Wbb75R3759Vbt2bSUlJenTTz/VwYMHNWfOHOtmNvbm5+enyMhIvfTSS+rVq5fuvvtu7dy5Ux988IHatm2rgQMHWse2bt1aX331lSZNmqS2bdta1yC96667tHDhQt17773q06eP9u3bpxkzZqhp06bWjVrK63sqqfr166tatWqaMWOGvLy85OHhofbt26tu3brasGGDunfvrhdffPGyG/SMHj1aH330kfr06aMnn3xSzs7Oeuutt+Tv76//+7//sxlb3O/kgw8+qKCgIDVt2lTp6en69NNPtXfvXi1dutSmW3P16tV6+eWX1aNHD/n6+mr9+vWaNWuWevXqpfHjx9u8TknvGRYWpscee0yffvqp8vLyrLXNnz9fkZGRNtPco6KitHLlSt12223WDcLeffdd1ahRwzpdXZIef/xxpaenq0uXLrrhhhuUnJysuXPnaseOHZoyZUq5WAoBAAC7uO77mAMAUMWNGTPGuPD/BZ8+fdqYOHGiERQUZDg7OxsNGzY03njjDcNisdiM27Fjh9GlSxfDzc3NkGQMGTLEMAzDOHXqlPHoo48aNWvWNDw9PY2ePXsaO3bsMEJDQ61jDMMwVq1aZUgyVq1adckaZ82aZUgyfv/99yLn8vPzjfr16xv169c38vLyDMMwjD179hiDBw82AgICDGdnZ+OGG24w7rrrLmPBggVFrq9Vq5YhyUhJSbEe++WXXwxJRufOnYuM37ZtmxEeHm54enoaNWvWNIYPH25s3rzZkGTMmjXLOm7IkCGGh4dHse8nKyvLGDdunOHr62t4eHgYffv2NQ4dOmRIMl588UWbsZKMrl27XvLzuZyTJ08aQ4cONXx9fQ13d3eja9euxX6WoaGhRmhoaJHje/bsMe69917D29vbcHNzM2677TZjw4YNRcbl5+cbUVFRRmhoqOHi4mI0a9bM+Pzzz4utKT4+3ujRo4fh7u5uVKtWzXjkkUeM5OTkIuN27Nhh9O/f3wgJCTGcnZ2N0NBQ48knnzQyMzNtxhV+l4p7nP/5/fDDD8Ydd9xh/W5Uq1bN6NGjh7Fy5crLfIpla/78+cX+Lrz//vtG48aNDWdnZ8Pf398YNWqUcerUKZsxGRkZxoABA4xq1aoZkqx/hhaLxfrnYTabjbCwMOO7774zhgwZUuTPubjv3oX27dtnSDLeeOONS4671O9rSd/Tiy++aEgyjh07ZnO8a9euRX4fFi9ebDRt2tRwcnKy+T0s/E5c7n0VOnTokNG/f3/D29vb8PT0NO666y5j9+7dRcYV9zv52muvGY0bNzZcXV2N6tWrG3fffbfx559/Frk2ISHB6NGjh1GzZk3DbDYbjRs3Nl599VUjOzu7yNiS3tMwDCMnJ8f417/+ZYSGhhrOzs5GgwYNjLfffrvYsZs2bTLCw8MNDw8Pw8vLy7jnnnuMXbt22Yz58ssvjfDwcMPf399wcnIyqlevboSHhxuLFy8u9p4AAFQVJsNgizoAAAAAAAAA5QtrXAIAAAAAAAAodwguAQAAAAAAAJQ7BJcAAAAAAAAAyh2CSwAAAAAAAADlDsElAAAAAAAAgHKH4BIAAAAAAABAuUNwCQAAAAAAAKDcIbgEAAAAAAAAUO4QXAIAAAAAAAAodwguAQAAAAAAAJQ7BJcAAAAAAAAAyh2CSwAAAAAAAADlDsElAAAAAAAAgHKH4BIAAAAAAABAuUNwCQAAAAAAAKDcIbgEAAAAAAAAUO4QXAIAAAAAAAAodwguAQAAAAAAAJQ7BJcAAAAAAAAAyh2CSwAAAAAAAADlDsElAAAAAAAAgHKH4BIAAAAAAABAuUNwCQAAAAAAAKDcIbgEAAAAAAAAUO4QXAIAAAAAAAAodwguAQAAAAAAAJQ7BJcAAAAAAAAAyh2CSwAAAAAAAADlDsElAAAAAAAAgHKH4BIAAAAAAABAuUNwCQAAAAAAAKDcIbgEAAAAAAAAUO4QXAIAAAAAAAAodwguAQAAAAAAAJQ7BJcAAAAAAAAAyh2CSwAAAAAAAADlDsElAAAAAAAAgHKH4BIAAAAAAABAuUNwCQAAAAAAAKDcIbgEAAAAAAAAUO4QXAIAAAAAAAAodwguAQAAAAAAAJQ7BJcAAAAAAAAAyh0nexdQ0VgsFh05ckReXl4ymUz2LgcAAAAAAACoUAzD0OnTpxUUFCQHh4v3VRJcltKRI0cUEhJi7zIAAAAAAACACu3QoUMKDg6+6HmCy1Ly8vKSVPDBent727kaAAAAAAAAoGJJT09XSEiINWe7GILLUiqcHu7t7U1wCQAAAAAAAFyhyy3DyOY8AAAAAAAAAModgksAAAAAAAAA5Q7BJQAAAAAAAIByh+ASAAAAAAAAQLlDcAkAAAAAAACg3CG4BAAAAAAAAFDuEFwCAAAAAAAAKHcILgEAAAAAAACUOwSXAAAAAAAAAModgksAAAAAAAAA5Q7BJQAAAAAAAIByh+ASAAAAAAAAQLlDcAkAAAAAAACg3HGydwEAAAAArl6+JV9rDq5R0ukkBXoFqnPtznJ0cLR3WQAAAFes1B2Xq1evVt++fRUUFCSTyaRvvvnmstfExsbq5ptvltlsVoMGDRQdHV1kTGJiogYOHChfX1+5ubmpRYsW2rhxo/V8SkqKIiIiFBQUJHd3d/Xq1Uu7d++2nt+/f79MJlOxj/nz51vHFXf+f//7X2k/BgAAAKDcWLh9oeq8U0fdZ3fXgIUD1H12d9V5p44Wbl9o79IAAACuWKmDy8zMTLVq1UrTpk0r0fh9+/apT58+6t69u+Li4jRhwgQNGzZMy5cvt445deqUOnXqJGdnZy1btkzbtm3TlClTVL16dUmSYRjq16+f9u7dq8WLF+vPP/9UaGiowsPDlZmZKUkKCQlRUlKSzeOll16Sp6enevfubVPTrFmzbMb169evtB8DAAAAUC4s3L5Q/ef11+H0wzbHE9MT1X9ef8JLAABQYZkMwzCu+GKTSYsWLbpk8PfMM89o6dKlio+Ptx57+OGHlZqaqpiYGEnSs88+q19//VVr1qwp9h67du1So0aNFB8fr2bNmkmSLBaLAgICFBUVpWHDhhV7XVhYmG6++WZ98sknpar5UtLT0+Xj46O0tDR5e3tf0T0AAACAayHfkq8679QpEloWMsmkYO9g7Ru/j2njAACg3Chpvlbmm/OsW7dO4eHhNsd69uypdevWWZ8vWbJEbdq00QMPPKBatWopLCxMH330kfV8dna2JMnV1dV6zMHBQWazWb/88kuxr7tp0ybFxcVp6NChRc6NGTNGNWvWVLt27fTpp5/qUtltdna20tPTbR4AAABAWcrLtyjtTK4SU7O0M/m0Nh04qZ93HdPSv5L01e8H9fGavXrnx90a8dWci4aWkmTI0KH0Q1pzsPgGAQAAgPKszDfnSU5Olr+/v80xf39/paenKysrS25ubtq7d6+mT5+uSZMm6bnnntPvv/+ucePGycXFRUOGDFHjxo1Vu3ZtRUZG6sMPP5SHh4fefvttHT58WElJScW+7ieffKImTZqoY8eONsdffvll3XbbbXJ3d9cPP/yg0aNHKyMjQ+PGjSv2Pq+++qpeeumla/NhAAAAoNIyDENncvKVmZ2n09l5yjibZ/NzRvZ5j7MX+fnc86zc/BK9ZqbjNsnl8uOSThf/v5kBAADKs3Kxq7jFYlGbNm0UFRUlqWCKd3x8vGbMmKEhQ4bI2dlZCxcu1NChQ1WjRg05OjoqPDxcvXv3LrZbMisrS1988YUmT55c5Nz5x8LCwpSZmak33njjosFlZGSkJk2aZH2enp6ukJCQq33LAAAAKCey8/KVmZ2vjLN5Op2dWxA45uTp9LlAMfNcmGgNI887d344mZmdJ8sVL8JUPBcnB3mZneRhdpKn2Umerk5/P3d10rHsZvp45+XvE+gVeG0LAwAAuA7KPLgMCAhQSkqKzbGUlBR5e3vLzc1NkhQYGKimTZvajGnSpIm+/vpr6/PWrVsrLi5OaWlpysnJkZ+fn9q3b682bdoUec0FCxbozJkzGjx48GXra9++vV555RVlZ2fLbDYXOW82m4s9DgBARZFvydeag2uUdDpJgV6B6ly7M2vdVXJV4c8832IoM+fvbsXT5wLE4joYLzxXGDIWPs/Jt1zT2hxMkqfZSV6uzvIwO54LHJ3lWfiz2Vmerk7nnjvbhpFmJ3m5FvzTw+wkF6dLr+yUb2mqmHeeV2J6ogwVl5qaFOIdrM61O1/T9wgAAHA9lHlw2aFDB33//fc2x1asWKEOHTpYn3fq1Ek7d9r+p+Jdu3YpNDS0yP18fHwkSbt379bGjRv1yiuvFBnzySef6O6775afn99l64uLi1P16tUJJwEAldLC7Qs1Pma8zRp4wd7BeqfXO7qvyX12rAxlpTz/mRuGobO5FmtX46WmSl/u3Jmckk2lLg13l8Jg0elcsOhU5LnHecFikbHn/unm7CiTyXTN6yuOo4Oj3un1jvrP6y+TTLbhpVHwfya0/U+lC64BAEDVUOrgMiMjQwkJCdbn+/btU1xcnGrUqGFdhzIxMVGfffaZJGnkyJF6//339fTTT+uxxx7TTz/9pHnz5mnp0qXWe0ycOFEdO3ZUVFSUHnzwQW3YsEEzZ87UzJkzrWPmz58vPz8/1a5dW1u2bNH48ePVr18/9ejRw6a+hIQErV69ukhYKknffvutUlJSdMstt8jV1VUrVqxQVFSUnnzyydJ+DAAAlHsLty9U/3n9i3RhJaYnqv+8/lrw4AK7B1m4tsrqzzwnz2LtUDx9bqq0zdRpm3Ucc5WZnX/uecHPBdflKqMMplI7O5rO62x0lte5ALG47sVLnfM0O8nR4fqEjdfafU3u04IHFxQJrN0ca8kza5hmr/RX7/rpahJ48R07AQAAyiOTcakttYsRGxur7t27Fzk+ZMgQRUdHKyIiQvv371dsbKzNNRMnTtS2bdsUHBysyZMnKyIiwub67777TpGRkdq9e7fq1q2rSZMmafjw4dbz7777rt544w2lpKQoMDBQgwcP1uTJk+XiYrsa+XPPPafPP/9c+/fvl4OD7dSamJgYRUZGKiEhQYZhqEGDBho1apSGDx9eZOzFlHS7dgAA7Cnfkq8679S56G7DJpl0g9cN2jY6gU6sSiLfkq8mHzRQ4umL/5n7uQfqk56/6UyOYV23MeO89RmLm0Z9OjtPOXnXdiq16dxU6kt1N56/jmNhyOjhUnjuXFDp6iSzE9/fQhcuERDm30ERszbqz4Opqunpov+N6KAGtTztXSYAAECJ87VSB5dVHcElgAovP19as0ZKSpICA6XOnSVH/uJ/veXmW3Q2N19ZufnKzi34+WyuRWfz8pWVk1/wPK/gePa5cWcvGFfw8/nH//45JXuTtuVdfkaBf3aUXC0tr8M7Rlk76/CXUszPXXbc1fyZuzk7FgkZbaZOX2wK9QVhpLvL9ZtKXdWlZeVqwEfrtfVIuvy9zZr3eAeF+nrYuywAAFDFlTRfKxe7igMArpOFC6Xx46XD53VkBQdL77wj3Ve1pwzn5VusQWHRQPDcPwtDxTyLss8bk3X++LzigsYLfs6zKP9az5e9QKZjiuRy+XH5plNlWgeun5L+WYb65ahpNT+bwNF26nRBN2Phz56uTvJ0cZKH2VFOjiWboYLyw8fNWXOGttfDM9dpV0qGBnz0m+aP7KCgam72Lg0AAOCy6LgsJTouAVRYCxdK/ftLF/5rv7DracGCchVeWiyGTVB4qe7CwnHZF4w7P1DMPteheGHnYmHImFfGQeKluDk7ytXZQa7OjnJ1dpTZyUFuLo5ydbI97nr+uHPnCseZzxvn5uyoLcfXatj3fS/72jEDflSX0G5l/yZR5lYfiFWvL8IvO27VkFXqVqdb2ReEcuXo6bN66MP12nc8U3V83TXv8Q6q5e1q77IAAEAVxVTxMkJwCaBCys+X6tSx7bQ8n8lU0Hm5b99Fp41bLMbfwaB1OvPfU5azL+w8PDfubDHBY+G4bJspz7bHc/Kv7Zp6pWF2crAGgIVBodnZUa4lChTPXeviKPMF49wuCB7Nzg4yOzmUyZTZwjUuE9MTi2zUIhWsdxjsHax94/exxmUlwZ85LudIapYe/HCdDp/KUsNanvrfiFvk62m2d1kAAKAKYqo4rsj/NhzUvuOZ8nF3VnV3F1Vzc1Y1dxdVc3dWtXPHXJ35yw5QEeTkWZSalaPUM7nK/2mVmlwstJQKujAPHdJLT83QH/VaXbCGYkGgeK035ygNFycHayB4fgBoLgwNiwkUzRcGhc4OBeeLCR5tAsoyChKvN0cHR73T6x31n9dfJplsgiyTCt7f1F5TCbAqEf7McTlB1dz0xbBb9OCH67T7aIYGfbJBXw6/RT7uzvYuDQAAoFgEl7ARszVZsTuPXXKM2cmhINR0d5aPm7P1Z2vAWUzY6ePmTOAJXKHcfItSz+QqLStHp87kKvVMrlLPFASShcFk4c+nMnOVllVwPjMn33qPu7f9ondL8FonEg5os0udy45zcXQ4b2ryuUDwXAB4/pRla6B47mdzcZ2HlwkUzU4OcnCo+EGiPdzX5D4teHCBxseMt9ldPNg7WFN7TdV9TcrP0gC4Nvgzx+XU9nXX3OHt9dCH67UtKV2DZ23Q50PbycuV8BIAAJQ/TBUvpco+VXz+xkPamXxap86FJKlncnXqTM65ICT3qtaAc3N2vGTYWd3d5e9Oz3PHfNydZXYi8ETlkJdvUVpW7gW/X5cPITOy8674NR1MBRszdE3apqkzJl52/LqPF+hMx1utgaLZydEmeCwMFB0JEiuUfEu+1hxco6TTSQr0ClTn2p3puqvk+DPH5exMPq2HZ67TqTO5alenhmY/1k5uLnxHAADA9cEal2WksgeXl2IYhjKy82yClfM7v06dO5Z2LuxMzfq7M+xq9rxwd3G06eIsDDgvFnZWO9fh6eLEzqcoG/kWw9rVeGEImXbuWGrWBYFkZq5OX0UAaToXQBYE+i6qfmF3s5uzqnu42P6HATcXebk6FXQrFq5xmZhYdHOewhe4zBqXAIDKJT4xTf/4aL1On81T54Y19dHgNsyQAQAA1wXBZRmpysHllbJYDGXk5Ck18++ws7CL81TmxcPOtKzcqwo8Pc1OBUFPMWGntdvz3Pnzwx8nRwLPqiLfYig9q5iQsZgQ0vrzmRyln73yAFKSvF2dVN3D5eIhpM33s+C8l6vz1Xc5Fu4qLtmGl+V0V3EAQNnbdOCUBn3ym87k5Ov2xrU0fWBr/uMvAAAocwSXZYTg8vqxWAydPptXTNhpG3CmZv0dMqVmFUytvZpvtZfZybaL88KA84Kws7q7i7xdnQg87ej874rN1OsLwvALQ8j0s1f5XXF1snY2Fn4nqrtf0BV5QQjp43YNAsirsXChNH687e7iISHS1KmElgBQRa3bc0IRszYoO8+iO1sE6N2Hw/jfNQAAoEwRXJYRgsvyL99i6PTZ89YOPL+b7iJh56nMq++i83J1umTYWd2jIOA6f8d2b3uHWOWMYRhKP5untMI1Hs9c8Gd33vIEBVO0r113rrXT0a3oxlLVigkhfSpyd25+vrRmjZSUJAUGSp07Mz0cAKq4n3cd0/DZG5WTb9G9YTdoygOt2BgNAACUGYLLMkJwWXmdv27hpcLOwqDs1Lnzp68i8DSZJG/Xot2chYFZ9XPHzw87q7k7y9vVuVz/ZaK49VBtpl5fuEbqua7ItKxc5V9FAunh4mg73foyIaTPufPOFTWABADgGvpha7JGz/1DeRZD/2gXoqh7W8hkKr//ewMAAFRcBJdlhOASFyrcKfrCsPPUBQHn+T+nnrm6naILN2r5O4w7P+wsuk5i4RqfXmanUgWehmEoMye/SNejtVv1zN87ZJ8f6l6LHeiru9tOt/ZxO6/r8YKp2T7ndqtnB3oAAK7Ot5uPaPz//pTFkCI61tGLfZsSXgIAgGuupPma03WsCaiUnBwd5Otplq+nuVTX5RYGnpcIO4vr/MzMyZdhyHqsNBxM+nuq8wVT1jOz8y4IIQt+zs2/8gDS1dnBpuuxmpuLqnvYhpB//1zwT283Z3Y0BQDATvq2ClJ2nkVPzt+s6LX75ebiqKd7NiK8BAAAdkFwCdiJs6ODanqaVbOUgWdOnuW8ndiLmdp+3s/nr+N5JidfFkM6mZmjk5k5pXpNFyeHc5vOXDAN+9y6nTYhpMff4wggAQCoePq3DtbZ3Hw9/028psfukbuzo564vaG9ywIAAFUQwSVQwbg4OaiWl6tqebmW6rqzuflKPzel/e+d2QsCzvSzufIwOxXbHVnNzUWuzg50WgAAUIUMvCVUZ3Pz9e+l2zVlxS65OjtqeJd69i4LAABUMQSXQBXh6uwoV2dH1fIuXeAJAACqpmGd6+lsbr7e/GGX/vP9drk6O2hQhzr2LgsAAFQhbKULAAAAoFhjb2uoMd3rS5ImL96qeRsP2bkiAABQlRBcAgAAALioJ3s00mOd6kqSnvn6Ly2OS7RzRQAAoKoguAQAAABwUSaTSZPvaqJH2teWYUiT5m1WTHyyvcsCAABVAMElAAAAgEsymUx65Z7muu/mG5RvMfTEl38odudRe5cFAAAqOYJLAAAAAJfl4GDS6/e3VJ+WgcrNN/T4nE1am3Dc3mUBAIBKjOASAAAAQIk4OTpo6kM3KbyJv7LzLBr22UZt3H/S3mUBAIBKiuASAAAAQIk5Ozro/QFh6tywps7k5OvRWb/rr8Op9i4LAABUQgSXAAAAAErF1dlRMwe1Ufu6NXQ6O0+DPtmg7Unp9i4LAABUMgSXAAAAAErNzcVRn0S0VVjtakrLytWgT35TwtEMe5cFAAAqEYJLAAAAAFfE0+yk6EfbqVmQt45n5OiRj9frwIlMe5cFAAAqCYJLAAAAAFfMx81Zc4a2143+nkpJz9aAj35TYmqWvcsCAACVAMElAAAAgKtSw8NFnw9rr7o1PZSYmqVHPlqvo+ln7V0WAACo4AguAQAAAFy1Wl6umjusvYKru2n/iTN65OPfdCIj295lAQCACozgEgAAAMA1EVTNTV8Mu0UB3q7afTRDgz7ZoLQzufYuCwAAVFAElwAAAACumdq+7po7vL1qepq1LSldg2dt0OmzhJcAAKD0CC4BAAAAXFP1/Tw1d1h7VXd31uZDqRoavVFZOfn2LgsAAFQwBJcAAAAArrlGAV6aM7S9vFydtGH/SY2Ys1FncwkvAQBAyRFcAgAAACgTzW/wUfSj7eTu4qg1u49rzNw/lJNnsXdZAACggiC4BAAAAFBmWodW1ydD2srs5KCVO45qwld/Ki+f8BIAAFwewSUAAACAMtWhvq9mDm4jF0cHfb8lWU8t+EsWi2HvsgAAQDlHcAkAAACgzHW90U/vDwiTk4NJi/5M1D+/2SLDILwEAAAXR3AJAAAA4Lro0SxAbz90kxxM0pcbDumlb7cRXgIAgIsiuAQAAABw3fRtFaTX+7eSJEWv3a/Xl+8kvAQAAMUiuAQAAABwXfVvHax/92suSZoeu0fv/ZRg54oAAEB5RHAJAAAA4LobeEuonu/TRJL01opdmrl6j50rAgAA5Q3BJQAAAAC7GNa5np7scaMkKer7HZqzbr99CwIAAOUKwSUAAAAAuxl7W0ON6V5fkjR58VbN23jIzhUBAIDyguASAAAAgF092aORHutUV5L0zNd/aXFcop0rAgAA5QHBJQAAAAC7MplMmnxXEz3SvrYMQ5o0b7Ni4pPtXRYAALAzgksAAAAAdmcymfTKPc113803KN9i6Ikv/9CqnUftXRYAALCjUgeXq1evVt++fRUUFCSTyaRvvvnmstfExsbq5ptvltlsVoMGDRQdHV1kTGJiogYOHChfX1+5ubmpRYsW2rhxo/V8SkqKIiIiFBQUJHd3d/Xq1Uu7d++2uUe3bt1kMplsHiNHjrQZc/DgQfXp00fu7u6qVauWnnrqKeXl5ZX2YwAAAABwjTk4mPT6/S3Vp2WgcvMNjZyzSWsTjtu7LAAAYCelDi4zMzPVqlUrTZs2rUTj9+3bpz59+qh79+6Ki4vThAkTNGzYMC1fvtw65tSpU+rUqZOcnZ21bNkybdu2TVOmTFH16tUlSYZhqF+/ftq7d68WL16sP//8U6GhoQoPD1dmZqbN6w0fPlxJSUnWx+uvv249l5+frz59+ignJ0dr167V7NmzFR0drRdeeKG0HwMAAACAMuDk6KCpD92k8Cb+ys6zaNhnG7Vx/0l7lwUAAOzAZBiGccUXm0xatGiR+vXrd9ExzzzzjJYuXar4+HjrsYcfflipqamKiYmRJD377LP69ddftWbNmmLvsWvXLjVq1Ejx8fFq1qyZJMlisSggIEBRUVEaNmyYpIKOy5tuuklTp04t9j7Lli3TXXfdpSNHjsjf31+SNGPGDD3zzDM6duyYXFxcilyTnZ2t7Oxs6/P09HSFhIQoLS1N3t7eF/9wAAAAAFyxs7n5Gv7ZRq3ZfVxeZifNHd5eLYOr2bssAABwDaSnp8vHx+ey+VqZr3G5bt06hYeH2xzr2bOn1q1bZ32+ZMkStWnTRg888IBq1aqlsLAwffTRR9bzhcGhq6ur9ZiDg4PMZrN++eUXm3vPnTtXNWvWVPPmzRUZGakzZ87Y1NKiRQtraFlYS3p6urZu3Vps/a+++qp8fHysj5CQkCv4FAAAAACUhquzo2YOaqP2dWvodHaeBn2yQduT0u1dFgAAuI7KPLhMTk62CQolyd/fX+np6crKypIk7d27V9OnT1fDhg21fPlyjRo1SuPGjdPs2bMlSY0bN1bt2rUVGRmpU6dOKScnR6+99poOHz6spKQk630HDBigzz//XKtWrVJkZKTmzJmjgQMHXraWwnPFiYyMVFpamvVx6NChq/9QAAAAAFyWm4ujPoloq7Da1ZSWlatBn/ymhKMZ9i4LAABcJ072LkAqmPbdpk0bRUVFSZLCwsIUHx+vGTNmaMiQIXJ2dtbChQs1dOhQ1ahRQ46OjgoPD1fv3r11/kz3ESNGWH9u0aKFAgMDdfvtt2vPnj2qX7/+FdVmNptlNpuv7g0CAAAAuCKeZidFP9pOAz5ar61H0vXIx+s17/EOCvX1sHdpAACgjJV5x2VAQIBSUlJsjqWkpMjb21tubm6SpMDAQDVt2tRmTJMmTXTw4EHr89atWysuLk6pqalKSkpSTEyMTpw4oXr16l30tdu3by9JSkhIuGQthecAAAAAlD8+bs6aM7S9bvT3VEp6tgZ89JsSU7PsXRYAAChjZR5cdujQQStXrrQ5tmLFCnXo0MH6vFOnTtq5c6fNmF27dik0NLTI/Xx8fOTn56fdu3dr48aNuueeey762nFxcZIKgtHCWrZs2aKjR4/a1OLt7V0kOAUAAABQftTwcNHnw9qrbk0PJaZm6ZGP1uto+ll7lwUAAMpQqYPLjIwMxcXFWUPBffv2KS4uztodGRkZqcGDB1vHjxw5Unv37tXTTz+tHTt26IMPPtC8efM0ceJE65iJEydq/fr1ioqKUkJCgr744gvNnDlTY8aMsY6ZP3++YmNjtXfvXi1evFh33HGH+vXrpx49ekiS9uzZo1deeUWbNm3S/v37tWTJEg0ePFhdunRRy5YtJUk9evRQ06ZNNWjQIG3evFnLly/X888/rzFjxjAdHAAAACjnanm5au6w9gqu7qb9J87okY9/04mMbHuXBQAAyojJOH+RyBKIjY1V9+7dixwfMmSIoqOjFRERof379ys2NtbmmokTJ2rbtm0KDg7W5MmTFRERYXP9d999p8jISO3evVt169bVpEmTNHz4cOv5d999V2+88YZSUlIUGBiowYMHa/LkyXJxcZEkHTp0SAMHDlR8fLwyMzMVEhKie++9V88//7zNtuoHDhzQqFGjFBsbKw8PDw0ZMkT//e9/5eRUsuU+S7pdOwAAAICycejkGT0wY52S08+qaaC3vhx+i3zcne1dFgAAKKGS5mulDi6rOoJLAAAAwP72HsvQgx+u1/GMbLUKqabPh7aTlyvhJQAAFUFJ87UyX+MSAAAAAK61en6emjusvaq7O2vzoVQNjd6orJx8e5cFAACuIYJLAAAAABVSowAvzRnaXl6uTtqw/6SGf7ZRZ3MJLwEAqCwILgEAAABUWM1v8FH0o+3k7uKoXxKOa/TcP5STZ7F3WQAA4BoguAQAAABQobUOra5PhrSV2clBP+04qglf/am8fMJLAAAqOoJLAAAAABVeh/q+mjm4jVwcHfT9lmQ9teAvWSzsQwoAQEVGcAkAAACgUuh6o5/eHxAmJweTFv2ZqH9+s0WGQXgJAEBFRXAJAAAAoNLo0SxAbz90kxxM0pcbDumlb7cRXgIAUEERXAIAAACoVPq2CtLr/VtJkqLX7tfry3cSXgIAUAERXAIAAACodPq3Dta/+zWXJE2P3aP3fkqwc0UAAKC0CC4BAAAAVEoDbwnV832aSJLeWrFLM1fvsXNFAACgNAguAQAAAFRawzrX05M9bpQkRX2/Q3PW7bdvQQAAoMQILgEAAABUamNva6gx3etLkiYv3qp5Gw/ZuSIAAFASBJcAAAAAKr0nezTS0FvrSpKe+fovLY5LtHNFAADgcgguAQAAAFR6JpNJz/dpokfa15ZhSJPmbVZMfLK9ywIAAJdAcAkAAACgSjCZTHrlnua67+YblG8x9MSXf2jVzqP2LgsAAFwEwSUAAACAKsPBwaTX72+pPi0DlZtvaOScTVqbcNzeZQEAgGIQXAIAAACoUpwcHTT1oZsU3sRf2XkWDftsozbuP2nvsgAAwAUILgEAAABUOc6ODnp/QJg6N6ypMzn5enTW7/rrcKq9ywIAAOchuAQAAABQJbk6O2rmoDZqX7eGTmfnadAnG7Q9Kd3eZQEAgHMILgEAAABUWW4ujvokoq3CaldTWlauBn78mxKOZti7LAAAIIJLAAAAAFWcp9lJ0Y+2U7Mgb53IzNEjH6/XgROZ9i4LAIAqj+ASAAAAQJXn4+asOUPb60Z/T6WkZ2vAR78pMTXL3mUBAFClEVwCAAAAgKQaHi76fFh71a3pocTULD3y0XodTT9r77IAAKiyCC4BAAAA4JxaXq6aO6y9gqu7af+JM3rk4990IiPb3mUBAFAlEVwCAAAAwHmCqrnpy+G3KMDbVbuPZmjQJxuUdibX3mUBAFDlEFwCAAAAwAVCarjri+HtVdPTrG1J6Ro8a4NOnyW8BADgeiK4BAAAAIBi1PPz1Nxh7VXd3VmbD6VqaPRGncnJs3dZAABUGQSXAAAAAHARjQK8NGdoe3m5OmnD/pMa8dkmnc3Nt3dZAABUCQSXAAAAAHAJzW/wUfSj7eTu4qhfEo5r9Nw/lJNnsXdZAABUegSXAAAAAHAZrUOr65MhbWV2ctBPO45qwld/Ki+f8BIAgLJEcAkAAAAAJdChvq9mDm4jF0cHfb8lWU8t+EsWi2HvsgAAqLQILgEAAACghLre6Kf3B4TJycGkRX8m6p/fbJFhEF4CAFAWCC4BAAAAoBR6NAvQ2w/dJAeT9OWGQ3rp222ElwAAlAGCSwAAAAAopb6tgvR6/1aSpOi1+/VazE7CSwAArjGCSwAAAAC4Av1bB+vf/ZpLkmb8vEfv/ZRg54oAAKhcCC4BAAAA4AoNvCVUz/dpIkl6a8UuzVy9x84VAQBQeRBcAgAAAMBVGNa5np7scaMkKer7HZqzbr99CwIAoJIguAQAAACAqzT2toYa072+JGny4q2at/GQnSsCAKDiI7gEAAAAgGvgyR6NNPTWupKkZ77+S4vjEu1cEQAAFRvBJQAAAABcAyaTSc/3aaJH2teWYUiT5m1WTHyyvcsCAKDCIrgEAAAAgGvEZDLplXua676bb1C+xdATX/6hVTuP2rssAAAqJIJLAAAAALiGHBxMev3+lurTMlC5+YZGztmktQnH7V0WAAAVDsElAAAAAFxjTo4OmvrQTQpv4q/sPIuGfbZRG/eftHdZAABUKASXAAAAAFAGnB0d9P6AMHVuWFNncvL16Kzf9dfhVHuXBQBAhUFwCQAAAABlxNXZUTMHtVH7ujV0OjtPgz7ZoO1J6fYuCwCACqHUweXq1avVt29fBQUFyWQy6ZtvvrnsNbGxsbr55ptlNpvVoEEDRUdHFxmTmJiogQMHytfXV25ubmrRooU2btxoPZ+SkqKIiAgFBQXJ3d1dvXr10u7du63nT548qSeeeEKNGjWSm5ubateurXHjxiktLc3mdUwmU5HH//73v9J+DAAAAABQIm4ujvokoq3CaldTWlauBn78mxKOZti7LAAAyr1SB5eZmZlq1aqVpk2bVqLx+/btU58+fdS9e3fFxcVpwoQJGjZsmJYvX24dc+rUKXXq1EnOzs5atmyZtm3bpilTpqh69eqSJMMw1K9fP+3du1eLFy/Wn3/+qdDQUIWHhyszM1OSdOTIER05ckRvvvmm4uPjFR0drZiYGA0dOrRITbNmzVJSUpL10a9fv9J+DAAAAABQYp5mJ0U/2k7Ngrx1IjNHj3y8XgdOZNq7LAAAyjWTYRjGFV9sMmnRokWXDP6eeeYZLV26VPHx8dZjDz/8sFJTUxUTEyNJevbZZ/Xrr79qzZo1xd5j165datSokeLj49WsWTNJksViUUBAgKKiojRs2LBir5s/f74GDhyozMxMOTk5lbjmS0lPT5ePj4/S0tLk7e19RfcAAAAAUDWdzMzRwzPXaVdKhm6o5qZ5Izvohmpu9i4LAIDrqqT5Wpmvcblu3TqFh4fbHOvZs6fWrVtnfb5kyRK1adNGDzzwgGrVqqWwsDB99NFH1vPZ2dmSJFdXV+sxBwcHmc1m/fLLLxd97cI3XxhaFhozZoxq1qypdu3a6dNPP9Wlstvs7Gylp6fbPAAAAADgStTwcNHnw9qrbk0PJaZm6ZGP1uto+ll7lwUAQLlU5sFlcnKy/P39bY75+/srPT1dWVlZkqS9e/dq+vTpatiwoZYvX65Ro0Zp3Lhxmj17tiSpcePGql27tiIjI3Xq1Cnl5OTotdde0+HDh5WUlFTs6x4/flyvvPKKRowYYXP85Zdf1rx587RixQrdf//9Gj16tN57772L1v/qq6/Kx8fH+ggJCbmajwMAAABAFVfLy1Vzh7VXcHU37T9xRo98/JtOZGTbuywAAMqdMp8qfuONN+rRRx9VZGSk9dj333+vPn366MyZM3Jzc5OLi4vatGmjtWvXWseMGzdOv//+u7Uzc9OmTRo6dKg2b94sR0dHhYeHy8HBQYZhaNmyZTavmZ6erjvuuEM1atTQkiVL5OzsfNH6XnjhBc2aNUuHDh0q9nx2dra147Pw3iEhIUwVBwAAAHBVDp08owdmrFNy+lk1DfTWl8NvkY/7xf/uAgBAZVFupooHBAQoJSXF5lhKSoq8vb3l5lawlktgYKCaNm1qM6ZJkyY6ePCg9Xnr1q0VFxen1NRUJSUlKSYmRidOnFC9evVsrjt9+rR69eolLy8vLVq06JKhpSS1b99ehw8ftgknz2c2m+Xt7W3zAAAAAICrFVLDXV8Mb6+anmZtS0rX4FkbdPpsrr3LAgCg3Cjz4LJDhw5auXKlzbEVK1aoQ4cO1uedOnXSzp07bcbs2rVLoaGhRe7n4+MjPz8/7d69Wxs3btQ999xjPZeenq4ePXrIxcVFS5YssVkT82Li4uJUvXp1mc3m0r41AAAAALgq9fw8NXdYe1V3d9bmQ6kaGr1RZ3Ly7F0WAADlQqmDy4yMDMXFxSkuLk6StG/fPsXFxVm7IyMjIzV48GDr+JEjR2rv3r16+umntWPHDn3wwQeaN2+eJk6caB0zceJErV+/XlFRUUpISNAXX3yhmTNnasyYMdYx8+fPV2xsrPbu3avFixfrjjvuUL9+/dSjRw9Jf4eWmZmZ+uSTT5Senq7k5GQlJycrPz9fkvTtt9/q448/Vnx8vBISEjR9+nRFRUXpiSeeKP0nBwAAAADXQKMAL80Z2l5erk7asP+kRny2SWdz8+1dFgAAdlfqNS5jY2PVvXv3IseHDBmi6OhoRUREaP/+/YqNjbW5ZuLEidq2bZuCg4M1efJkRURE2Fz/3XffKTIyUrt371bdunU1adIkDR8+3Hr+3Xff1RtvvKGUlBQFBgZq8ODBmjx5slxcXC5Zl1QQrtapU0cxMTGKjIxUQkKCDMNQgwYNNGrUKA0fPlwODiXLcEs6Bx8AAAAASmPTgVMa9MlvOpOTr9sa19KMga3l4lTmk+QAALjuSpqvXdXmPFURwSUAAACAsrJuzwlFzNqg7DyL7mwRoHcfDpOTI+ElAKByKTeb8wAAAAAASqZDfV/NHNxGLo4O+n5Lsp5a8JcsFnpNAABVE8ElAAAAAJQjXW/007RHbpaTg0mL/kzUP7/ZIibKAQCqIoJLAAAAAChn7mjqr7cfukkOJunLDYf00rfbCC8BAFUOwSUAAAAAlEN9WwXp9f6tJEnRa/frtZidhJcAgCqF4BIAAAAAyqn+rYP1737NJUkzft6j935KsHNFAABcPwSXAAAAAFCODbwlVM/3aSJJemvFLs1cvcfOFQEAcH042bsAAAAAAMClDetcT2dz8/XmD7sU9f0OuTk7akD7EK05uEZJp5MU6BWozrU7y9HB0d6lAgBwzZgMFkkplfT0dPn4+CgtLU3e3t72LgcAAABAFfLG8h2atmqPzjisVb7PLJ08m2Q9F+wdrHd6vaP7mtxnxwoBALi8kuZrTBUHAAAAgAriyR6N1KFZgo65ROlkVpLNucT0RPWf118Lty+0U3UAAFxbBJcAAAAAUEFYDIt+Of6mZFLB4zyGCibTTYiZoHxL/vUvDgCAa4zgEgAAAAAqiDUH1+hw+uGLnjdk6FD6Ia05uOY6VgUAQNkguAQAAACACiLpdNLlB0n655LVmrP+gI6mny3jigAAKDvsKg4AAAAAFUSgV2CJxu1Jdtbkb+L1wuJ4ta5dXb2aB6hnswCF1HAv4woBALh22FW8lNhVHAAAAIC95FvyVeedOkpMT7SuaXk+k0wK8LxBk1uv1PKtxxR3KNXmfIsbfNSreYB6NQ9QfT/P61Q1AAC2SpqvEVyWEsElAAAAAHtauH2h+s/rL0k24aXp3G49Cx5coPua3CdJOpKapR+2JmtZfLJ+339SlvP+9nejv6d6NQtQr+aBahLoJZPpgt1+AAAoIwSXZYTgEgAAAIC9Ldy+UONjxtts1BPiHaKpvaZaQ8sLHc/I1optKVoWn6y1CceVd16KGerrfi7EDFCr4GpycCDEBACUHYLLMkJwCQAAAKA8yLfka83BNUo6naRAr0B1rt1Zjg6OJbo2LStXK7enKCY+WT/vOqbsPIv1XKCPq3qeCzHb1qkhR0JMAMA1RnBZRgguAQAAAFQmmdl5it15TDFbk/XT9hRl5uRbz/l6uKhHM3/1ah6oDvV85eLkYMdKAQCVBcFlGSG4BAAAAFBZnc3N1y+7jytma7JWbEtRWlau9Zy3q5PCm/irV/MAdbnRT67OJevuBADgQgSXZYTgEgAAAEBVkJtv0fq9JxQTn6zlW1N0PCPbes7dxVHdG9VSr+YB6t64ljzNTnasFABQ0RBclhGCSwAAAABVTb7F0KYDp86FmMlKTM2ynnNxclCXhjXVq3mgwpvUUjV3FztWCgCoCAguywjBJQAAAICqzDAMbUlM07L4ZMXEJ2vf8UzrOScHkzrU91Wv5gG6o6m/anm52rFSAEB5RXBZRgguAQAAAKCAYRjalZKhZfFJiolP1o7k09ZzJpPUNrSGejYv2KH8hmpudqwUAFCeEFyWEYJLAAAAACjevuOZiolPVkx8kjYfTrM51yrYRz2bB6h380DVrelhpwoBAOUBwWUZIbgEAAAAgMtLTM3S8nPTyX8/cFLn/82zcYCXejYLUO8WAWrk7yWTyWS/QgEA1x3BZRkhuAQAAACA0jl2Ols/bCsIMdftOaE8y99/Da1b06MgxGweoJbBPoSYAFAFEFyWEYJLAAAAALhyqWdy9OP2o4qJT9bq3ceUk2exngvycbVOJ28dWl2ODoSYAFAZEVyWEYJLAAAAALg2MrLztGrHUcVsTdaqHUd1Jiffeq6mp1k9mvmrd/MA3VLPV86ODnasFABwLRFclhGCSwAAAAC49s7m5mv1rmOK2ZqsH7elKP1snvWcj5uzwpsUhJi3NqwpV2dHO1YKALhaBJdlhOASAAAAAMpWTp5F6/aeUEx8slZsS9bxjBzrOQ8XR3VvXEu9mweqWyM/eZid7FgpAOBKEFyWEYJLAAAAALh+8i2GNu4/qWXxyVq+NVlJaWet58xODupyo596Nw/Q7Y395ePubMdKAQAlRXBZRgguAQAAAMA+LBZDfyWmaVl8kmLik3XgxBnrOScHkzo2qKlezQLUo5m/anqa7VgpAOBSCC7LCMElAAAAANifYRjakXy6oBMzPlk7U05bzzmYpLZ1aqhX8wD1ah6gQB83O1YKALgQwWUZIbgEAAAAgPJn77EM63Tyvw6n2Zy7KaSaejUPUO/mAQr19bBThQCAQgSXZYTgEgAAAADKt8Onzmj51hTFxCdp44FTOv9vvY0DvNS7eaB6twhQw1qeMplM9isUAKoogssyQnAJAAAAABXH0fSz+mFbimLik7Vu7wnlW/7+K3C9mh7W6eQtbvAhxASA64TgsowQXAIAAABAxXQqM0c/bi8IMdfsPq6cfIv13A3V3KwhZuva1eXgQIgJAGWF4LKMEFwCAAAAQMV3+myuVu08ppj4JK3acUxZufnWc35eZvVs5q9ezQLVvl4NOTs62LFSAKh8CC7LCMElAAAAAFQuWTn5Wr37mGLik/Xj9hSdPptnPVfN3Vl3NPFXr+YBurVhTZmdHO1YKQBUDgSXZYTgEgAAAAAqr5w8i9buOa6Y+GT9sC1FJzNzrOc8zU66rXEt9WoeoG6N/OTu4mTHSgGg4iK4LCMElwAAAABQNeTlW/T7/lOKiU9SzNZkpaRnW8+ZnRzU9UY/9W4RoNsa+8vHzdmOlQJAxUJwWUYILgEAAACg6rFYDMUdTlVMfLKWxSfp0Mks6zlnR5M61q+p3s0DdEdTf/l6mu1YKQCUfwSXZYTgEgAAAACqNsMwtC0pXTHxyYqJT9buoxnWcw4mqV3dGurdPFA9mwUowMfVjpUCQPlEcFlGCC4BAAAAAOdLOJqh5VsLOjHjE9NtzoXVrqbezQPUq1mgavu626lCAChfCC7LCMElAAAAAOBiDp08cy7ETNamA6dszjUN9Fbv5gHq3SJADWp52alCALC/kuZrDqW98erVq9W3b18FBQXJZDLpm2++uew1sbGxuvnmm2U2m9WgQQNFR0cXGZOYmKiBAwfK19dXbm5uatGihTZu3Gg9n5KSooiICAUFBcnd3V29evXS7t27be5x9uxZjRkzRr6+vvL09NT999+vlJQUmzEHDx5Unz595O7urlq1aumpp55SXl5eaT8GAAAAAACKCKnhrmGd6+nrUR3123O365V7mqljfV85Opi0LSldU1bsUvhbq3X7lFi9uXyn4hPTRD8RABSv1MFlZmamWrVqpWnTppVo/L59+9SnTx91795dcXFxmjBhgoYNG6bly5dbx5w6dUqdOnWSs7Ozli1bpm3btmnKlCmqXr26pIL1Q/r166e9e/dq8eLF+vPPPxUaGqrw8HBlZmZa7zNx4kR9++23mj9/vn7++WcdOXJE9913n/V8fn6++vTpo5ycHK1du1azZ89WdHS0XnjhhdJ+DAAAAAAAXJK/t6sGdaijL4bfot//Ga7X72+p7o385Oxo0p5jmXp/VYLueu8XdXljlf6zdJs2HTgpi+XiIWa+JV+x+2P15ZYvFbs/VvmW/Ov4bgDg+ruqqeImk0mLFi1Sv379LjrmmWee0dKlSxUfH2899vDDDys1NVUxMTGSpGeffVa//vqr1qxZU+w9du3apUaNGik+Pl7NmjWTJFksFgUEBCgqKkrDhg1TWlqa/Pz89MUXX6h///6SpB07dqhJkyZat26dbrnlFi1btkx33XWXjhw5In9/f0nSjBkz9Mwzz+jYsWNycXG57HtmqjgAAAAA4Gqkn83Vqh1HtWxLsmJ3HdXZXIv1nL+3WT2bBahXswC1q1tDTo4F/UYLty/U+JjxOpx+2Do22DtY7/R6R/c1ua/IawBAeVZmU8VLa926dQoPD7c51rNnT61bt876fMmSJWrTpo0eeOAB1apVS2FhYfroo4+s57OzsyVJrq5/78bm4OAgs9msX375RZK0adMm5ebm2rxW48aNVbt2betrrVu3Ti1atLCGloW1pKena+vWrcXWn52drfT0dJsHAAAAAABXytvVWffcdINmDGqtPyf30IyBN6vfTUHyMjspJT1bn607oAEf/6Z2USv1zIK/9NKPs9R/Xn+b0FKSEtMT1X9efy3cvtBO7wQAylaZB5fJyck2QaEk+fv7Kz09XVlZWZKkvXv3avr06WrYsKGWL1+uUaNGady4cZo9e7akvwPIyMhInTp1Sjk5OXrttdd0+PBhJSUlWV/HxcVF1apVK/JaycnJl6yl8FxxXn31Vfn4+FgfISEhV/eBAAAAAABwjpuLo3o1D9TUh8O0cXK4ZkW01UNtQlTd3VknM3P0v4379cqap4tdB9NQwbEJMROYNg6gUirz4LIkLBaLbr75ZkVFRSksLEwjRozQ8OHDNWPGDEmSs7OzFi5cqF27dqlGjRpyd3fXqlWr1Lt3bzk4lO1biIyMVFpamvVx6NChMn09AAAAAEDVZHZyVPfGtfRa/5b6/Z/h+mJYe3VtfkL5DsclU/HXGDJ0KP2Q1hwsfuk1AKjIyjy4DAgIKLKzd0pKiry9veXm5iZJCgwMVNOmTW3GNGnSRAcPHrQ+b926teLi4pSamqqkpCTFxMToxIkTqlevnvV1cnJylJqaWuS1AgICLllL4bnimM1meXt72zwAAAAAAChLTo4O6tigpnq3cr38YElJp5PKuCIAuP7KPLjs0KGDVq5caXNsxYoV6tChg/V5p06dtHPnTpsxu3btUmhoaJH7+fj4yM/PT7t379bGjRt1zz33SCoINp2dnW1ea+fOnTp48KD1tTp06KAtW7bo6NGjNrV4e3sXCU4BAAAAALC3QK/AazoOACoSp9JekJGRoYSEBOvzffv2KS4uTjVq1LCuQ5mYmKjPPvtMkjRy5Ei9//77evrpp/XYY4/pp59+0rx587R06VLrPSZOnKiOHTsqKipKDz74oDZs2KCZM2dq5syZ1jHz58+Xn5+fateurS1btmj8+PHq16+fevToIakg0Bw6dKgmTZqkGjVqyNvbW0888YQ6dOigW265RZLUo0cPNW3aVIMGDdLrr7+u5ORkPf/88xozZozMZvOVfYIAAAAAAJSRzrU7K9g7WInpidY1Lc9nkknB3sHqXLuzHaoDgLJV6o7LjRs3KiwsTGFhYZKkSZMmKSwsTC+88IIkKSkpyWaKd926dbV06VKtWLFCrVq10pQpU/Txxx+rZ8+e1jFt27bVokWL9OWXX6p58+Z65ZVXNHXqVD3yyCPWMUlJSRo0aJAaN26scePGadCgQfryyy9tanv77bd111136f7771eXLl0UEBCghQv/3l3N0dFR3333nRwdHdWhQwcNHDhQgwcP1ssvv1zajwEAAAAAgDLn6OCod3q9I6kgpLRxLsec2muqHB0cr3NlAFD2TEZxW5PhotLT0+Xj46O0tDTWuwQAAAAAXBcLty/U+JjxOpx+2HrM0VJTDzWcrLkDx9mxMgAovZLma6WeKg4AAAAAAK6v+5rcp3sa3aM1B9co6XSSkk+5aur3jtq001nHTmfLz4vlzwBUPgSXAAAAAABUAI4OjupWp5skyTAMxW5Zq82HUvXhz3v0/F1sOAug8inzXcUBAAAAAMC1ZTKZNCG8oSTp898O6Ojps3auCACuPYJLAAAAAAAqoG43+ummkGo6m2vRhz/vtXc5AHDNEVwCAAAAAFABmUwmTbzjRknS5+vpugRQ+RBcAgAAAABQQXVpWFNhtaspO8+iGbF0XQKoXAguAQAAAACooEwmkyaGF3Rdzv3tgI6m03UJoPIguAQAAAAAoALr3LCmbj7XdTn95z32LgcArhmCSwAAAAAAKrDz17qc+9tBpdB1CaCSILgEAAAAAKCCu7VBTbUJra6cPIumx9J1CaByILgEAAAAAKCCM5lMmnBurcsvNhxUchpdlwAqPoJLAAAAAAAqgU4NfNW2TmHXZYK9ywGAq0ZwCQAAAABAJXB+1+WXGw7RdQmgwiO4BAAAAACgkuhY31ft6tRQTr5FH9B1CaCCI7gEAAAAAKCSMJlMmnBHQ0nS/zYc0pHULDtXBABXjuASAAAAAIBKpEM9X7WrW9B1yQ7jACoygksAAAAAACoRk8mkiefWuvzqd7ouAVRcBJcAAAAAAFQyHer76pZ6BV2X01ax1iWAiongEgAAAACASqhwh/F5Gw8pka5LABUQwSUAAAAAAJXQLfV81aGer3LzDbouAVRIBJcAAAAAAFRSE8ILdhifv/GQDp86Y+dqAKB0CC4BAAAAAKik2tfzVcf6hV2X7DAOoGIhuAQAAAAAoBIrXOty/sZDOnSSrksAFQfBJQAAAAAAlVi7ujXUqYGv8iyGPohlrUsAFQfBJQAAAAAAldzfXZeH6boEUGEQXAIAAAAAUMm1rVNDtzaoqTwLO4wDqDgILgEAAAAAqAIm3lGww/iCTXRdAqgYCC4BAAAAAKgCWofWUOeGBV2X7/20297lAMBlEVwCAAAAAFBFFK51+fUfiTp4gq5LAOUbwSUAAAAAAFVE69Dq6nKjn/LpugRQARBcAgAAAABQhUwIL1jrcuGfidp/PNPO1QDAxRFcAgAAAABQhdxcu7q6nuu6fJ8dxgGUYwSXAAAAAABUMYVdl4vougRQjhFcAgAAAABQxYTVrq5ujQrXuqTrEkD5RHAJAAAAAEAVVLjD+KI/D2sfXZcAyiGCSwAAAAAAqqCbQqrptsa1ZDGk91aywziA8ofgEgAAAACAKmr87QVrXX4Tl6i9xzLsXA0A2CK4BAAAAACgimoVUk23F3ZdstYlgHKG4BIAAAAAgCqscK3LxXGJ2kPXJYByhOASAAAAAIAqrEWwj8KbsNYlgPKH4BIAAAAAgCqusOtyyeYjSjhK1yWA8oHgEgAAAACAKq75DT4Kb+J/bq1Lui4BlA8ElwAAAAAAQBPCC3YYL+i6PG3nagCA4BIAAAAAAKig67JHU38ZhvTOSnYYB2B/BJcAAAAAAECSNP5c1+V3fx3R7hS6LgHYV6mDy9WrV6tv374KCgqSyWTSN998c9lrYmNjdfPNN8tsNqtBgwaKjo4uMiYxMVEDBw6Ur6+v3Nzc1KJFC23cuNF6PiMjQ2PHjlVwcLDc3NzUtGlTzZgxw3p+//79MplMxT7mz59vHVfc+f/973+l/RgAAAAAAKh0mgX5qGezwq5L1roEYF+lDi4zMzPVqlUrTZs2rUTj9+3bpz59+qh79+6Ki4vThAkTNGzYMC1fvtw65tSpU+rUqZOcnZ21bNkybdu2TVOmTFH16tWtYyZNmqSYmBh9/vnn2r59uyZMmKCxY8dqyZIlkqSQkBAlJSXZPF566SV5enqqd+/eNjXNmjXLZly/fv1K+zEAAAAAAFApjb+9YIfxpVuStIuuSwB25FTaC3r37l0kCLyUGTNmqG7dupoyZYokqUmTJvrll1/09ttvq2fPnpKk1157TSEhIZo1a5b1urp169rcZ+3atRoyZIi6desmSRoxYoQ+/PBDbdiwQXfffbccHR0VEBBgc82iRYv04IMPytPT0+Z4tWrViowFAAAAAABS0yBv9WoWoJityXpn5W5NG3CzvUsCUEWV+RqX69atU3h4uM2xnj17at26ddbnS5YsUZs2bfTAAw+oVq1aCgsL00cffWRzTceOHbVkyRIlJibKMAytWrVKu3btUo8ePYp93U2bNikuLk5Dhw4tcm7MmDGqWbOm2rVrp08//VSGYVy0/uzsbKWnp9s8AAAAAACozArXuvx+S5J2JtN1CcA+yjy4TE5Olr+/v80xf39/paenKysrS5K0d+9eTZ8+XQ0bNtTy5cs1atQojRs3TrNnz7Ze895776lp06YKDg6Wi4uLevXqpWnTpqlLly7Fvu4nn3yiJk2aqGPHjjbHX375Zc2bN08rVqzQ/fffr9GjR+u99967aP2vvvqqfHx8rI+QkJAr/SgAAAAAAKgQmgR6q3fzABmG9C5rXQKwk1JPFS8LFotFbdq0UVRUlCQpLCxM8fHxmjFjhoYMGSKpILhcv369lixZotDQUK1evVpjxoxRUFBQkY7OrKwsffHFF5o8eXKR1zr/WFhYmDIzM/XGG29o3LhxxdYWGRmpSZMmWZ+np6cTXgIAAAAAKr3x4Q21LD5ZS7ck6YnkdDUO8LZ3SQCqmDLvuAwICFBKSorNsZSUFHl7e8vNzU2SFBgYqKZNm9qMadKkiQ4ePCipIIh87rnn9NZbb6lv375q2bKlxo4dq4ceekhvvvlmkddcsGCBzpw5o8GDB1+2vvbt2+vw4cPKzs4u9rzZbJa3t7fNAwAAAACAyq5xgLf6tAiURNclAPso8+CyQ4cOWrlypc2xFStWqEOHDtbnnTp10s6dO23G7Nq1S6GhoZKk3Nxc5ebmysHBtlxHR0dZLJYir/nJJ5/o7rvvlp+f32Xri4uLU/Xq1WU2m0v8ngAAAAAAqArG3d5QJpP0/ZZkbU9izwcA11epp4pnZGQoISHB+nzfvn2Ki4tTjRo1VLt2bUVGRioxMVGfffaZJGnkyJF6//339fTTT+uxxx7TTz/9pHnz5mnp0qXWe0ycOFEdO3ZUVFSUHnzwQW3YsEEzZ87UzJkzJUne3t7q2rWrnnrqKbm5uSk0NFQ///yzPvvsM7311ls29SUkJGj16tX6/vvvi9T+7bffKiUlRbfccotcXV21YsUKRUVF6cknnyztxwAAAAAAQKXXKMBLd7YI1NK/kvTOj7s1Y1Bre5cEoAoxGZfaUrsYsbGx6t69e5HjQ4YMUXR0tCIiIrR//37FxsbaXDNx4kRt27ZNwcHBmjx5siIiImyu/+677xQZGandu3erbt26mjRpkoYPH249n5ycrMjISP3www86efKkQkNDNWLECE2cOFEmk8k67rnnntPnn3+u/fv3F+nQjImJUWRkpBISEmQYhho0aKBRo0Zp+PDhRcZeTHp6unx8fJSWlsa0cQAAAABApbcr5bR6Tl0tw5C+H9dZTYP4uzCAq1PSfK3UwWVVR3AJAAAAAKhqxn7xh777K0k9m/nrw0Ft7F0OgAqupPlama9xCQAAAAAAKrbx59a6XL41RVuPpNm7HABVBMElAAAAAAC4pIb+XrqrZZAk6Z0f2WEcwPVBcAkAAAAAAC5r/O0NZDJJP2xLUXwiXZcAyh7BJQAAAAAAuKwGtbzUt7DrciVdlwDKHsElAAAAAAAokXG3N5SDSVpB1yWA64DgEgAAAAAAlEiDWp66u1VB1+XUH3fZuRoAlR3BJQAAAAAAKLEnznVd/rj9qLYcpusSQNkhuAQAAAAAACVW389T99x0gyS6LgGULYJLAAAAAABQKk/c1kAOJmnljqPafCjV3uUAqKQILgEAAAAAQKnU8/NUv3Ndl+wwDqCsEFwCAAAAAIBSK1zr8qcdRxVH1yWAMkBwCQAAAAAASq1uTQ/1CzvXdclalwDKAMElAAAAAAC4IuNuayhHB5NW7TymPw+esnc5ACoZgksAAAAAAHBF6tT00L1hhTuMs9YlgGuL4BIAAAAAAFyxJ25rIEcHk37edUx/0HUJ4BoiuAQAAAAAAFcs1NdD99F1CaAMEFwCAAAAAICrMvZc1+XqXce06QBdlwCuDYJLAAAAAABwVUJ9PXT/zYVdl+wwDuDaILgEAAAAAABXbWz3hnJyMGnN7uPadOCkvcsBUAkQXAIAAAAAgKtW29dd998cLIm1LgFcGwSXAAAAAADgmhh7WwNr1+XG/XRdArg6BJcAAAAAAOCaCKnhrgfaFHRdvs1alwCuEsElAAAAAAC4ZkZ3K+i6/DXhhDbso+sSwJUjuAQAAAAAANdMQddliCR2GAdwdQguAQAAAADANTWme305O5q0ds8J/bb3hL3LAVBBEVwCAAAAAIBrKrj6+V2X7DAO4MoQXAIAAAAAgGtuTPcGcnY0ad3eE1pP1yWAK0BwCQAAAAAArrkbqrnpQda6BHAVCC4BAAAAAECZKOy6XL/3pNbtoesSQOkQXAIAAAAAgDIRVM1ND7Wl6xLAlSG4BAAAAAAAZWZM9wZycXTQb/tOau2e4/YuB0AFQnAJAAAAAADKTKCPmx5ud67rcsVuGYZh54oAVBQElwAAAAAAoEyN6lZfLo4O2rCftS4BlBzBJQAAAAAAKFOBPm76x7muy7d/3EXXJYASIbgEAAAAAABlblS3BnJxctDv+0/p1wS6LgFcHsElAAAAAAAocwE+rhrQrrakgh3G6boEcDkElwAAAAAA4LoY1a2+XJwctPHAKf2SwA7jAC6N4BIAAAAAAFwX/t7nd12ywziASyO4BAAAAAAA183obvVldnLQpgOntGY3XZcALo7gEgAAAAAAXDe1vF31SPtQSewwDuDSCC4BAAAAAMB1NbJrPZmdHPTnwVStpusSwEUQXAIAAAAAgOuqlrerBt5yrutyBV2XAIpHcAkAAAAAAK67x7vWk6uzg+IOpernXcfsXQ6AcojgEgAAAAAAXHe1vFw10LrWJTuMAyiK4BIAAAAAANjF413ry9XZQZsPpSp2J12XAGyVOrhcvXq1+vbtq6CgIJlMJn3zzTeXvSY2NlY333yzzGazGjRooOjo6CJjEhMTNXDgQPn6+srNzU0tWrTQxo0breczMjI0duxYBQcHy83NTU2bNtWMGTNs7tGtWzeZTCabx8iRI23GHDx4UH369JG7u7tq1aqlp556Snl5eaX9GAAAAAAAwFXy8zJr0Lm1LqeywziAC5Q6uMzMzFSrVq00bdq0Eo3ft2+f+vTpo+7duysuLk4TJkzQsGHDtHz5cuuYU6dOqVOnTnJ2dtayZcu0bds2TZkyRdWrV7eOmTRpkmJiYvT5559r+/btmjBhgsaOHaslS5bYvN7w4cOVlJRkfbz++uvWc/n5+erTp49ycnK0du1azZ49W9HR0XrhhRdK+zEAAAAAAIBr4PGu9eXm7KjNh9O0audRe5cDoBwxGVfxnzNMJpMWLVqkfv36XXTMM888o6VLlyo+Pt567OGHH1ZqaqpiYmIkSc8++6x+/fVXrVmz5qL3ad68uR566CFNnjzZeqx169bq3bu3/v3vf0sq6Li86aabNHXq1GLvsWzZMt111106cuSI/P39JUkzZszQM888o2PHjsnFxeWy7zk9PV0+Pj5KS0uTt7f3ZccDAAAAAIBLe/X77fpw9V61DPbR4jGdZDKZ7F0SgDJU0nytzNe4XLduncLDw22O9ezZU+vWrbM+X7Jkidq0aaMHHnhAtWrVUlhYmD766CObazp27KglS5YoMTFRhmFo1apV2rVrl3r06GEzbu7cuapZs6aaN2+uyMhInTlzxqaWFi1aWEPLwlrS09O1devWYuvPzs5Wenq6zQMAAAAAAFw7w7vUk5uzo/46nKafdtB1CaBAmQeXycnJNkGhJPn7+ys9PV1ZWVmSpL1792r69Olq2LChli9frlGjRmncuHGaPXu29Zr33ntPTZs2VXBwsFxcXNSrVy9NmzZNXbp0sY4ZMGCAPv/8c61atUqRkZGaM2eOBg4ceNlaCs8V59VXX5WPj4/1ERIScnUfCAAAAAAAsFHT06zBHQvXumSHcQAFnOxdgCRZLBa1adNGUVFRkqSwsDDFx8drxowZGjJkiKSC4HL9+vVasmSJQkNDtXr1ao0ZM0ZBQUHWjs4RI0ZY79miRQsFBgbq9ttv1549e1S/fv0rqi0yMlKTJk2yPk9PTye8BAAAAADgGhvRuZ7mrDugLYlpWrn9qMKb+l/+IgCVWpl3XAYEBCglJcXmWEpKiry9veXm5iZJCgwMVNOmTW3GNGnSRAcPHpQkZWVl6bnnntNbb72lvn37qmXLlho7dqweeughvfnmmxd97fbt20uSEhISLllL4bnimM1meXt72zwAAAAAAMC15etp1uAOdSRJU1eywziA6xBcdujQQStXrrQ5tmLFCnXo0MH6vFOnTtq5c6fNmF27dik0tKBNPDc3V7m5uXJwsC3X0dFRFovloq8dFxcnqSAYLaxly5YtOnr07/UyVqxYIW9v7yLBKQAAAAAAuL5GdKkndxdHxSema8W2lMtfAKBSK3VwmZGRobi4OGsouG/fPsXFxVm7IyMjIzV48GDr+JEjR2rv3r16+umntWPHDn3wwQeaN2+eJk6caB0zceJErV+/XlFRUUpISNAXX3yhmTNnasyYMZIkb29vde3aVU899ZRiY2O1b98+RUdH67PPPtO9994rSdqzZ49eeeUVbdq0Sfv379eSJUs0ePBgdenSRS1btpQk9ejRQ02bNtWgQYO0efNmLV++XM8//7zGjBkjs9l8ZZ8gAAAAAAC4Jmp4uGhIxzqSWOsSgGQySvlvgdjYWHXv3r3I8SFDhig6OloRERHav3+/YmNjba6ZOHGitm3bpuDgYE2ePFkRERE213/33XeKjIzU7t27VbduXU2aNEnDhw+3nk9OTlZkZKR++OEHnTx5UqGhoRoxYoQmTpwok8mkQ4cOaeDAgYqPj1dmZqZCQkJ077336vnnn7eZ3n3gwAGNGjVKsbGx8vDw0JAhQ/Tf//5XTk4lW+6zpNu1AwAAAACA0juVmaNbX/tJmTn5+nBQa/VsVvzSbgAqrpLma6UOLqs6gksAAAAAAMrWG8t3aNqqPWoS6K2lT9wqBweTvUsCcA2VNF8r8zUuAQAAAAAASmPYrfXkaXbS9qR0/cBal0CVRXAJAAAAAADKleoeLoqwrnW5SxYLk0WBqojgEgAAAAAAlDvDOteVp9lJO5JP64dtyfYuB4AdEFwCAAAAAIByp5q7ix7tVEdSwQ7jdF0CVQ/BJQAAAAAAKJeG3lpXXue6LmO20nUJVDUElwAAAAAAoFw6v+vyHbougSqH4BIAAAAAAJRbQ2+tJy+zk3amnNayeLougaqE4BIAAAAAAJRbPu7OevTWupKkd1aywzhQlRBcAgAAAACAcm3orXXl5eqkXSkZ+j4+yd7lALhOCC4BAAAAAEC55uPmrKGFXZc/7lY+XZdAlUBwCQAAAAAAyr1HOxV0Xe4+mqHvt9B1CVQFBJcAAAAAAKDc83Fz1rBb60mS3llJ1yVQFRBcAgAAAACACuHRW+vI29VJCUcz9N1fR+xdDoAyRnAJAAAAAAAqBG9XZw3rXNB1+S5dl0ClR3AJAAAAAAAqjIhOdeTj5qw9xzLpugQqOYJLAAAAAABQYXi7OmtY4Q7jdF0ClRrBJQAAAAAAqFAiOtVRNXdn7T2WqW8303UJVFYElwAAAAAAoELxcnXW8PPWuszLt9i5IgBlgeASAAAAAABUOIM7hBZ0XR7P1LesdQlUSgSXAAAAAACgwrHtukyg6xKohAguAQAAAABAhTSkYx1Vd3fWvuOZWsJal0ClQ3AJAAAAAAAqJE+zk4Z3Ya1LoLIiuAQAAAAAABXWkA4FXZf7T5zRN3F0XQKVCcElAAAAAACosDzMThrRpb4k6b2f6LoEKhOCSwAAAAAAUKEN7hCqGh4uOnDijBb9mWjvcgBcIwSXAAAAAACgQvMwO+nxc2tdvvdTgnLpugQqBYJLAAAAAABQ4Q3qECpfDxcdPEnXJVBZEFwCAAAAAIAKz93FSY93Ley63E3XJVAJEFwCAAAAAIBKYeAtoarp6aJDJ7O06A+6LoGKjuASAAAAAABUCu4uTnq8cIfxVXRdAhUdwSUAAAAAAKg0zu+6/HrTYXuXA+AqEFwCAAAAAIBKw83FUSO7FnRdvr8qQTl5dF0CFRXBJQAAAAAAqFQeaR+qmp5mHT6Vpa//oOsSqKgILgEAAAAAQKVS0HVZsMP4+z/RdQlUVASXAAAAAACg0hl4S6j8vMxKTM3SAta6BCokgksAAAAAAFDpuDo7atS5tS6nsdYlUCERXAIAAAAAgEppQPvaqnWu63L+pkP2LgdAKRFcAgAAAACASsnV2VGjup3ruvwpQdl5+XauCEBpEFwCAAAAAIBK6x/tCrouj6Sd1byNrHUJVCQElwAAAAAAoNJydXbU6HNdlx+sousSqEgILgEAAAAAQKX2cLva8vc2KyntrOb9zlqXQEVBcAkAAAAAACq1gq7LBpKkaav20HUJVBAElwAAAAAAoNJ7qG2IArxdlZx+Vl/RdQlUCASXAAAAAACg0nN1dtSY7ud2GF+VoLO5dF0C5R3BJQAAAAAAqBIebBuiQB9XpaRn03UJVAClDi5Xr16tvn37KigoSCaTSd98881lr4mNjdXNN98ss9msBg0aKDo6usiYxMREDRw4UL6+vnJzc1OLFi20ceNG6/mMjAyNHTtWwcHBcnNzU9OmTTVjxgzr+ZMnT+qJJ55Qo0aN5Obmptq1a2vcuHFKS0uzeR2TyVTk8b///a+0HwMAAAAAAKhgzE6OGt29YK3LD2LpugTKu1IHl5mZmWrVqpWmTZtWovH79u1Tnz591L17d8XFxWnChAkaNmyYli9fbh1z6tQpderUSc7Ozlq2bJm2bdumKVOmqHr16tYxkyZNUkxMjD7//HNt375dEyZM0NixY7VkyRJJ0pEjR3TkyBG9+eabio+PV3R0tGJiYjR06NAiNc2aNUtJSUnWR79+/Ur7MQAAAAAAgArowTbBCjrXdfnlhoP2LgfAJZgMwzCu+GKTSYsWLbpk8PfMM89o6dKlio+Ptx57+OGHlZqaqpiYGEnSs88+q19//VVr1qy56H2aN2+uhx56SJMnT7Yea926tXr37q1///vfxV4zf/58DRw4UJmZmXJycipxzZeSnp4uHx8fpaWlydvb+4ruAQAAAAAA7Ofz9Qf0/DfxquVl1uqnu8vV2dHeJQFVSknztTJf43LdunUKDw+3OdazZ0+tW7fO+nzJkiVq06aNHnjgAdWqVUthYWH66KOPbK7p2LGjlixZosTERBmGoVWrVmnXrl3q0aPHRV+78M0XhpaFxowZo5o1a6pdu3b69NNPdansNjs7W+np6TYPAAAAAABQcT3YJkRBPq46ejpbX/xG1yVQXpV5cJmcnCx/f3+bY/7+/kpPT1dWVpYkae/evZo+fboaNmyo5cuXa9SoURo3bpxmz55tvea9995T06ZNFRwcLBcXF/Xq1UvTpk1Tly5din3d48eP65VXXtGIESNsjr/88suaN2+eVqxYofvvv1+jR4/We++9d9H6X331Vfn4+FgfISEhV/pRAAAAAACAcsDFyUFjbitY63L6z3tY6xIop5wuP6TsWSwWtWnTRlFRUZKksLAwxcfHa8aMGRoyZIikguBy/fr1WrJkiUJDQ7V69WqNGTNGQUFBRTo609PT1adPHzVt2lT/+te/bM6dP9U8LCxMmZmZeuONNzRu3Lhia4uMjNSkSZNs7k14CQAAAABAxfZA6xB9sGqPElOzNPe3gxp6a117lwTgAmXecRkQEKCUlBSbYykpKfL29pabm5skKTAwUE2bNrUZ06RJEx08WNCunZWVpeeee05vvfWW+vbtq5YtW2rs2LF66KGH9Oabb9pcd/r0afXq1UteXl5atGiRnJ2dL1lf+/btdfjwYWVnZxd73mw2y9vb2+YBAAAAAAAqNhcnB40t7LqM3aOsHLougfKmzIPLDh06aOXKlTbHVqxYoQ4dOlifd+rUSTt37rQZs2vXLoWGhkqScnNzlZubKwcH23IdHR1lsVisz9PT09WjRw+5uLhoyZIlcnV1vWx9cXFxql69usxmc6nfGwAAAAAAqLjuvzlYN1Rz0/GMbM397YC9ywFwgVJPFc/IyFBCQoL1+b59+xQXF6caNWqodu3aioyMVGJioj777DNJ0siRI/X+++/r6aef1mOPPaaffvpJ8+bN09KlS633mDhxojp27KioqCg9+OCD2rBhg2bOnKmZM2dKkry9vdW1a1c99dRTcnNzU2hoqH7++Wd99tlneuuttyT9HVqeOXNGn3/+uc1GOn5+fnJ0dNS3336rlJQU3XLLLXJ1ddWKFSsUFRWlJ5988so/QQAAAAAAUCG5ODnoidsa6NmFWzTj5716pH2o3FzYYRwoL0zGpbbULkZsbKy6d+9e5PiQIUMUHR2tiIgI7d+/X7GxsTbXTJw4Udu2bVNwcLAmT56siIgIm+u/++47RUZGavfu3apbt64mTZqk4cOHW88nJycrMjJSP/zwg06ePKnQ0FCNGDFCEydOlMlkumhdUkG4WqdOHcXExCgyMlIJCQkyDEMNGjTQqFGjNHz48CLdnBdT0u3aAQAAAABA+Zebb1H3N2N1+FSWnu/TRMM617N3SUClV9J8rdTBZVVHcAkAAAAAQOXy1e8H9czXW1TT00Wrn+4ud5dysZcxUGmVNF8r8zUuAQAAAAAAyrP7bg5WSA03/X979x6kZV3/Dfy9uwqSsagoJxeQPPwERQHxADyekjwM2s88pI0WWtnkgAo0Jjihz0yKWulYoiB2QDMqMy2yUXNIDiqkYvhIKlggEApoKgsoB3f3+QPY3NAE3eW6ZV+vmf1jv/d17/We++LSmfd87u/1+ur1uXuWvS6hVCguAQAAgGZt54ryXHL8/kmS26ctyNvr3y04EZAoLgEAAADyhT57p8sen8q/1qzPz2eauoRSoLgEAAAAmr2dKzY+YTxJbp++IGvWmbqEoikuAQAAAJJ8offe6dr2U3ljzfr83F6XUDjFJQAAAECSnSrKc8lnN+51OcHUJRROcQkAAACwyem9OmWfTVOXd9nrEgqluAQAAADYpOHU5T+y2tQlFEZxCQAAAPAe/9urU7rtuWvefHtD7nzi5aLjQLOluAQAAAB4j53e84TxO2YsMHUJBVFcAgAAAPyHzx/aKZ/Zc9e8ZeoSCqO4BAAAAPgPO1WU55IT/j11uWrthoITQfOjuAQAAAB4H58/dO98Zi9Tl1AUxSUAAADA+6goL8tlJ2x8wvgdMxam2tQlbFeKSwAAAIAPcOohnbLvXrtm5TsbcufjLxcdB5oVxSUAAADAB6goL8ul9VOXC0xdwnakuAQAAAD4L049pFP2a/fpVK99Nz977OWi40CzobgEAAAA+C/eO3X5k8cWZOU7pi5he1BcAgAAAHyIQT07Zv/NU5ePLyw6DjQLiksAAACAD9Fw6nKhqUvYDhSXAAAAAFthUM+OOaD9p7Nq7bv56WOmLqGpKS4BAAAAtkJ5eVkuO+GAJMlPH1uYlW+buoSmpLgEAAAA2EqnHNwh/9O+dVatezc/sdclNCnFJQAAAMBWKi8vy2UDN+51+TNTl9CkFJcAAAAA2+DkgzrkwA6bpi4fW1B0HNhhKS4BAAAAtsHGvS43Tl3+9PGX89bb6wtOBDsmxSUAAADANjpp09Tl6nXv5scz7HUJTUFxCQAAALCNysvLMmzTXpcTn3g5b64xdQmNTXEJAAAA8BGc2KNDunes3Dh1aa9LaHSKSwAAAICPoMHU5eMv5w1Tl9CoFJcAAAAAH9GJPdqnR8fKrFlfkx/PMHUJjUlxCQAAAPARlZX9e+ryzidMXUJjUlwCAAAAfAyf69E+B3XaOHV5h6lLaDSKSwAAAICPYePU5QFJNk5d/mv1uoITwY5BcQkAAADwMQ3s3i4H712Zt9fXZIKpS2gUiksAAACAj6msrCzDTtg4dXnXE4tMXUIjUFwCAAAANIITurdLz73b5J0NNZkw3dQlfFyKSwAAAIBG8N4njN81c1FeN3UJH4viEgAAAKCRfPbAdjm0ytQlNAbFJQAAAEAjee8Txu+a+XJeW2XqEj4qxSUAAABAIzruf/bKoZ13y9oNtZkw/R9Fx4FPLMUlAAAAQCN6716XP5+1KCtWrS04EXwyKS4BAAAAGtlxB+yVXpumLm+fZq9L+CgUlwAAAACN7L1Tl3ebuoSPRHEJAAAA0ASOPWCv9O6yW9a9W5vxU01dwrZSXAIAAAA0gfc+YfwXf1mUFdWmLmFbbHNxOX369Jx22mnp1KlTysrK8rvf/e5D3zN16tT06dMnLVu2zH777ZeJEyducczSpUtz/vnnp23btmnVqlV69uyZp59+uv711atXZ+jQoamqqkqrVq3So0ePjB8/vsHfWLt2bYYMGZK2bdvm05/+dM4888wsX768wTGLFy/OoEGD8qlPfSrt2rXL5ZdfnnfffXdbPwYAAACAD3XM/numz6apy3HTPGEctsU2F5dr1qzJoYcemltvvXWrjl+4cGEGDRqU448/PnPmzMmwYcPy9a9/PQ8//HD9MW+++WYGDBiQnXfeOQ8++GCef/753Hjjjdl9993rjxkxYkQeeuih3H333XnhhRcybNiwDB06NJMnT64/Zvjw4fnDH/6Q3/zmN5k2bVpeeeWVnHHGGfWv19TUZNCgQVm/fn2eeOKJ3HnnnZk4cWKuuuqqbf0YAAAAAD5UWVlZhn9u89Tl4iw3dQlbrayurq7uI7+5rCz3339/Tj/99A885oorrsgf//jHzJ07t37t3HPPzVtvvZWHHnooSTJy5Mg8/vjjmTFjxgf+nYMPPjjnnHNORo8eXb922GGH5ZRTTsk111yTlStXZq+99sqkSZNy1llnJUlefPHFdO/ePTNnzsxRRx2VBx98MKeeempeeeWVtG/fPkkyfvz4XHHFFXnttdfSokWLLc67bt26rFu3rv736urqdO7cOStXrkxlZeXWfVAAAABAs1VXV5ezxs/M7EVv5oL+++T/fv6goiNBoaqrq9OmTZsP7deafI/LmTNnZuDAgQ3WTjrppMycObP+98mTJ6dv3745++yz065du/Tu3Tt33HFHg/f0798/kydPztKlS1NXV5dHH3008+fPz4knnpgkmT17djZs2NDgXAceeGC6dOlSf66ZM2emZ8+e9aXl5izV1dX529/+9r75r7vuurRp06b+p3Pnzh/vAwEAAACalbKysgzftNflpCcXZ9lKU5ewNZq8uFy2bFmDojBJ2rdvn+rq6rzzzjtJkgULFmTcuHHZf//98/DDD+fiiy/OpZdemjvvvLP+Pbfcckt69OiRqqqqtGjRIieffHJuvfXWHHPMMfXnadGiRXbbbbctzrVs2bL/mmXza+9n1KhRWblyZf3PkiVLPvqHAQAAADRLA/Zrm8P32T3r363NuKl/LzoOfCLsVHSAJKmtrU3fvn0zZsyYJEnv3r0zd+7cjB8/PoMHD06ysbicNWtWJk+enK5du2b69OkZMmRIOnXqtMVEZ2Nq2bJlWrZs2WR/HwAAANjxbX7C+Hk//kt++eSSXHzcfunQZpeiY0FJa/KJyw4dOmzxZO/ly5ensrIyrVq1SpJ07NgxPXr0aHBM9+7ds3jx4iTJO++8kyuvvDI33XRTTjvttBxyyCEZOnRozjnnnPzgBz+oP8/69evz1ltvbXGuDh06/Ncsm18DAAAAaCr9922bI/bZI+tranObqUv4UE1eXPbr1y9TpkxpsPbII4+kX79+9b8PGDAg8+bNa3DM/Pnz07Vr1yTJhg0bsmHDhpSXN4xbUVGR2traJBsf1LPzzjs3ONe8efOyePHi+nP169cvzz33XFasWNEgS2Vl5RbFKQAAAEBj2jh1uX+S5FdPLsmrK98pOBGUtm0uLlevXp05c+Zkzpw5SZKFCxdmzpw59dORo0aNyle+8pX647/5zW9mwYIF+fa3v50XX3wxt912W+65554MHz68/pjhw4dn1qxZGTNmTP7+979n0qRJmTBhQoYMGZIkqayszLHHHpvLL788U6dOzcKFCzNx4sTcdddd+cIXvpAkadOmTb72ta9lxIgRefTRRzN79uxceOGF6devX4466qgkyYknnpgePXrky1/+cp599tk8/PDD+c53vpMhQ4b4OjgAAADQ5Prt2zZHdNs0dfnoP4qOAyWtrK6urm5b3jB16tQcf/zxW6wPHjw4EydOzAUXXJCXX345U6dObfCe4cOH5/nnn09VVVVGjx6dCy64oMH7H3jggYwaNSovvfRSunXrlhEjRuSiiy6qf33ZsmUZNWpU/vSnP+WNN95I165d841vfCPDhw9PWVlZkmTt2rX51re+lV/+8pdZt25dTjrppNx2220Nvga+aNGiXHzxxZk6dWp23XXXDB48ONdff3122mnrtvvc2se1AwAAALyfmf/4V750x6y0qCjP1MuPS6fdWhUdCbarre3Xtrm4bO4UlwAAAMDHdc7tM/OXhW/k/KO65JrTexYdB7arre3XmnyPSwAAAAAaGjbwgCTJr59akqVv2esS3o/iEgAAAGA767dv2xz1mT2yoaYutz3qCePwfhSXAAAAAAUYvmnq8p6nl+Sfb75dcBooPYpLAAAAgAIc+Zm26b9v22yoqcutnjAOW1BcAgAAABRk816XvzF1CVtQXAIAAAAU5Ihue2TAfm3zbm1dbrXXJTSguAQAAAAo0L+nLv+ZJW+YuoTNFJcAAAAABTp8nz3yf/bb09Ql/AfFJQAAAEDBhg3cP0ly72xTl7CZ4hIAAACgYH332SNH779x6nLsn01dQqK4BAAAACgJm/e6vPeZf2bxv0xdguISAAAAoAQc1nX3HHPAXqmprcstf36p6DhQOMUlAAAAQInYvNflfX9dmkX/WlNwGiiW4hIAAACgRPTpsnuOrZ+6tNclzdtORQcAAAAA4N+GDdw/0+a/lvv+ujh9D3gldeVvpWPrjjm6y9GpKK8oOh5NrKa2JjMWz8irq15t9tddcQkAAABQQnp32T3dOs/NjBU/yJd+93r9elVlVX548g9zRvczCkxHU7rvhfty2UOX5Z/V/6xfa87X3VfFAQAAAErIfS/cl2mvj0pN2esN1pdWL81Z95yV+164r6BkNKX7XrgvZ91zVoPSMmne111xCQAAAFAiamprctlDl6UudUlZw9fqUpckGfbQsNTU1hSQjqbS4Lr/h+Z83X1VHAAAAKBEzFg8Y4uJu/eqS12WVC/JZ28Zl71a9NmOyWhKr61/Zquu+4zFM3LcPsdtv2AFU1wCAAAAlIhXV726Vcc9v2Jxdq3p1sRp2F7WVCxOWnz4cVv772NHobgEAAAAKBEdW3fcquMuH3hkDtnLxOWO4v+9tjZXTP/w47b238eOQnEJAAAAUCKO7nJ0qiqrsrR66fvud1iWslRVVuVbx56eivKKAhLSFD5Xe3pumfPh1/3oLkcXkK44Hs4DAAAAUCIqyivyw5N/mGRjWfVem3+/+eSblZY7GNf9/SkuAQAAAErIGd3PyL1fvDd7V+7dYL2qsir3fvHenNH9jIKS0ZRc9y2V1dXVbTl/ygeqrq5OmzZtsnLlylRWVhYdBwAAANhB1dTWZMbiGXl11avp2Lpjju5ydLObuGuOmsN139p+zR6XAAAAACWoorwix+1zXNEx2M5c93/zVXEAAAAAoOQoLgEAAACAkqO4BAAAAABKjuISAAAAACg5iksAAAAAoOQoLgEAAACAkqO4BAAAAABKjuISAAAAACg5iksAAAAAoOQoLgEAAACAkqO4BAAAAABKjuISAAAAACg5iksAAAAAoOTsVHSAT5q6urokSXV1dcFJAAAAAOCTZ3Ovtrln+yCKy220atWqJEnnzp0LTgIAAAAAn1yrVq1KmzZtPvD1sroPqzZpoLa2Nq+88kpat26dsrKyouM0uurq6nTu3DlLlixJZWVl0XGAJuJehx2f+xyaB/c67Pjc5+yI6urqsmrVqnTq1Cnl5R+8k6WJy21UXl6eqqqqomM0ucrKSv9BhGbAvQ47Pvc5NA/uddjxuc/Z0fy3ScvNPJwHAAAAACg5iksAAAAAoOQoLmmgZcuWufrqq9OyZcuiowBNyL0OOz73OTQP7nXY8bnPac48nAcAAAAAKDkmLgEAAACAkqO4BAAAAABKjuISAAAAACg5iksAAAAAoOQoLgEAAACAkqO4pIFbb701++yzT3bZZZcceeSRefLJJ4uOBDSS6667Locffnhat26ddu3a5fTTT8+8efOKjgU0seuvvz5lZWUZNmxY0VGARrR06dKcf/75adu2bVq1apWePXvm6aefLjoW0IhqamoyevTodOvWLa1atcq+++6b7373u6mrqys6Gmw3ikvq/frXv86IESNy9dVX55lnnsmhhx6ak046KStWrCg6GtAIpk2bliFDhmTWrFl55JFHsmHDhpx44olZs2ZN0dGAJvLUU0/l9ttvzyGHHFJ0FKARvfnmmxkwYEB23nnnPPjgg3n++edz4403Zvfddy86GtCIbrjhhowbNy5jx47NCy+8kBtuuCHf+973cssttxQdDbabsjpVPZsceeSROfzwwzN27NgkSW1tbTp37pxLLrkkI0eOLDgd0Nhee+21tGvXLtOmTcsxxxxTdBygka1evTp9+vTJbbfdlmuuuSa9evXKzTffXHQsoBGMHDkyjz/+eGbMmFF0FKAJnXrqqWnfvn1+8pOf1K+deeaZadWqVe6+++4Ck8H2Y+KSJMn69esze/bsDBw4sH6tvLw8AwcOzMyZMwtMBjSVlStXJkn22GOPgpMATWHIkCEZNGhQg/+3AzuGyZMnp2/fvjn77LPTrl279O7dO3fccUfRsYBG1r9//0yZMiXz589Pkjz77LN57LHHcsoppxScDLafnYoOQGl4/fXXU1NTk/bt2zdYb9++fV588cWCUgFNpba2NsOGDcuAAQNy8MEHFx0HaGS/+tWv8swzz+Spp54qOgrQBBYsWJBx48ZlxIgRufLKK/PUU0/l0ksvTYsWLTJ48OCi4wGNZOTIkamurs6BBx6YioqK1NTU5Nprr815551XdDTYbhSXAM3QkCFDMnfu3Dz22GNFRwEa2ZIlS3LZZZflkUceyS677FJ0HKAJ1NbWpm/fvhkzZkySpHfv3pk7d27Gjx+vuIQdyD333JNf/OIXmTRpUg466KDMmTMnw4YNS6dOndzrNBuKS5Ike+65ZyoqKrJ8+fIG68uXL0+HDh0KSgU0haFDh+aBBx7I9OnTU1VVVXQcoJHNnj07K1asSJ8+ferXampqMn369IwdOzbr1q1LRUVFgQmBj6tjx47p0aNHg7Xu3bvnt7/9bUGJgKZw+eWXZ+TIkTn33HOTJD179syiRYty3XXXKS5pNuxxSZKkRYsWOeywwzJlypT6tdra2kyZMiX9+vUrMBnQWOrq6jJ06NDcf//9+fOf/5xu3boVHQloAieccEKee+65zJkzp/6nb9++Oe+88zJnzhylJewABgwYkHnz5jVYmz9/frp27VpQIqApvP322ykvb1jbVFRUpLa2tqBEsP2ZuKTeiBEjMnjw4PTt2zdHHHFEbr755qxZsyYXXnhh0dGARjBkyJBMmjQpv//979O6dessW7YsSdKmTZu0atWq4HRAY2nduvUWe9fuuuuuadu2rT1tYQcxfPjw9O/fP2PGjMkXv/jFPPnkk5kwYUImTJhQdDSgEZ122mm59tpr06VLlxx00EH561//mptuuilf/epXi44G201ZXV1dXdEhKB1jx47N97///Sxbtiy9evXKj370oxx55JFFxwIaQVlZ2fuu/+xnP8sFF1ywfcMA29Vxxx2XXr165eabby46CtBIHnjggYwaNSovvfRSunXrlhEjRuSiiy4qOhbQiFatWpXRo0fn/vvvz4oVK9KpU6d86UtfylVXXZUWLVoUHQ+2C8UlAAAAAFBy7HEJAAAAAJQcxSUAAAAAUHIUlwAAAABAyVFcAgAAAAAlR3EJAAAAAJQcxSUAAAAAUHIUlwAAAABAyVFcAgAAAAAlR3EJAAAAAJQcxSUAAAAAUHIUlwAAAABAyfn/ezwLDleEv5AAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "env.unwrapped.render_all()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (45) does not match length of index (89)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[213], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mquantstats\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mqs\u001b[39;00m\n\u001b[0;32m      3\u001b[0m qs\u001b[38;5;241m.\u001b[39mextend_pandas()\n\u001b[1;32m----> 5\u001b[0m net_worth \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrapped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_profit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mend_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m returns \u001b[38;5;241m=\u001b[39m net_worth\u001b[38;5;241m.\u001b[39mpct_change()\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(returns)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:503\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    501\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n\u001b[1;32m--> 503\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# create/copy the manager\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (SingleBlockManager, SingleArrayManager)):\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (45) does not match length of index (89)"
     ]
    }
   ],
   "source": [
    "import quantstats as qs\n",
    "\n",
    "qs.extend_pandas()\n",
    "\n",
    "net_worth = pd.Series(env.unwrapped.history['total_profit'], index=df.index[start_index+1:end_index])\n",
    "returns = net_worth.pct_change().iloc[1:]\n",
    "\n",
    "print(returns)\n",
    "\n",
    "qs.reports.full(returns)\n",
    "qs.reports.html(returns, output='SB3_a2c_quantstats.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
