{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "import math\n",
    "import random\n",
    "from itertools import count\n",
    "from collections import namedtuple, deque\n",
    "\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sys\n",
    "\n",
    "# Create a custom log formatter\n",
    "formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Configure logging with custom format and file output\n",
    "logging.basicConfig(filename='./logs/app.log', level=logging.DEBUG, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        open     high      low    close\n",
      "timestamp                                              \n",
      "2023-01-01 17:05:00  1.06973  1.06978  1.06970  1.06970\n",
      "2023-01-01 17:06:00  1.06966  1.06966  1.06966  1.06966\n",
      "2023-01-01 17:08:00  1.06970  1.06974  1.06970  1.06970\n",
      "2023-01-01 17:10:00  1.06975  1.06980  1.06972  1.06972\n",
      "2023-01-01 17:11:00  1.06972  1.06972  1.06972  1.06972\n",
      "2023-01-01 17:12:00  1.06975  1.06980  1.06975  1.06975\n",
      "2023-01-01 17:13:00  1.07066  1.07066  1.06917  1.06917\n",
      "2023-01-01 17:14:00  1.06937  1.06937  1.06899  1.06899\n",
      "2023-01-01 17:15:00  1.06788  1.06788  1.06788  1.06788\n",
      "2023-01-01 17:16:00  1.06788  1.06788  1.06788  1.06788\n",
      "            open       high        low      close\n",
      "count  10.000000  10.000000  10.000000  10.000000\n",
      "mean    1.069410   1.069429   1.069217   1.069217\n",
      "std     0.000871   0.000879   0.000751   0.000751\n",
      "min     1.067880   1.067880   1.067880   1.067880\n",
      "25%     1.069443   1.069443   1.069035   1.069035\n",
      "50%     1.069710   1.069730   1.069680   1.069680\n",
      "75%     1.069745   1.069795   1.069715   1.069715\n",
      "max     1.070660   1.070660   1.069750   1.069750\n"
     ]
    }
   ],
   "source": [
    "filename = \"EURUSD_M1_2023.csv\"\n",
    "\n",
    "df = pd.read_csv(\"./data_saved/\"+filename)\n",
    " # Convert 'timestamp' column to datetime\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp']) \n",
    "\n",
    "#timestamp as index\n",
    "df.set_index('timestamp', inplace=True)\n",
    "data = df.resample('15min').agg({'open': 'first', 'high': 'max', 'low': 'min', 'close': 'last'}).reset_index()\n",
    "\n",
    "df = df.iloc[:10]\n",
    "\n",
    "data = df.copy()\n",
    "#Drop NA rows\n",
    "data = data.dropna(axis=0)\n",
    "\n",
    "print(data.head(10))\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "class Actions(Enum):\n",
    "    Sell = 0\n",
    "    Buy = 1\n",
    "    Hold = 2\n",
    "\n",
    "\n",
    "class Positions(Enum):\n",
    "    Short = 0\n",
    "    Long = 1\n",
    "    Hold = 2\n",
    "\n",
    "    def opposite(self):\n",
    "        return Positions.Short if self == Positions.Long else Positions.Long\n",
    "\n",
    "\n",
    "class TradingEnv(gym.Env):\n",
    "\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': 3}\n",
    "\n",
    "    def __init__(self, df, window_size, render_mode=None):\n",
    "        # logging.debug(\"Trading Env init.\")\n",
    "        assert df.ndim == 2\n",
    "        assert render_mode is None or render_mode in self.metadata['render_modes']\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self._done = False\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.prices, self.signal_features = self._process_data()\n",
    "        self.shape = (window_size, self.signal_features.shape[1])\n",
    "\n",
    "        # spaces\n",
    "        self.action_space = gym.spaces.Discrete(len(Actions))\n",
    "        INF = 1e10\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-INF, high=INF, shape=self.shape, dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # episode\n",
    "        self._start_tick = self.window_size\n",
    "        self._end_tick = len(self.prices) - 1\n",
    "        self._truncated = None\n",
    "        self._current_tick = None\n",
    "        self._last_trade_tick = None\n",
    "        self._position = None\n",
    "        self._position_history = None\n",
    "        self._total_reward = None\n",
    "        self._total_profit = None\n",
    "        self._first_rendering = None\n",
    "        self.history = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # logging.debug(\"Trading Env reset.\")\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.action_space.seed(int((self.np_random.uniform(0, seed if seed is not None else 1))))\n",
    "\n",
    "        self._truncated = False\n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_trade_tick = self._current_tick - 1\n",
    "\n",
    "        self._position = Positions.Short\n",
    "        # logging.debug(\"Last tick  \", self._last_trade_tick)\n",
    "\n",
    "        # self._position = self.get_position(seed)\n",
    "        self._position_history = (self.window_size * [None]) + [self._position]\n",
    "        self._total_reward = 0.\n",
    "        self._total_profit = 1.  # unit\n",
    "        self._first_rendering = True\n",
    "        self.history = {}\n",
    "\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._done = True\n",
    "\n",
    "        observation = self._get_observation()\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # logging.debug(\"Trading Env step.\")\n",
    "        self._truncated = False\n",
    "        self._current_tick += 1\n",
    "\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._truncated = True\n",
    "\n",
    "        step_reward = self._calculate_reward(action)\n",
    "        print(\"step_reward: \", step_reward)\n",
    "        print(\"self._total_reward: \", self._total_reward)\n",
    "        self._total_reward += step_reward\n",
    "\n",
    "        # print(\"Updating profit\")\n",
    "\n",
    "        self._update_profit(action)\n",
    "\n",
    "        trade = False\n",
    "        if (\n",
    "            (action == Actions.Buy.value and self._position == Positions.Short) or\n",
    "            (action == Actions.Sell.value and self._position == Positions.Long)\n",
    "        ):\n",
    "            trade = True\n",
    "\n",
    "        if trade:\n",
    "            self._position = self._position.opposite()\n",
    "            self._last_trade_tick = self._current_tick\n",
    "\n",
    "        self._position_history.append(self._position)\n",
    "        observation = self._get_observation()\n",
    "        info = self._get_info()\n",
    "        self._update_history(info)\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self._render_frame()\n",
    "        \n",
    "\n",
    "\n",
    "        return observation, step_reward, self._done, self._truncated, info\n",
    "\n",
    "    def _get_info(self):\n",
    "        # logging.debug(\"Trading Env get info.\")\n",
    "        return dict(\n",
    "            total_reward=self._total_reward,\n",
    "            total_profit=self._total_profit,\n",
    "            position=self._position\n",
    "        )\n",
    "    #return a random position\n",
    "    # def get_position(seed=None):\n",
    "    #     logging.debug(\"Trading Env get position.\")\n",
    "    #     if seed is not None:\n",
    "    #         random.seed(seed)\n",
    "\n",
    "    #     return random.choice(list(Positions))\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # logging.debug(\"Trading Env get observation.\")\n",
    "        return self.signal_features[(self._current_tick - self.window_size + 1) : self._current_tick + 1]\n",
    "\n",
    "    def _update_history(self, info):\n",
    "        # logging.debug(\"Trading Env uodate history.\")\n",
    "        if not self.history:\n",
    "            self.history = {key: [] for key in info.keys()}\n",
    "\n",
    "        for key, value in info.items():\n",
    "            self.history[key].append(value)\n",
    "\n",
    "    def _render_frame(self):\n",
    "        self.render()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # logging.debug(\"Trading Env Render.\")\n",
    "\n",
    "        def _plot_position(position, tick):\n",
    "            color = None\n",
    "            if position == Positions.Short:\n",
    "                color = 'red'\n",
    "            elif position == Positions.Long:\n",
    "                color = 'green'\n",
    "            if color:\n",
    "                plt.scatter(tick, self.prices[tick], color=color)\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        if self._first_rendering:\n",
    "            self._first_rendering = False\n",
    "            plt.cla()\n",
    "            plt.plot(self.prices)\n",
    "            start_position = self._position_history[self._start_tick]\n",
    "            _plot_position(start_position, self._start_tick)\n",
    "\n",
    "        _plot_position(self._position, self._current_tick)\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
    "            \"Total Profit: %.6f\" % self._total_profit\n",
    "        )\n",
    "\n",
    "        end_time = time()\n",
    "        process_time = end_time - start_time\n",
    "\n",
    "        pause_time = (1 / self.metadata['render_fps']) - process_time\n",
    "        assert pause_time > 0., \"High FPS! Try to reduce the 'render_fps' value.\"\n",
    "\n",
    "        plt.pause(pause_time)\n",
    "\n",
    "    def render_all(self, title=None):\n",
    "        # logging.debug(\"Trading Env render all.\")\n",
    "        window_ticks = np.arange(len(self._position_history))\n",
    "        plt.plot(self.prices)\n",
    "\n",
    "        short_ticks = []\n",
    "        long_ticks = []\n",
    "        for i, tick in enumerate(window_ticks):\n",
    "            if self._position_history[i] == Positions.Short:\n",
    "                short_ticks.append(tick)\n",
    "            elif self._position_history[i] == Positions.Long:\n",
    "                long_ticks.append(tick)\n",
    "\n",
    "        plt.plot(short_ticks, self.prices[short_ticks], 'ro')\n",
    "        plt.plot(long_ticks, self.prices[long_ticks], 'go')\n",
    "\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
    "            \"Total Profit: %.6f\" % self._total_profit\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        plt.close()\n",
    "\n",
    "    def save_rendering(self, filepath):\n",
    "        plt.savefig(filepath)\n",
    "\n",
    "    def pause_rendering(self):\n",
    "        plt.show()\n",
    "\n",
    "    def _process_data(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def _update_profit(self, action):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def max_possible_profit(self):  # trade fees are ignored\n",
    "        raise NotImplementedError\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# from .TradingEnv import TradingEnv, Actions, Positions\n",
    "\n",
    "\n",
    "class ForexEnv(TradingEnv):\n",
    "\n",
    "    def __init__(self, df, window_size, frame_bound, unit_side='left', render_mode=None):\n",
    "        # logging.debug(\"Forex Env init.\")\n",
    "        assert len(frame_bound) == 2\n",
    "        assert unit_side.lower() in ['left', 'right']\n",
    "\n",
    "        self.frame_bound = frame_bound\n",
    "        self.unit_side = unit_side.lower()\n",
    "        super().__init__(df, window_size, render_mode)\n",
    "\n",
    "        self.trade_fee = 0.0003  # unit\n",
    "\n",
    "    def _process_data(self):\n",
    "        # logging.debug(\"Forex Env process data.\")\n",
    "        prices = self.df.loc[:, 'close'].to_numpy()\n",
    "\n",
    "        prices[self.frame_bound[0] - self.window_size]  # validate index (TODO: Improve validation)\n",
    "        prices = prices[self.frame_bound[0]-self.window_size:self.frame_bound[1]]\n",
    "\n",
    "        diff = np.insert(np.diff(prices), 0, 0)\n",
    "        signal_features = np.column_stack((prices, diff))\n",
    "\n",
    "        return prices.astype(np.float32), signal_features.astype(np.float32)\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        # logging.debug(\"Forex Env calculate reward.\")\n",
    "        \n",
    "        step_reward = 0  # pip\n",
    "\n",
    "        # print(\"action :\", action)\n",
    "        # print(\"position :\", self._position)\n",
    "        trade = False\n",
    "        if (\n",
    "            (action == Actions.Buy.value and self._position == Positions.Short) or\n",
    "            (action == Actions.Sell.value and self._position == Positions.Long)\n",
    "        ):\n",
    "            trade = True\n",
    "\n",
    "        if trade:\n",
    "            # print(\"Forex Env calculate reward - trade is true :\", self._position)\n",
    "            # if self._current_tick < len(self.prices):  # Check if current_tick is within bounds\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            last_trade_price = self.prices[self._last_trade_tick]\n",
    "            price_diff = current_price - last_trade_price\n",
    "\n",
    "            if self._position == Positions.Short:\n",
    "                step_reward += -price_diff * 10000\n",
    "            elif self._position == Positions.Long:\n",
    "                step_reward += price_diff * 10000\n",
    "            \n",
    "            # print(\"price_diff: \", price_diff, current_price, last_trade_price)\n",
    "\n",
    "        return step_reward\n",
    "\n",
    "\n",
    "\n",
    "    def _update_profit(self, action):\n",
    "        # logging.debug(\"Forex Env Update profit.\")\n",
    "        trade = False\n",
    "        # print(\"Updating profit with action: \", action)\n",
    "\n",
    "        if (\n",
    "            (action == Actions.Buy.value and self._position == Positions.Short) or\n",
    "            (action == Actions.Sell.value and self._position == Positions.Long)\n",
    "        ):\n",
    "            trade = True\n",
    "\n",
    "        if trade or self._truncated:\n",
    "            # if 0 <= self._current_tick < len(self.prices) and 0 <= self._last_trade_tick < len(self.prices):\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            last_trade_price = self.prices[self._last_trade_tick]\n",
    "\n",
    "            if self.unit_side == 'left':\n",
    "                if self._position == Positions.Short:\n",
    "                    quantity = self._total_profit * (last_trade_price - self.trade_fee)\n",
    "                    self._total_profit = quantity / current_price\n",
    "\n",
    "            elif self.unit_side == 'right':\n",
    "                if self._position == Positions.Long:\n",
    "                    quantity = self._total_profit / last_trade_price\n",
    "                    self._total_profit = quantity * (current_price - self.trade_fee)\n",
    "\n",
    "    def max_possible_profit(self):\n",
    "        # logging.debug(\"Forex Env max posssible progit.\")\n",
    "        current_tick = self._start_tick\n",
    "        last_trade_tick = current_tick - 1\n",
    "        profit = 1.\n",
    "\n",
    "        while current_tick <= self._end_tick:\n",
    "            position = None\n",
    "            if self.prices[current_tick] < self.prices[current_tick - 1]:\n",
    "                while (current_tick <= self._end_tick and\n",
    "                       self.prices[current_tick] < self.prices[current_tick - 1]):\n",
    "                    current_tick += 1\n",
    "                position = Positions.Short\n",
    "            else:\n",
    "                while (current_tick <= self._end_tick and\n",
    "                       self.prices[current_tick] >= self.prices[current_tick - 1]):\n",
    "                    current_tick += 1\n",
    "                position = Positions.Long\n",
    "\n",
    "            current_price = self.prices[current_tick - 1]\n",
    "            last_trade_price = self.prices[last_trade_tick]\n",
    "\n",
    "            if self.unit_side == 'left':\n",
    "                if position == Positions.Short:\n",
    "                    quantity = profit * (last_trade_price - self.trade_fee)\n",
    "                    profit = quantity / current_price\n",
    "\n",
    "            elif self.unit_side == 'right':\n",
    "                if position == Positions.Long:\n",
    "                    quantity = profit / last_trade_price\n",
    "                    profit = quantity * (current_price - self.trade_fee)\n",
    "\n",
    "            last_trade_tick = current_tick - 1\n",
    "\n",
    "        return profit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from enum import Enum\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import gymnasium as gym\n",
    "\n",
    "# import random\n",
    "\n",
    "\n",
    "\n",
    "class Actions(Enum):\n",
    "    Sell = 0\n",
    "    Buy = 1\n",
    "    Hold = 2\n",
    "\n",
    "\n",
    "class Positions(Enum):\n",
    "    Short = 0\n",
    "    Long = 1\n",
    "    Hold = 2\n",
    "\n",
    "    def opposite(self):\n",
    "        return Positions.Short if self == Positions.Long else Positions.Long\n",
    "\n",
    "\n",
    "class CustomTradingEnv(gym.Env):\n",
    "\n",
    "    metadata = {'render_modes': ['human'], 'render_fps': 3}\n",
    "\n",
    "    def __init__(self, df, window_size, render_mode=None):\n",
    "        # super().__init__(df, window_size, render_mode)\n",
    "\n",
    "        self.trade_fee = 0.0003  # unit\n",
    "        assert df.ndim == 2\n",
    "        assert render_mode is None or render_mode in self.metadata['render_modes']\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self._done = False\n",
    "        self.df = df\n",
    "        self.window_size = window_size\n",
    "        self.prices, self.signal_features = self._process_data()\n",
    "        self.shape = (window_size, self.signal_features.shape[1])\n",
    "\n",
    "        # spaces\n",
    "        self.action_space = gym.spaces.Discrete(len(Actions))\n",
    "        INF = 1e10\n",
    "        self.observation_space = gym.spaces.Box(\n",
    "            low=-INF, high=INF, shape=self.shape, dtype=np.float32,\n",
    "        )\n",
    "\n",
    "        # episode\n",
    "        self._start_tick = self.window_size\n",
    "        self._end_tick = len(self.prices) - 1\n",
    "        self._truncated = None\n",
    "        self._current_tick = None\n",
    "        self._last_trade_tick = None\n",
    "        self._position = None\n",
    "        self._position_history = None\n",
    "        self._total_reward = None\n",
    "        self._total_profit = None\n",
    "        self._first_rendering = None\n",
    "        self.history = None\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        # logging.debug(\"Trading Env reset.\")\n",
    "        super().reset(seed=seed, options=options)\n",
    "        self.action_space.seed(int((self.np_random.uniform(0, seed if seed is not None else 1))))\n",
    "\n",
    "        self._truncated = False\n",
    "        self._current_tick = self._start_tick\n",
    "        self._last_trade_tick = self._current_tick - 1\n",
    "\n",
    "        self._position = Positions.Short\n",
    "\n",
    "        # self._position = self.get_position(seed)\n",
    "        self._position_history = (self.window_size * [None]) + [self._position]\n",
    "        self._total_reward = 0.\n",
    "        self._total_profit = 1.  # unit\n",
    "        self._first_rendering = True\n",
    "        self.history = {}\n",
    "\n",
    "        observation = self._get_observation(self._done)\n",
    "        info = self._get_info()\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self._render_frame()\n",
    "\n",
    "        return observation, info\n",
    "\n",
    "    def step(self, action):\n",
    "        # logging.debug(\"Trading Env step.\")\n",
    "        self._truncated = False\n",
    "        self._current_tick += 1\n",
    "\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._truncated = True\n",
    "\n",
    "        print(\"Current Action: \", action)\n",
    "        #update position according with action\n",
    "        if action == Actions.Buy.value:\n",
    "            self._position = Positions.Long\n",
    "        elif action == Actions.Sell.value:\n",
    "            self._position = Positions.Short\n",
    "        else:\n",
    "            self._position = Positions.Hold\n",
    "            \n",
    "        \n",
    "        trade = False\n",
    "        if action == Actions.Buy.value  or action == Actions.Sell.value:\n",
    "            trade = True\n",
    "\n",
    "        step_reward  = 0.0\n",
    "        \n",
    "        ### Last trade tick\n",
    "        self._last_trade_tick = self._current_tick - 1\n",
    "\n",
    "        self._position_history.append(self._position)\n",
    "\n",
    "        ## End in last tick\n",
    "        if self._current_tick == self._end_tick:\n",
    "            self._done = True\n",
    "        \n",
    "        \n",
    "        ## Calculate step rewards\n",
    "        step_reward = self._calculate_reward(action)\n",
    "        self._total_reward += step_reward\n",
    "        self._total_profit += (step_reward - self.trade_fee)\n",
    "\n",
    "        print(\"step_reward: \", step_reward)\n",
    "        print(\"total_reward: \", self._total_reward)\n",
    "        print(\"self._total_profit: \", self._total_profit)\n",
    "\n",
    "        if self.render_mode == 'human':\n",
    "            self._render_frame()\n",
    "\n",
    "        observation = self._get_observation(self._done)\n",
    "        info = self._get_info()\n",
    "        self._update_history(info)\n",
    "\n",
    "        return observation, step_reward, self._done, self._truncated, info\n",
    "\n",
    "    def _get_info(self):\n",
    "        # logging.debug(\"Trading Env get info.\")\n",
    "        return dict(\n",
    "            total_reward=self._total_reward,\n",
    "            total_profit=self._total_profit,\n",
    "            position=self._position\n",
    "        )\n",
    "\n",
    "    def _get_observation(self, done):\n",
    "        # logging.debug(\"Trading Env get observation.\")\n",
    "        # return self.signal_features[(self._current_tick - self.window_size + 1) : self._current_tick + 1]\n",
    "        if done:\n",
    "            return [self.prices[self._current_tick], self._total_reward]\n",
    "        return [self.prices[self._current_tick + 1], self._total_reward]\n",
    "\n",
    "    def _update_history(self, info):\n",
    "        # logging.debug(\"Trading Env uodate history.\")\n",
    "        if not self.history:\n",
    "            self.history = {key: [] for key in info.keys()}\n",
    "\n",
    "        for key, value in info.items():\n",
    "            self.history[key].append(value)\n",
    "\n",
    "    def _render_frame(self):\n",
    "        self.render()\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        # logging.debug(\"Trading Env Render.\")\n",
    "\n",
    "        def _plot_position(position, tick):\n",
    "            color = None\n",
    "            if position == Positions.Short:\n",
    "                color = 'red'\n",
    "            elif position == Positions.Long:\n",
    "                color = 'green'\n",
    "            if color:\n",
    "                plt.scatter(tick, self.prices[tick], color=color)\n",
    "\n",
    "        start_time = time()\n",
    "\n",
    "        if self._first_rendering:\n",
    "            self._first_rendering = False\n",
    "            plt.cla()\n",
    "            plt.plot(self.prices)\n",
    "            start_position = self._position_history[self._start_tick]\n",
    "            _plot_position(start_position, self._start_tick)\n",
    "\n",
    "        _plot_position(self._position, self._current_tick)\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
    "            \"Total Profit: %.6f\" % self._total_profit\n",
    "        )\n",
    "\n",
    "        end_time = time()\n",
    "        process_time = end_time - start_time\n",
    "\n",
    "        pause_time = (1 / self.metadata['render_fps']) - process_time\n",
    "        assert pause_time > 0., \"High FPS! Try to reduce the 'render_fps' value.\"\n",
    "\n",
    "        plt.pause(pause_time)\n",
    "\n",
    "    def render_all(self, title=None):\n",
    "        # logging.debug(\"Trading Env render all.\")\n",
    "        window_ticks = np.arange(len(self._position_history))\n",
    "        plt.plot(self.prices)\n",
    "\n",
    "        short_ticks = []\n",
    "        long_ticks = []\n",
    "        for i, tick in enumerate(window_ticks):\n",
    "            if self._position_history[i] == Positions.Short:\n",
    "                short_ticks.append(tick)\n",
    "            elif self._position_history[i] == Positions.Long:\n",
    "                long_ticks.append(tick)\n",
    "\n",
    "        plt.plot(short_ticks, self.prices[short_ticks], 'ro')\n",
    "        plt.plot(long_ticks, self.prices[long_ticks], 'go')\n",
    "\n",
    "        if title:\n",
    "            plt.title(title)\n",
    "\n",
    "        plt.suptitle(\n",
    "            \"Total Reward: %.6f\" % self._total_reward + ' ~ ' +\n",
    "            \"Total Profit: %.6f\" % self._total_profit\n",
    "        )\n",
    "\n",
    "    def close(self):\n",
    "        plt.close()\n",
    "\n",
    "    def save_rendering(self, filepath):\n",
    "        plt.savefig(filepath)\n",
    "\n",
    "    def pause_rendering(self):\n",
    "        plt.show()\n",
    "\n",
    "    def _process_data(self):\n",
    "        prices = self.df.loc[:, 'close'].to_numpy()\n",
    "        \n",
    "        diff = np.insert(np.diff(prices), 0, 0)\n",
    "        signal_features = np.column_stack((prices, diff))\n",
    "\n",
    "        return prices.astype(np.float32), signal_features.astype(np.float32)\n",
    "\n",
    "    def _calculate_reward(self, action):\n",
    "        step_reward = 0  # pip\n",
    "        trade = False\n",
    "        if self._position == Positions.Short or self._position == Positions.Long:\n",
    "            trade = True\n",
    "            print(\"Trade Found.\")\n",
    "        \n",
    "\n",
    "        if not trade and self._position == Positions.Hold:\n",
    "            print(\"trade not found...\")\n",
    "            step_reward =  -0.5\n",
    "        \n",
    "\n",
    "        if trade:\n",
    "            print(\"Cacluating reward..\")\n",
    "            current_price = self.prices[self._current_tick]\n",
    "            last_trade_price = self.prices[self._last_trade_tick]\n",
    "            price_diff = current_price - last_trade_price\n",
    "            \n",
    "            formatted_price_diff = \"{:.5f}\".format(price_diff)\n",
    "            print(\"price_diff\", formatted_price_diff)\n",
    "\n",
    "            if self._position == Positions.Short:\n",
    "                step_reward += -price_diff * 10000\n",
    "            elif self._position == Positions.Long:\n",
    "                step_reward += price_diff * 10000\n",
    "            \n",
    "            print(\"step_reward : \", step_reward)\n",
    "        return step_reward\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Alternative Trading envionment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from envs import ForexEnv, Actions\n",
    "\n",
    "\n",
    "window_size = 2\n",
    "start_index = window_size\n",
    "end_index = len(data)\n",
    "# len(data)\n",
    "\n",
    "env  = CustomTradingEnv(\n",
    "    df= data,\n",
    "    window_size=window_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from stable_baselines3 import DQN\n",
    "# import torch.optim as optim\n",
    "\n",
    "# learning_rate = 0.01\n",
    "# optimizer = optim.Adam\n",
    "\n",
    "# model = DQN(\n",
    "#     \"MlpPolicy\",\n",
    "#     env = env,\n",
    "#     buffer_size=10000, \n",
    "#     batch_size = 32,\n",
    "#     train_freq = 32,\n",
    "#     gradient_steps = 32,\n",
    "#     target_update_interval = 32,\n",
    "#     learning_rate=learning_rate,\n",
    "#     verbose=1,\n",
    "#     device=device,\n",
    "# )\n",
    "# model.learn(total_timesteps=10, log_interval=10)\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "state_info:  ([1.06972, 0.0], {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "state_array:  [1.06972, 0.0]\n",
      "q_values_cpu:  tensor([-0.0214,  0.0278,  0.1597])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [1.06972, -0.5]\n",
      "state_array:  [1.06972, -0.5]\n",
      "q_values_cpu:  tensor([ 0.0361, -0.0227,  0.1290])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.0\n",
      "self._total_profit:  -0.0005999999999999339\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.0, 'total_profit': -0.0005999999999999339, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [1.06975, -1.0]\n",
      "state_array:  [1.06975, -1.0]\n",
      "q_values_cpu:  tensor([ 0.0581, -0.0591,  0.0890])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -1.5\n",
      "self._total_profit:  -0.5008999999999999\n",
      "reward: -0.5\n",
      "info: {'total_reward': -1.5, 'total_profit': -0.5008999999999999, 'position': <Positions.Hold: 2>}\n",
      "state_info:  [1.06917, -1.5]\n",
      "state_array:  [1.06917, -1.5]\n",
      "q_values_cpu:  tensor([ 0.0798, -0.0417,  0.0564])\n",
      "torch.argmax(q_values).item() 0\n",
      "action from training:  0\n",
      "Current Action:  0\n",
      "Trade Found.\n",
      "Cacluating reward..\n",
      "price_diff -0.00058\n",
      "step_reward :  5.799531936645508\n",
      "step_reward:  5.799531936645508\n",
      "total_reward:  4.299531936645508\n",
      "self._total_profit:  5.298331936645508\n",
      "reward: 5.799531936645508\n",
      "info: {'total_reward': 4.299531936645508, 'total_profit': 5.298331936645508, 'position': <Positions.Short: 0>}\n",
      "state_info:  [1.06899, 4.299531936645508]\n",
      "state_array:  [1.06899, 4.299531936645508]\n",
      "q_values_cpu:  tensor([0.0953, 0.3596, 0.1512])\n",
      "torch.argmax(q_values).item() 1\n",
      "action from training:  1\n",
      "Current Action:  1\n",
      "Trade Found.\n",
      "Cacluating reward..\n",
      "price_diff -0.00018\n",
      "step_reward :  -1.8000602722167969\n",
      "step_reward:  -1.8000602722167969\n",
      "total_reward:  2.499471664428711\n",
      "self._total_profit:  3.497971664428711\n",
      "reward: -1.8000602722167969\n",
      "info: {'total_reward': 2.499471664428711, 'total_profit': 3.497971664428711, 'position': <Positions.Long: 1>}\n",
      "state_info:  [1.06788, 2.499471664428711]\n",
      "state_array:  [1.06788, 2.499471664428711]\n",
      "q_values_cpu:  tensor([-0.0468,  0.2108,  0.1340])\n",
      "torch.argmax(q_values).item() 1\n",
      "action from training:  1\n",
      "Current Action:  1\n",
      "Trade Found.\n",
      "Cacluating reward..\n",
      "price_diff -0.00111\n",
      "step_reward :  -11.099576950073242\n",
      "step_reward:  -11.099576950073242\n",
      "total_reward:  -8.600105285644531\n",
      "self._total_profit:  -7.601905285644531\n",
      "reward: -11.099576950073242\n",
      "info: {'total_reward': -8.600105285644531, 'total_profit': -7.601905285644531, 'position': <Positions.Long: 1>}\n",
      "state_info:  [1.06788, -8.600105285644531]\n",
      "state_array:  [1.06788, -8.600105285644531]\n",
      "q_values_cpu:  tensor([ 0.1206,  0.2912, -0.0697])\n",
      "torch.argmax(q_values).item() 1\n",
      "action from training:  1\n",
      "Current Action:  1\n",
      "Trade Found.\n",
      "Cacluating reward..\n",
      "price_diff 0.00000\n",
      "step_reward :  0.0\n",
      "step_reward:  0.0\n",
      "total_reward:  -8.600105285644531\n",
      "self._total_profit:  -7.602205285644531\n",
      "reward: 0.0\n",
      "info: {'total_reward': -8.600105285644531, 'total_profit': -7.602205285644531, 'position': <Positions.Long: 1>}\n",
      "Episode 0, Reward: -8.600105285644531\n",
      "state_info:  ([1.0697, 0.0], {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "state_array:  [1.0697, 0.0]\n",
      "q_values_cpu:  tensor([-0.0214,  0.0278,  0.1597])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  ([1.0697, 0.0], {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "state_array:  [1.0697, 0.0]\n",
      "q_values_cpu:  tensor([-0.0214,  0.0278,  0.1597])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  ([1.0697, 0.0], {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "state_array:  [1.0697, 0.0]\n",
      "q_values_cpu:  tensor([-0.0214,  0.0278,  0.1597])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  ([1.0697, 0.0], {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "state_array:  [1.0697, 0.0]\n",
      "q_values_cpu:  tensor([-0.0214,  0.0278,  0.1597])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  ([1.0697, 0.0], {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "state_array:  [1.0697, 0.0]\n",
      "q_values_cpu:  tensor([-0.0214,  0.0278,  0.1597])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  ([1.0697, 0.0], {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "state_array:  [1.0697, 0.0]\n",
      "q_values_cpu:  tensor([-0.0214,  0.0278,  0.1597])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  ([1.0697, 0.0], {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "state_array:  [1.0697, 0.0]\n",
      "q_values_cpu:  tensor([-0.0214,  0.0278,  0.1597])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  ([1.0697, 0.0], {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "state_array:  [1.0697, 0.0]\n",
      "q_values_cpu:  tensor([-0.0214,  0.0278,  0.1597])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n",
      "state_info:  ([1.0697, 0.0], {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Short: 0>})\n",
      "state_array:  [1.0697, 0.0]\n",
      "q_values_cpu:  tensor([-0.0214,  0.0278,  0.1597])\n",
      "torch.argmax(q_values).item() 2\n",
      "action from training:  2\n",
      "Current Action:  2\n",
      "trade not found...\n",
      "step_reward:  -0.5\n",
      "total_reward:  -0.5\n",
      "self._total_profit:  0.49970000000000003\n",
      "reward: -0.5\n",
      "info: {'total_reward': -0.5, 'total_profit': 0.49970000000000003, 'position': <Positions.Hold: 2>}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, output_dim)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "    \n",
    "\n",
    "def epsilon_greedy_policy(model, epsilon, num_actions, device):\n",
    "    def policy_fn(state_info):\n",
    "        print(\"state_info: \", state_info)\n",
    "        state_array = state_info[0] if isinstance(state_info, tuple) else state_info  # Extract the array from the tuple if needed\n",
    "        print(\"state_array: \", state_array)\n",
    "        # state_array = state_array[0]\n",
    "        # print(\"state_array: \", state_array)\n",
    "        state_tensor = torch.tensor(state_array, dtype=torch.float32, device=device)\n",
    "        # print(\"state_tensor: \", state_tensor)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = model(state_tensor)\n",
    "            \n",
    "        \n",
    "        if q_values.numel() == 0:  # Check if q_values tensor is empty\n",
    "            return np.random.choice(num_actions)\n",
    "        else:\n",
    "            q_values_cpu = q_values.cpu()\n",
    "            print(\"q_values_cpu: \", q_values_cpu)\n",
    "            print(\"torch.argmax(q_values).item()\", torch.argmax(q_values_cpu).item())\n",
    "            return torch.argmax(q_values_cpu).item()\n",
    "    return policy_fn\n",
    "\n",
    "\n",
    "def train_dqn(env, model, target_model, optimizer, gamma, batch_size, device, epsilon):\n",
    "    state_info = env.reset()  # Ensure env.reset() returns a tuple (state, info) or just the state\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(model, epsilon, env.action_space.n, device)(state_info)\n",
    "        print(\"action from training: \", action)\n",
    "        step_result = env.step(action)\n",
    "        # print(\"step_result: \",step_result)\n",
    "        next_state, reward, done, truncate, info = step_result[:5]  # Extract the first four values\n",
    "        episode_reward += reward\n",
    "        \n",
    "        # # print(\"Shapes before converting to PyTorch tensors:\")\n",
    "        # if isinstance(state_info, tuple) and len(state_info) == 2:\n",
    "        #     print(\"state shape:\", state_info[0].shape)  # Access the state from the state_info tuple\n",
    "        # else:\n",
    "        #     print(\"state shape:\", state_info.shape)  # If state_info is not a tuple, assume it's the state itself\n",
    "        \n",
    "        # print(\"next_state:\", next_state)\n",
    "        # print(\"action:\", action)\n",
    "        print(\"reward:\", reward)\n",
    "        print(\"info:\", info)\n",
    "        \n",
    "        # if len(next_state) > 0:  # Check if next_state is not empty\n",
    "        #     print(\"next_state shape:\", next_state[0].shape)\n",
    "        # else:\n",
    "        #     print(\"next_state is empty\")\n",
    "        \n",
    "        # print(\"done:\", done)\n",
    "\n",
    "        done = done or truncate\n",
    "\n",
    "        state_info = next_state  # Update state_info\n",
    "        \n",
    "    return episode_reward\n",
    "\n",
    "\n",
    "# Parameters\n",
    "input_dim = 2  # Adjusted input dimensions\n",
    "output_dim = env.action_space.n\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "epsilon = 0.1\n",
    "lr = 1e-3\n",
    "target_update = 10\n",
    "num_episodes = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize DQN and target DQN\n",
    "model = DQN(input_dim, output_dim).to(device)\n",
    "target_model = DQN(input_dim, output_dim).to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "target_model.eval()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    # print(episode)\n",
    "    episode_reward = train_dqn(env, model, target_model, optimizer, gamma, batch_size, device, epsilon)\n",
    "    \n",
    "    if episode % target_update == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "        target_model.eval()\n",
    "    \n",
    "    if episode % 100 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "# After training, you can use the model for inference as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9997\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 1.0, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9994000000000001\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9997, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9991000000000001\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9994000000000001, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9988000000000001\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9991000000000001, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9985000000000002\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9988000000000001, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9982000000000002\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9985000000000002, 'position': <Positions.Hold: 2>} 0 2\n",
      "Current Action:  2\n",
      "Trade -  False\n",
      "trade not found...\n",
      "step_reward:  0\n",
      "total_reward:  0.0\n",
      "self._total_profit:  0.9979000000000002\n",
      "Info:  {'total_reward': 0.0, 'total_profit': 0.9982000000000002, 'position': <Positions.Hold: 2>} 0 2\n",
      "Episode 0, Reward: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, hidden_dim):\n",
    "        super(DQN, self).__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Input shape of LSTM layer should be (batch_size, seq_len, input_size)\n",
    "        # Assuming batch size is 1 for reinforcement learning\n",
    "        x, _ = self.lstm(x.unsqueeze(0))\n",
    "        x = x.view(-1, self.lstm.hidden_size)  # Reshape to (batch_size, hidden_dim)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "def epsilon_greedy_policy(model, epsilon, num_actions, device):\n",
    "    def policy_fn(state_info):\n",
    "        state_array = state_info[0] if isinstance(state_info, tuple) else state_info  # Extract the array from the tuple if needed\n",
    "        state_array = state_array[0]\n",
    "        state_tensor = torch.tensor(state_array, dtype=torch.float32, device=device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            q_values = model(state_tensor)\n",
    "            \n",
    "        if q_values.numel() == 0:  # Check if q_values tensor is empty\n",
    "            return np.random.choice(num_actions)\n",
    "        else:\n",
    "            q_values_cpu = q_values.cpu()\n",
    "            return torch.argmax(q_values_cpu).item()\n",
    "    return policy_fn\n",
    "\n",
    "def train_dqn(env, model, target_model, optimizer, gamma, batch_size, device, epsilon):\n",
    "    state_info = env.reset()  # Ensure env.reset() returns a tuple (state, info) or just the state\n",
    "    episode_reward = 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        action = epsilon_greedy_policy(model, epsilon, env.action_space.n, device)(state_info)\n",
    "        step_result = env.step(action)\n",
    "        next_state, reward, done, truncate, info = step_result[:5]  # Extract the first four values\n",
    "        episode_reward += reward\n",
    "        \n",
    "        done = done or truncate\n",
    "\n",
    "        print(\"Info: \", info, reward, action)\n",
    "        state_info = next_state  # Update state_info\n",
    "        \n",
    "    return episode_reward\n",
    "\n",
    "# Parameters\n",
    "input_dim = 2  # Adjusted input dimensions\n",
    "output_dim = env.action_space.n\n",
    "hidden_dim = 64  # Dimensionality of the hidden state in LSTM\n",
    "gamma = 0.99\n",
    "batch_size = 32\n",
    "epsilon = 0.1\n",
    "lr = 1e-3\n",
    "target_update = 10\n",
    "num_episodes = 1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Initialize DQN and target DQN\n",
    "model = DQN(input_dim, output_dim, hidden_dim).to(device)\n",
    "target_model = DQN(input_dim, output_dim, hidden_dim).to(device)\n",
    "target_model.load_state_dict(model.state_dict())\n",
    "target_model.eval()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "# Training loop\n",
    "for episode in range(num_episodes):\n",
    "    print(episode)\n",
    "    episode_reward = train_dqn(env, model, target_model, optimizer, gamma, batch_size, device, epsilon)\n",
    "    \n",
    "    if episode % target_update == 0:\n",
    "        target_model.load_state_dict(model.state_dict())\n",
    "        target_model.eval()\n",
    "    \n",
    "    if episode % 10 == 0:\n",
    "        print(f\"Episode {episode}, Reward: {episode_reward}\")\n",
    "\n",
    "# After training, you can use the model for inference as needed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "ArrayRef: invalid index Index = 18446744073709551615; Length = 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[253], line 32\u001b[0m\n\u001b[0;32m     29\u001b[0m num_eval_episodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Evaluate the model\u001b[39;00m\n\u001b[1;32m---> 32\u001b[0m eval_rewards \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_dqn\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_eval_episodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# Plot the evaluation rewards\u001b[39;00m\n\u001b[0;32m     35\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(eval_rewards)\n",
      "Cell \u001b[1;32mIn[253], line 17\u001b[0m, in \u001b[0;36mevaluate_dqn\u001b[1;34m(env, model, num_episodes, device)\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     16\u001b[0m     model\u001b[38;5;241m.\u001b[39meval();\n\u001b[1;32m---> 17\u001b[0m     q_values \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate_tensor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m action \u001b[38;5;241m=\u001b[39m q_values\u001b[38;5;241m.\u001b[39margmax()\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     19\u001b[0m next_state, reward, done, truncate, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[252], line 15\u001b[0m, in \u001b[0;36mDQN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 15\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     16\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(x))\n\u001b[0;32m     17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(x)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: ArrayRef: invalid index Index = 18446744073709551615; Length = 0"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def evaluate_dqn(env, model, num_episodes, device):\n",
    "    episode_rewards = []\n",
    "    for episode in range(num_episodes):\n",
    "        state = env.reset()\n",
    "        episode_reward = 0\n",
    "        done = False\n",
    "        while not done:\n",
    "            # print(state)\n",
    "            state_array = state[0] if isinstance(state, tuple) else state  # Extract the array from the tuple if needed\n",
    "            state_array = state_array[0]\n",
    "            state_tensor = torch.tensor(state_array, dtype=torch.float32, device=device)\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                model.eval();\n",
    "                q_values = model(state_tensor)\n",
    "            action = q_values.argmax().item()\n",
    "            next_state, reward, done, truncate, info = env.step(action)\n",
    "\n",
    "            # print(reward)\n",
    "            state = next_state\n",
    "            episode_reward += reward\n",
    "            done = done or truncate\n",
    "        episode_rewards.append(episode_reward)\n",
    "    return episode_rewards\n",
    "\n",
    "# Evaluation parameters\n",
    "num_eval_episodes = 100\n",
    "\n",
    "# Evaluate the model\n",
    "eval_rewards = evaluate_dqn(env, model, num_eval_episodes, device)\n",
    "\n",
    "# Plot the evaluation rewards\n",
    "plt.plot(eval_rewards)\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Total Reward')\n",
    "plt.title('DQN Evaluation Rewards')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM - DQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), './results/dqn_model_10000ep.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.save(\"dqn_stable_baseline3_eurusd_2023_1M\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(16, 6))\n",
    "env.unwrapped.render_all()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Length of values (45) does not match length of index (89)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[213], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mquantstats\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mqs\u001b[39;00m\n\u001b[0;32m      3\u001b[0m qs\u001b[38;5;241m.\u001b[39mextend_pandas()\n\u001b[1;32m----> 5\u001b[0m net_worth \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munwrapped\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtotal_profit\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m[\u001b[49m\u001b[43mstart_index\u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43mend_index\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m returns \u001b[38;5;241m=\u001b[39m net_worth\u001b[38;5;241m.\u001b[39mpct_change()\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m1\u001b[39m:]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(returns)\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:503\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[1;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[0;32m    501\u001b[0m     index \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(data))\n\u001b[0;32m    502\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n\u001b[1;32m--> 503\u001b[0m     \u001b[43mcom\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrequire_length_match\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;66;03m# create/copy the manager\u001b[39;00m\n\u001b[0;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, (SingleBlockManager, SingleArrayManager)):\n",
      "File \u001b[1;32mc:\\Users\\ACER\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\common.py:561\u001b[0m, in \u001b[0;36mrequire_length_match\u001b[1;34m(data, index)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    558\u001b[0m \u001b[38;5;124;03mCheck the length of data matches the length of the index.\u001b[39;00m\n\u001b[0;32m    559\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    560\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(data) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(index):\n\u001b[1;32m--> 561\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    562\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLength of values \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    563\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(data)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    564\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdoes not match length of index \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    565\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(index)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    566\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Length of values (45) does not match length of index (89)"
     ]
    }
   ],
   "source": [
    "import quantstats as qs\n",
    "\n",
    "qs.extend_pandas()\n",
    "\n",
    "net_worth = pd.Series(env.unwrapped.history['total_profit'], index=df.index[start_index+1:end_index])\n",
    "returns = net_worth.pct_change().iloc[1:]\n",
    "\n",
    "print(returns)\n",
    "\n",
    "qs.reports.full(returns)\n",
    "qs.reports.html(returns, output='SB3_a2c_quantstats.html')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
